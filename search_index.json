[["index.html", "데이터 과학 라이브 북 (Data Science Live Book) 서문 (Preface)", " 데이터 과학 라이브 북 (Data Science Live Book) Pablo Casas (번역: fkt) 2026-01-29 서문 (Preface) 종이책 &amp; 킨들 (Amazon) 이 책은 현재 아마존에서 구매 가능합니다. 확인해 보세요! 📗 🚀. 흑백 버전 링크이며, 풀컬러 버전도 준비되어 있습니다. 100개국 이상으로 배송 가능합니다. 🌎 왜 이 책인가요? 이 책은 데이터 분석과 머신러닝을 수행할 때 마주치는 일반적인 문제들에 대한 이해를 돕기 위해 작성되었습니다. 예측 모델을 만드는 것은 단 한 줄의 R 코드로 가능할 만큼 간단할 수 있습니다. my_fancy_model=randomForest(target ~ var_1 + var_2, my_complicated_data) 그게 전부입니다. 하지만 실제 데이터는 매우 지저분합니다. 우리는 예술가가 조각을 하듯 데이터를 다듬어, 그 안에 숨겨진 정보를 드러내고 답(그리고 새로운 질문)을 찾아야 합니다. 해결해야 할 도전 과제는 많으며, 어떤 데이터셋은 다른 것보다 더 많은 조각(sculpting) 과정이 필요합니다. 예를 들어, 랜덤 포레스트는 결측값(empty values)을 허용하지 않습니다. 그렇다면 어떻게 해야 할까요? 문제가 되는 행을 삭제해야 할까요? 아니면 결측값을 다른 값으로 변환해야 할까요? 어떤 경우든 내 데이터에 미치는 영향은 무엇일까요? 결측값 문제 외에도, 이상치(outliers)는 예측 모델 자체뿐만 아니라 최종 결과의 해석까지 왜곡할 수 있습니다. 예측 모델이 각 변수를 어떻게 고려하는지(변수 중요도 순위), 그리고 어떤 값이 특정 사건의 발생 가능성을 높이거나 낮추는지(변수 프로파일링)를 “추측하고 시도”하는 것은 흔한 일입니다. 변수의 데이터 유형을 결정하는 것도 사소하지 않습니다. 범주형 변수가 수치형이 될 수도 있고 그 반대일 수도 있는데, 이는 문맥, 데이터, 그리고 알고리즘 자체(일부는 특정 데이터 유형만 처리 가능)에 따라 달라집니다. 이러한 변환 역시 _모델이 변수를 바라보는 방식_에 영향을 미칩니다. 이 책은 데이터 준비, 데이터 분석, 그리고 머신러닝에 관한 책입니다. 일반적으로 문헌에서는 데이터 준비 과정이 머신러닝 모델 생성만큼 인기를 끌지 못하곤 합니다. 배움을 향한 여정 이 책은 매우 실무적인 접근 방식을 취하며, 주장하는 바를 직접 증명하려고 노력합니다. 예를 들어, “변수는 그룹으로 작동합니다”라고 말한 뒤, 그 아이디어를 뒷받침하는 코드를 보여줍니다. 거의 모든 장의 코드를 복사해서 붙여넣고 직접 실행해 보며 스스로 결론을 도출할 수 있습니다. 더 나아가, 제안된 코드나 스크립트(R 언어)는 범용적으로 생각되었으므로 연구나 업무 등 실제 시나리오에서도 사용할 수 있습니다. 이 책의 뿌리는 funModeling이라는 R 라이브러리입니다. 처음에는 교육용 문서로 시작했으나 빠르게 이 책으로 발전했습니다. 단순히 히스토그램을 그려서 타겟 변수를 프로파일링하는 함수(cross_plot)를 사용하는 법을 가르치는 것을 넘어, 어떻게 의미론적인 결론에 도달하는지 설명하기 때문에 교육적이라 할 수 있습니다. 의도는 내부 개념을 학습하여 그 지식을 Python, Julia 등 다른 언어로도 확장할 수 있도록 하는 것입니다. 데이터 프로젝트 개발과 마찬가지로 이 책도 선형적이지 않습니다. 각 장은 서로 연결되어 있습니다. 예를 들어, 결측값 장은 범주형 변수의 카디널리티 감소로 이어질 수 있습니다. 또는 데이터 유형 장을 읽고 난 뒤 결측값을 처리하는 방식을 바꿀 수도 있습니다. 학습을 확장할 수 있도록 다른 웹사이트 참조 링크도 포함되어 있습니다. 이 책은 배움의 여정에서 또 하나의 단계일 뿐입니다. 이 책이 나에게 맞을까요? 이해할 수 있을까요? 이미 데이터 과학 분야에 종사하고 있다면 그렇게 생각하지 않을 수도 있습니다. 필요한 코드만 골라서 복사해 쓰면 그만이니까요. 하지만 데이터 과학 경력을 이제 막 시작했다면, 교육에서 흔히 발생하는 문제인 _“아직 하지 않은 질문에 대한 답을 듣는 것”_을 경험하게 될 것입니다. 확실한 것은 여러분이 데이터 과학의 세계에 더 가까워질 것이라는 점입니다. 모든 코드는 주석이 잘 달려 있어 프로그래머가 아닐지라도 이해할 수 있습니다. 읽기 편하면서도 논리, 상식, 직관을 사용하는 것, 그것이 이 책의 도전 과제입니다. 프로그래밍 언어 약간의 R을 배울 수는 있겠지만, 이 책만으로 직접 배우기는 쉽지 않을 수 있습니다. R 프로그래밍을 제대로 배우고 싶다면 프로그래밍에 특화된 다른 책이나 강의를 참고하시길 권장합니다. 이제 다음 섹션으로 넘어가겠습니다. 기계와 인공지능이 세상을 지배하게 될까요? 😱 컴퓨팅 파워가 기하급수적으로 증가하고 있는 것은 사실이지만, 기계의 반란은 아직 요원한 일입니다. 이 책은 예측 모델을 만들고 다룰 때 발생하는 공통적인 문제들을 드러내고자 합니다. “공짜 점심”은 없습니다. “원클릭 솔루션”만으로 예측 시스템이 뚝딱 실행되고 배포되는 것 사이에는 큰 간극이 있습니다. 데이터 준비, 변환, 테이블 조인, 타이밍 고려, 튜닝 등의 모든 과정이 한 번에 해결되는 것처럼 보일 수 있습니다. 어쩌면 그럴 수도 있습니다. 시간이 흐르면서 예측 모델링 작업을 자동화하는 데 도움을 주는 더 강력한 기술들이 등장하고 있습니다. 하지만 만약을 대비하여, 시스템이 어떻게 최적의 변수를 선택하는지, 모델을 검증하는 내부 절차는 무엇인지, 극단값이나 희귀값을 어떻게 처리하는지 등을 알지 못한 채 블랙박스 솔루션을 맹목적으로 믿지 않는 것이 좋습니다. 이 책은 바로 그런 주제들을 다룹니다. 만약 어떤 머신러닝 플랫폼을 평가하고 있다면, 이 책에서 언급된 이슈들이 최선의 선택을 내리는 데 도움이 될 것입니다. 즉, 블랙박스를 열어보는 것(unbox the black-box)입니다. 모든 경우에 들어맞는 단 하나의 솔루션을 갖기는 어렵습니다. 프로젝트가 성공하기 위해서는 인간의 개입이 매우 중요합니다. 기계 자체를 걱정하기보다는, 이 기술을 어떻게 사용할 것인지가 핵심입니다. 기술은 죄가 없습니다. 입력을 설정하고 모델이 학습할 타겟을 결정하는 것은 데이터 과학자입니다. 패턴은 나타날 것이고, 그중 일부는 누군가에게 해로울 수도 있습니다. 모든 기술과 마찬가지로 우리는 최종적인 목표를 의식해야 합니다. 기계는 인간에 의해 만들어지며, 그것으로 무엇을 할지는 인간에게 달려 있다. (스페인어 원문: “La maquina la hace el hombre, y es lo que el hombre hace con ella.”) 호르헤 드렉슬러 (뮤지션, 배우, 의사). “Guitarra y vos” 노래에서 발췌. 어쩌면 이것이 머신러닝(machine learning)과 데이터 과학(data science)의 차이일까요? 학습하는 기계 대 데이터를 사용하여 과학을 하는 인간? 🤔 열린 질문입니다. 시작하려면 무엇이 필요한가요? 일반적으로 시간과 인내심이 필요합니다. 대부분의 개념은 언어에 독립적이지만, 기술적인 예제가 필요할 때는 R 언어 (R version 4.5.2 (2025-10-31))를 사용하여 설명합니다. 이 책에서는 다음 라이브러리들을 사용합니다 (괄호 안은 패키지 버전입니다): ## funModeling (1.9.5), dplyr (1.1.4), Hmisc (5.2.5) ## reshape2 (1.4.5), ggplot2 (4.0.1), caret (7.0.1) ## minerva (1.5.10), missForest (1.6.1), gridExtra (2.3) ## mice (3.19.0), Lock5Data (4.0.1), corrplot (0.95) ## RColorBrewer (1.1.3), infotheo (1.2.0.1) funModeling 패키지는 이 책의 시작점이었습니다. 데이터 과학자가 일상적인 업무를 수행하는 데 도움을 주는 함수 모음으로 시작했으며, 이제 그 문서가 이 책으로 진화했습니다 ❤️! 필요한 패키지는 install.packages(\"PACKAGE_NAME\") 명령어로 설치할 수 있습니다. 추천하는 IDE는 Rstudio입니다. 이 책(PDF 및 웹 버전 모두)은 Rstudio에서 놀라운 Bookdown을 사용하여 제작되었습니다. Bookdown, R, Rstudio, 그리고 이 책까지 모두 자유롭게 사용할 수 있는 오픈 소스입니다 🙂. 이 책이 북다운에서 어떻게 생성되었는지, 그리고 아마존에 어떻게 발행했는지 뒷이야기를 확인하시려면 다음 글들을 참고하세요: How to self-publish a book: A handy list of resources 및 How to self publish a book: customizing Bookdown. 이 리소스들은 제1회 Bookdown Contest에서 RStudio로부터 상을 받기도 했습니다. 즐겁게 읽어주시길 바랍니다! 연락은 어떻게 하나요? 📩 인사를 건네거나, 설명이 부족한 부분을 알려주거나, 새로운 주제를 제안하거나, 배운 개념을 적용한 경험을 공유하고 싶다면 언제든지 이메일을 보내주세요: pcasas.biz (at) gmail.com. 저는 끊임없이 배우고 있으므로 다른 동료들과 지식을 나누고 연락을 주고받는 것을 좋아합니다. Twitter Linkedin Github Data Science Heroes Blog 또한, 책과 funModeling의 Github 저장소를 확인하여 버그 제보, 제안, 새로운 아이디어 등을 공유할 수 있습니다: funModeling Data Science Live Book 감사의 말 이 데이터 세계의 멘토인 Miguel Spindiak과 Marcelo Ferreyra에게 특별한 감사를 전합니다. 기술 리뷰어: Pablo Seibelt (aka The Sicarul) 🛠️. 진심 어린 헌신적인 도움에 감사드립니다. 표지 아트 제작: Bárbara Muñoz🎨. 이 책을 에두아르도 갈레아노의 단편 소설 _The Nobodies_에 헌정합니다. 책 정보 최초 발행: livebook.datascienceheroes.com. Attribution-NonCommercial-ShareAlike 4.0 International 라이선스를 따릅니다. ISBN: 978-987-42-5911-0 (eBook 버전). Copyright (c) 2018. "],["exploratory_data_analysis.html", "1 탐색적 데이터 분석 1.1 프로파일링: 숫자의 목소리 1.2 상관 관계 및 관계 {https://www.google.com/search?q=%23correlation}", " 1 탐색적 데이터 분석 숫자들의 소리에 귀 기울이기 :) 1.1 프로파일링: 숫자의 목소리 “숫자의 목소리” – 에두아르도 갈레아노, 작가이자 소설가의 비유. 우리가 탐색하는 데이터는 제대로 해석하지 못하면 이집트 상형문자와 같을 수 있습니다. 프로파일링은 데이터가 우리에게 무엇을 말하고 싶어 하는지, 우리가 충분히 인내심을 갖고 귀 기울인다면, 그 메시지를 찾는 일련의 반복적인 단계 중 첫걸음입니다. 이 장에서는 몇 가지 함수를 사용하여 완벽한 데이터 프로파일링을 다룰 것입니다. 이는 데이터 프로젝트의 첫 단계가 되어야 하며, 올바른 데이터 유형을 파악하고 수치형 및 범주형 변수의 분포를 탐색하는 것부터 시작합니다. 또한 비전문가에게 보고서를 작성할 때 유용한 의미론적 결론을 도출하는 데 중점을 둡니다. 이 장에서 무엇을 살펴볼까요? 데이터셋 상태: 총 행, 열, 데이터 유형, 0 값, 결측치와 같은 지표 얻기 각 항목이 다양한 분석에 미치는 영향 데이터를 정리하기 위해 이를 빠르게 필터링하고 조작하는 방법 범주형 변수의 단변량 분석: 빈도, 백분율, 누적 값, 시각적으로 매력적인 플롯 수치형 변수의 단변량 분석: 백분위수, 분산, 표준 편차, 평균, 상위 및 하위 값 백분위수 vs. 분위수 vs. 사분위수 왜도, 첨도, 사분위 범위, 변동 계수 분포 플로팅 “데이터 월드”를 기반으로 한 데이터 준비 및 데이터 분석의 완벽한 사례 연구 이 장에서 다룰 함수 요약: df_status(data): 데이터셋 구조 프로파일링 describe(data): 수치형 및 범주형 프로파일링 (정량적) freq(data): 범주형 프로파일링 (정량적 및 플롯). profiling_num(data): 수치형 변수 프로파일링 (정량적) plot_num(data): 수치형 변수 프로파일링 (플롯) 참고: describe는 Hmisc 패키지에 있으며, 나머지 함수는 funModeling에 있습니다. – 1.1.1 데이터셋 상태 {#dataset-health-status} 0, NA, Inf, 고유 값의 양과 데이터 유형은 모델의 좋고 나쁨에 영향을 미칠 수 있습니다. 다음은 데이터 모델링의 첫 단계를 다루는 접근 방식입니다. 먼저, funModeling 및 dplyr 라이브러리를 로드합니다. # Loading funModeling! library(funModeling) library(dplyr) data(heart_disease) 1.1.1.1 결측치, 0 값, 데이터 유형 및 고유 값 확인 새로운 데이터셋을 분석할 때 가장 먼저 해야 할 일 중 하나는 결측치(R에서는 NA)가 있는지와 데이터 유형을 파악하는 것입니다. funModeling에 포함된 df_status 함수는 상대적 값과 백분율로 이러한 수치를 보여주는 데 도움이 됩니다. 또한 무한대 값과 0 값에 대한 통계도 검색합니다. # Profiling the data input df_status(heart_disease) Figure 1.1: 데이터셋 상태 q_zeros: 0 값의 수 (p_zeros: 백분율) q_inf: 무한대 값의 수 (p_inf: 백분율) q_na: NA 값의 수 (p_na: 백분율) type: factor 또는 numeric unique: 고유 값의 수 1.1.1.2 이러한 지표가 왜 중요할까요? 0 값: 0 값이 많은 변수는 모델링에 유용하지 않을 수 있으며, 어떤 경우에는 모델을 심각하게 편향시킬 수 있습니다. NA: 여러 모델은 NA가 있는 행을 자동으로 제외합니다(예: 랜덤 포레스트). 결과적으로 단 하나의 변수 때문에 여러 행이 누락되어 최종 모델이 편향될 수 있습니다. 예를 들어, 데이터에 100개 변수 중 90%가 NA인 변수가 하나만 있다면, 모델은 원본 행의 10%만으로 학습하게 될 것입니다. Inf: 무한대 값은 R의 일부 함수에서 예상치 못한 동작을 유발할 수 있습니다. Type: 일부 변수는 숫자로 인코딩되어 있지만, 코드나 범주이며 모델이 이를 동일한 방식으로 처리하지 않습니다. Unique: 고유 값의 수가 많은(약 30개) Factor/범주형 변수는 범주의 카디널리티가 낮으면 과적합되는 경향이 있습니다(예: 의사결정 트리). 1.1.1.3 불필요한 사례 필터링 df_status 함수는 데이터 프레임을 받아 앞 섹션에서 설명한 모든 지표를 기반으로 피처(또는 변수)를 빠르게 제거하는 데 도움이 되는 상태 테이블을 반환합니다. 예를 들어: 0 값이 많은 변수 제거 # Profiling the Data Input my_data_status=df_status(heart_disease) ## variable q_zeros p_zeros q_na p_na q_inf p_inf type unique ## 1 age 0 0.00 0 0.00 0 0 integer 41 ## 2 gender 0 0.00 0 0.00 0 0 factor 2 ## 3 chest_pain 0 0.00 0 0.00 0 0 factor 4 ## 4 resting_blood_pressure 0 0.00 0 0.00 0 0 integer 50 ## 5 serum_cholestoral 0 0.00 0 0.00 0 0 integer 152 ## 6 fasting_blood_sugar 258 85.15 0 0.00 0 0 factor 2 ## 7 resting_electro 151 49.83 0 0.00 0 0 factor 3 ## 8 max_heart_rate 0 0.00 0 0.00 0 0 integer 91 ## 9 exer_angina 204 67.33 0 0.00 0 0 integer 2 ## 10 oldpeak 99 32.67 0 0.00 0 0 numeric 40 ## 11 slope 0 0.00 0 0.00 0 0 integer 3 ## 12 num_vessels_flour 176 58.09 4 1.32 0 0 integer 4 ## 13 thal 0 0.00 2 0.66 0 0 factor 3 ## 14 heart_disease_severity 164 54.13 0 0.00 0 0 integer 5 ## 15 exter_angina 204 67.33 0 0.00 0 0 factor 2 ## 16 has_heart_disease 0 0.00 0 0.00 0 0 factor 2 # Removing variables with 60% of zero values vars_to_remove=filter(my_data_status, p_zeros &gt; 60) %&gt;% pull(variable) vars_to_remove ## [1] &quot;fasting_blood_sugar&quot; &quot;exer_angina&quot; &quot;exter_angina&quot; # Keeping all columns except the ones present in &#39;vars_to_remove&#39; vector heart_disease_2=select(heart_disease, -any_of(vars_to_remove)) 0 값 백분율에 따라 데이터 정렬 arrange(my_data_status, -p_zeros) %&gt;% select(variable, q_zeros, p_zeros) ## variable q_zeros p_zeros ## 1 fasting_blood_sugar 258 85.15 ## 2 exer_angina 204 67.33 ## 3 exter_angina 204 67.33 ## 4 num_vessels_flour 176 58.09 ## 5 heart_disease_severity 164 54.13 ## 6 resting_electro 151 49.83 ## 7 oldpeak 99 32.67 ## 8 age 0 0.00 ## 9 gender 0 0.00 ## 10 chest_pain 0 0.00 ## 11 resting_blood_pressure 0 0.00 ## 12 serum_cholestoral 0 0.00 ## 13 max_heart_rate 0 0.00 ## 14 slope 0 0.00 ## 15 thal 0 0.00 ## 16 has_heart_disease 0 0.00 동일한 논리가 특정 임계값 이상 또는 이하의 변수를 제거(또는 유지)하려는 경우에도 적용됩니다. 결측치가 포함된 변수를 다룰 때의 함의에 대한 자세한 정보는 결측치 장을 참조하십시오. 1.1.1.4 이러한 주제에 대해 더 깊이 알아보기 df_status가 반환하는 값은 다른 장에서 심층적으로 다루고 있습니다: 결측치(NA) 처리, 분석 및 결측치 대치는 결측치 장에서 심층적으로 다룹니다. 데이터 유형, 그 변환 및 다양한 데이터 유형을 다룰 때의 함의 등은 데이터 유형 장에서 다룹니다. 높은 수의 고유 값은 고카디널리티 변수와 동의어입니다. 이 상황은 다음 두 장에서 연구됩니다: 기술 통계에서의 고카디널리티 변수 예측 모델링에서의 고카디널리티 변수 1.1.1.5 기타 일반적인 통계 얻기: 총 행 수, 총 열 수 및 열 이름: # Total rows nrow(heart_disease) ## [1] 303 # Total columns ncol(heart_disease) ## [1] 16 # Column names colnames(heart_disease) ## [1] &quot;age&quot; &quot;gender&quot; &quot;chest_pain&quot; ## [4] &quot;resting_blood_pressure&quot; &quot;serum_cholestoral&quot; &quot;fasting_blood_sugar&quot; ## [7] &quot;resting_electro&quot; &quot;max_heart_rate&quot; &quot;exer_angina&quot; ## [10] &quot;oldpeak&quot; &quot;slope&quot; &quot;num_vessels_flour&quot; ## [13] &quot;thal&quot; &quot;heart_disease_severity&quot; &quot;exter_angina&quot; ## [16] &quot;has_heart_disease&quot; – 1.1.2 범주형 변수 프로파일링 {#profiling-categorical-variables} 최신 ‘funModeling’ 버전(&gt;= 1.6)을 사용하고 있는지 확인하십시오. freq 함수는 빈도 또는 분포 분석을 간편하게 만듭니다. 이 함수는 분포를 테이블과 플롯(기본값)으로 검색하고 절대 및 상대 숫자의 분포를 보여줍니다. 두 변수에 대한 분포를 원한다면: freq(data=heart_disease, input = c(&#39;thal&#39;,&#39;chest_pain&#39;)) ## Warning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use &quot;none&quot; instead as ## of ggplot2 3.3.4. ## ℹ The deprecated feature was likely used in the funModeling package. ## Please report the issue at &lt;https://github.com/pablo14/funModeling/issues&gt;. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. Figure 1.2: 빈도 분석 1 ## thal frequency percentage cumulative_perc ## 1 3 166 54.79 54.79 ## 2 7 117 38.61 93.40 ## 3 6 18 5.94 99.34 ## 4 &lt;NA&gt; 2 0.66 100.00 Figure 1.3: 빈도 분석 2 ## chest_pain frequency percentage cumulative_perc ## 1 4 144 47.52 47.52 ## 2 3 86 28.38 75.90 ## 3 2 50 16.50 92.40 ## 4 1 23 7.59 100.00 ## [1] &quot;Variables processed: thal, chest_pain&quot; 나머지 funModeling 함수와 마찬가지로 input이 없으면 주어진 데이터 프레임에 있는 모든 factor 또는 character 변수에 대해 실행됩니다. freq(data=heart_disease) 플롯을 제외하고 테이블만 출력하려면 plot 매개변수를 FALSE로 설정합니다. freq 예시는 단일 변수를 입력으로 처리할 수도 있습니다. 기본적으로 NA 값은 테이블과 플롯 모두에서 고려됩니다. NA를 제외해야 한다면 na.rm = TRUE로 설정하십시오. 다음 줄에 있는 두 가지 예시: freq(data=heart_disease$thal, plot = FALSE, na.rm = TRUE) 하나의 변수만 제공되면 freq는 출력된 테이블을 반환하므로, 제공되는 변수를 기반으로 일부 계산을 쉽게 수행할 수 있습니다. 예를 들어, 공유의 80% 이상을 차지하는 범주를 출력하는 것(cumulative_perc &lt; 80을 기반으로). 긴 꼬리에 속하는 범주를 얻는 것, 즉 percentage &lt; 1로 필터링하여 1% 미만으로 나타나는 범주를 검색하는 것. 또한, 패키지의 다른 플롯 함수와 마찬가지로 플롯을 내보내야 하는 경우 path_out 매개변수를 추가하면 폴더가 아직 생성되지 않았다면 생성됩니다. freq(data=heart_disease, path_out=&#39;my_folder&#39;) 1.1.2.0.1 분석 출력은 frequency 변수에 따라 정렬되므로, 가장 빈번한 범주와 이들이 차지하는 비중(cummulative_perc 변수)을 빠르게 분석할 수 있습니다. 일반적으로 우리는 인간으로서 질서를 좋아합니다. 변수가 정렬되어 있지 않으면 우리의 눈은 모든 막대를 비교하기 위해 움직이고, 우리의 뇌는 각 막대를 다른 막대와 관련하여 배치합니다. 동일한 데이터 입력에 대해 정렬되지 않은 경우와 정렬된 경우의 차이를 확인해 보세요: Figure 1.4: 순서와 아름다움 일반적으로 대부분의 경우에 나타나는 범주는 소수에 불과합니다. 더 자세한 분석은 기술 통계에서의 고카디널리티 변수에서 확인하세요. 1.1.2.1 describe 함수 소개 이 함수는 Hmisc 패키지에 포함되어 있으며, 수치형 및 범주형 변수 모두에 대해 전체 데이터셋을 빠르게 프로파일링할 수 있게 해줍니다. 이 경우, 우리는 두 개의 변수만 선택하고 결과를 분석할 것입니다. # Just keeping two variables to use in this example heart_disease_3=select(heart_disease, thal, chest_pain) # Profiling the data! describe(heart_disease_3) ## heart_disease_3 ## ## 2 Variables 303 Observations ## -------------------------------------------------------------------------------- ## thal ## n missing distinct ## 301 2 3 ## ## Value 3 6 7 ## Frequency 166 18 117 ## Proportion 0.551 0.060 0.389 ## -------------------------------------------------------------------------------- ## chest_pain ## n missing distinct ## 303 0 4 ## ## Value 1 2 3 4 ## Frequency 23 50 86 144 ## Proportion 0.076 0.165 0.284 0.475 ## -------------------------------------------------------------------------------- 여기서: n: NA가 아닌 행의 수. 이 경우, 숫자를 포함하는 환자가 301명 있음을 나타냅니다. missing: 결측치의 수. 이 지표를 n에 더하면 총 행 수가 됩니다. unique: 고유(또는 구별되는) 값의 수. 다른 정보는 freq 함수와 매우 유사하며, 각기 다른 범주에 대해 상대적 및 절대적 값으로 총 수를 괄호 안에 반환합니다. – 1.1.3 수치형 변수 프로파일링 이 섹션은 두 부분으로 나뉩니다: 파트 1: “월드 데이터” 사례 연구 소개 파트 2: R에서 수치형 프로파일링 수행 데이터 월드의 데이터 준비 단계가 어떻게 계산되는지 알고 싶지 않다면, 프로파일링이 시작되는 “파트 2: R에서 수치형 프로파일링 수행”으로 건너뛰셔도 좋습니다. 1.1.3.1 파트 1: 월드 데이터 사례 연구 소개 이 데이터에는 세계 개발과 관련된 많은 지표가 포함되어 있습니다. 프로파일링 예제와는 별개로, 이 종류의 데이터 분석에 관심이 있는 사회학자, 연구원 등을 위해 바로 사용할 수 있는 테이블을 제공하는 것이 목적입니다. 원본 데이터 출처는 다음과 같습니다: http://databank.worldbank.org. 거기서 모든 변수를 설명하는 데이터 사전을 찾을 수 있습니다. 먼저, 데이터 정리가 필요합니다. 각 지표별로 가장 최신 값을 유지할 것입니다. library(Hmisc) # Loading data from the book repository without altering the format data_world=read.csv(file = &quot;https://goo.gl/2TrDgN&quot;, header = T, stringsAsFactors = F, na.strings = &quot;..&quot;) # Excluding missing values in Series.Code. The data downloaded from the web page contains four lines with &quot;free-text&quot; at the bottom of the file. data_world=filter(data_world, Series.Code!=&quot;&quot;) # The magical function that keeps the newest values for each metric. If you&#39;re not familiar with R, then skip it. max_ix&lt;-function(d) { ix=which(!is.na(d)) res=ifelse(length(ix)==0, NA, d[max(ix)]) return(res) } data_world$newest_value=apply(data_world[,5:ncol(data_world)], 1, FUN=max_ix) # Printing the first three rows head(data_world, 3) ## Series.Name Series.Code ## 1 Population living in slums (% of urban population) EN.POP.SLUM.UR.ZS ## 2 Population living in slums (% of urban population) EN.POP.SLUM.UR.ZS ## 3 Population living in slums (% of urban population) EN.POP.SLUM.UR.ZS ## Country.Name Country.Code X1990..YR1990. X2000..YR2000. X2007..YR2007. ## 1 Afghanistan AFG NA NA NA ## 2 Albania ALB NA NA NA ## 3 Algeria DZA 11.8 NA NA ## X2008..YR2008. X2009..YR2009. X2010..YR2010. X2011..YR2011. X2012..YR2012. ## 1 NA NA NA NA NA ## 2 NA NA NA NA NA ## 3 NA NA NA NA NA ## X2013..YR2013. X2014..YR2014. X2015..YR2015. X2016..YR2016. newest_value ## 1 NA 62.7 NA NA 62.7 ## 2 NA NA NA NA NA ## 3 NA NA NA NA 11.8 Series.Name과 Series.Code 열은 분석할 지표입니다. Country.Name과 Country.Code는 국가입니다. 각 행은 국가와 지표의 고유한 조합을 나타냅니다. 나머지 열인 X1990..YR1990. (1990년), X2000..YR2000. (2000년), X2007..YR2007. (2007년) 등은 해당 연도의 지표 값을 나타내며, 각 열은 연도입니다. 1.1.3.2 데이터 과학자의 의사결정 일부 국가에서는 해당 연도에 지표 측정이 없어 NA 값이 많습니다. 이 시점에서 우리는 데이터 과학자로서 결정을 내려야 합니다. 전문가(예: 사회학자)에게 묻지 않는다면 최적의 결정은 아닐 것입니다. NA 값은 어떻게 처리할까요? 이 경우, 모든 지표에 대해 가장 최신 값을 유지할 것입니다. 아마도 어떤 국가의 정보는 2016년까지 업데이트되어 있고, 다른 국가는 2009년까지만 업데이트되어 있을 것이므로, 논문 결론을 도출하는 데 최선의 방법은 아닐 수 있습니다. 하지만 첫 번째 분석을 위해 모든 지표를 최신 데이터와 비교하는 것은 유효한 접근 방식입니다. 다른 해결책은 가장 최신 값을 유지하되, 이 숫자가 지난 5년 이내의 값인 경우에만 유지하는 것이었을 수 있습니다. 이렇게 하면 분석할 국가의 수가 줄어들 것입니다. 이러한 질문들은 인공지능 시스템이 답하기 불가능하지만, 이 결정은 결과에 극적인 영향을 미칠 수 있습니다. 마지막 변환 다음 단계는 마지막 테이블을 ‘롱(long)’ 형식에서 ‘와이드(wide)’ 형식으로 변환하는 것입니다. 즉, 각 행은 국가를 나타내고 각 열은 지표를 나타냅니다(지표-국가 조합별 ’최신 값’을 포함하는 이전 변환 덕분입니다). 지표 이름이 명확하지 않으므로, 그중 몇 가지를 “번역”할 것입니다. # Get the list of indicator descriptions. names=unique(select(data_world, Series.Name, Series.Code)) head(names, 5) ## Series.Name Series.Code ## 1 Population living in slums (% of urban population) EN.POP.SLUM.UR.ZS ## 218 Income share held by second 20% SI.DST.02ND.20 ## 435 Income share held by third 20% SI.DST.03RD.20 ## 652 Income share held by fourth 20% SI.DST.04TH.20 ## 869 Income share held by highest 20% SI.DST.05TH.20 # Convert a few df_conv_world=data.frame( new_name=c(&quot;urban_poverty_headcount&quot;, &quot;rural_poverty_headcount&quot;, &quot;gini_index&quot;, &quot;pop_living_slums&quot;, &quot;poverty_headcount_1.9&quot;), Series.Code=c(&quot;SI.POV.URHC&quot;, &quot;SI.POV.RUHC&quot;, &quot;SI.POV.GINI&quot;, &quot;EN.POP.SLUM.UR.ZS&quot;, &quot;SI.POV.DDAY&quot;), stringsAsFactors = F) # adding the new indicator value data_world_2 = left_join(data_world, df_conv_world, by=&quot;Series.Code&quot;) data_world_2 = mutate(data_world_2, Series.Code_2= ifelse(!is.na(new_name), as.character(data_world_2$new_name), data_world_2$Series.Code) ) 모든 지표의 의미는 data.worldbank.org에서 확인할 수 있습니다. 예를 들어, EN.POP.SLUM.UR.ZS가 무엇을 의미하는지 알고 싶다면, http://data.worldbank.org/indicator/EN.POP.SLUM.UR.ZS를 입력하면 됩니다. # The package &#39;reshape2&#39; contains both &#39;dcast&#39; and &#39;melt&#39; functions library(reshape2) data_world_wide=dcast(data_world_2, Country.Name ~ Series.Code_2, value.var = &quot;newest_value&quot;) 참고: reshape2 패키지를 사용하여 long 및 wide 형식에 대해 더 자세히 이해하고, 이들 간에 변환하는 방법에 대해 알아보려면 http://seananderson.ca/2013/10/19/reshape.html을 참조하십시오. 이제 분석할 최종 테이블이 준비되었습니다: # Printing the first three rows head(data_world_wide, 3) ## Country.Name gini_index pop_living_slums poverty_headcount_1.9 ## 1 Afghanistan NA 62.7 NA ## 2 Albania 28.96 NA 1.06 ## 3 Algeria NA 11.8 NA ## rural_poverty_headcount SI.DST.02ND.20 SI.DST.03RD.20 SI.DST.04TH.20 ## 1 38.3 NA NA NA ## 2 15.3 13.17 17.34 22.81 ## 3 4.8 NA NA NA ## SI.DST.05TH.20 SI.DST.10TH.10 SI.DST.FRST.10 SI.DST.FRST.20 SI.POV.2DAY ## 1 NA NA NA NA NA ## 2 37.82 22.93 3.66 8.85 6.79 ## 3 NA NA NA NA NA ## SI.POV.GAP2 SI.POV.GAPS SI.POV.NAGP SI.POV.NAHC SI.POV.RUGP SI.POV.URGP ## 1 NA NA 8.4 35.8 9.3 5.6 ## 2 1.43 0.22 2.9 14.3 3.0 2.9 ## 3 NA NA NA 5.5 0.8 1.1 ## SI.SPR.PC40 SI.SPR.PC40.ZG SI.SPR.PCAP SI.SPR.PCAP.ZG urban_poverty_headcount ## 1 NA NA NA NA 27.6 ## 2 4.08 -1.2205 7.41 -1.3143 13.6 ## 3 NA NA NA NA 5.8 – 1.1.3.3 파트 2: R에서 수치형 프로파일링 수행 {#numerical-profiling-in-r} 다음 함수들을 살펴볼 것입니다: Hmisc 패키지의 describe funModeling 패키지의 profiling_num (전체 단변량 분석) 및 plot_num (히스토그램) 예시로 두 변수만 선택할 것입니다: library(Hmisc) # contains the `describe` function vars_to_profile=c(&quot;gini_index&quot;, &quot;poverty_headcount_1.9&quot;) data_subset=select(data_world_wide, any_of(vars_to_profile)) # Using the `describe` on a complete dataset. # It can be run with one variable; for example, describe(data_subset$poverty_headcount_1.9) describe(data_subset) ## data_subset ## ## 2 Variables 217 Observations ## -------------------------------------------------------------------------------- ## gini_index ## n missing distinct Info Mean pMedian Gmd .05 ## 140 77 136 1 38.8 38.39 9.594 26.81 ## .10 .25 .50 .75 .90 .95 ## 27.58 32.35 37.69 43.92 50.47 53.53 ## ## lowest : 24.09 25.59 25.9 26.12 26.13, highest: 56.24 60.46 60.79 60.97 63.38 ## -------------------------------------------------------------------------------- ## poverty_headcount_1.9 ## n missing distinct Info Mean pMedian Gmd .05 ## 116 101 107 1 18.33 15.18 23.56 0.025 ## .10 .25 .50 .75 .90 .95 ## 0.075 1.052 6.000 33.815 54.045 67.328 ## ## lowest : 0 0.01 0.03 0.04 0.06 , highest: 68.64 68.74 70.91 77.08 77.84 ## -------------------------------------------------------------------------------- poverty_headcount_1.9 (2011년 국제 가격 기준 하루 1.90달러 미만으로 생활하는 인구의 비율)를 다음과 같이 설명할 수 있습니다: n: 결측치가 없는 행의 수. 이 경우, 숫자를 포함하는 국가가 116개임을 나타냅니다. missing: 결측치 수. 이 지표를 n에 더하면 총 행 수가 됩니다. 거의 절반의 국가에 데이터가 없습니다. unique: 고유(또는 구분되는) 값의 수. Info: 변수에 존재하는 정보량의 추정치이며 이 시점에서는 중요하지 않습니다. Mean: 고전적인 평균. Numbers: .05, .10, .25, .50, .75, .90, .95는 백분위수를 나타냅니다. 이 값들은 분포를 설명하는 데 매우 유용합니다. 나중에 심층적으로 다룰 것입니다. 예를 들어, .05는 5번째 백분위수입니다. lowest 및 highest: 가장 낮은/가장 높은 다섯 가지 값. 여기서 이상치와 데이터 오류를 발견할 수 있습니다. 예를 들어, 변수가 백분율을 나타낸다면 음수 값을 포함할 수 없습니다. 다음 함수는 profiling_num이며, 데이터 프레임을 받아들여 방대한 테이블을 검색합니다. 이 테이블은 마치 영화 ’매트릭스’에서 볼 수 있는 것과 유사하게 수많은 지표의 바다 속에서 압도당하기 쉽습니다. Figure 1.5: 데이터의 매트릭스 영화 “매트릭스”(1999). 워쇼스키 형제(감독). 다음 테이블의 목적은 사용자에게 완벽한 지표 세트를 제공하여, 연구에 어떤 지표를 선택할지 결정할 수 있도록 하는 것입니다. 참고: 모든 지표 뒤에는 많은 통계 이론이 있습니다. 여기서는 개념을 소개하기 위해 작고 매우 단순화된 접근 방식을 다룰 것입니다. library(funModeling) # Full numerical profiling in one function automatically excludes non-numerical variables profiling_num(data_world_wide) ## variable mean std_dev variation_coef p_01 ## 1 gini_index 38.798571 8.4918157 0.21886929 25.710900 ## 2 pop_living_slums 45.698958 23.6568472 0.51766710 6.830000 ## 3 poverty_headcount_1.9 18.333707 22.7448672 1.24060384 0.000000 ## 4 rural_poverty_headcount 41.239393 21.9073185 0.53122311 2.902000 ## 5 SI.DST.02ND.20 10.940286 2.1659949 0.19798339 5.568000 ## 6 SI.DST.03RD.20 15.187571 2.0341395 0.13393448 9.137400 ## 7 SI.DST.04TH.20 21.456357 1.4915663 0.06951629 16.286100 ## 8 SI.DST.05TH.20 45.891714 7.1385549 0.15555215 35.004300 ## 9 SI.DST.10TH.10 30.510786 6.7513063 0.22127606 20.729200 ## 10 SI.DST.FRST.10 2.544571 0.8695044 0.34170958 0.915600 ## 11 SI.DST.FRST.20 6.524714 1.8737104 0.28717125 2.614300 ## 12 SI.POV.2DAY 32.370603 30.6360176 0.94641478 0.060500 ## 13 SI.POV.GAP2 14.163017 16.4036041 1.15819983 0.011500 ## 14 SI.POV.GAPS 6.892241 10.0965534 1.46491581 0.000000 ## 15 SI.POV.NAGP 12.246689 10.1167055 0.82607676 0.420700 ## 16 SI.POV.NAHC 30.695310 17.8844702 0.58264503 1.842000 ## 17 SI.POV.RUGP 15.870000 11.8254018 0.74514189 0.740000 ## 18 SI.POV.URGP 8.281972 8.2393371 0.99485211 0.300000 ## 19 SI.SPR.PC40 10.306375 9.7539668 0.94640131 0.856500 ## 20 SI.SPR.PC40.ZG 1.958495 3.6212194 1.84898052 -6.231772 ## 21 SI.SPR.PCAP 21.066000 17.4406999 0.82790752 2.426400 ## 22 SI.SPR.PCAP.ZG 1.457513 3.2074948 2.20066251 -5.897152 ## 23 urban_poverty_headcount 23.281195 15.0598876 0.64686918 0.579000 ## p_05 p_25 p_50 p_75 p_95 p_99 skewness kurtosis ## 1 26.81450 32.34750 37.6850 43.92250 53.53400 60.899800 0.55162475 2.874236 ## 2 10.75000 25.17500 46.2000 65.62500 83.37500 93.415000 0.08655596 2.045147 ## 3 0.02500 1.05250 6.0000 33.81500 67.32750 76.154500 1.12464564 2.947020 ## 4 6.46500 25.25000 38.1000 57.60000 75.77601 81.696000 0.05070376 1.967226 ## 5 7.36050 9.52750 11.1350 12.61250 14.20050 14.572200 -0.41139253 2.695430 ## 6 11.82750 13.87750 15.4600 16.69750 17.86200 18.144400 -0.87594286 3.843357 ## 7 18.28850 20.75750 21.9100 22.52250 23.01300 23.387600 -1.53741889 5.599431 ## 8 36.35950 40.49500 44.8250 49.82750 58.11500 65.894000 0.73763641 3.317412 ## 9 21.98850 25.71000 29.4950 34.11500 42.21150 50.616500 0.90532034 3.596310 ## 10 1.14750 1.88500 2.5400 3.23750 3.88200 4.346600 0.04275050 2.212272 ## 11 3.36950 5.09250 6.5050 8.00000 9.39200 9.996600 -0.11850862 2.232570 ## 12 0.39250 3.82750 20.2700 63.20500 84.57750 90.341000 0.53561716 1.757298 ## 13 0.08500 1.30500 5.5250 26.30000 48.53000 56.178500 1.06276529 2.871285 ## 14 0.00000 0.28750 1.4150 10.29250 31.45500 38.288000 1.65405890 4.701748 ## 15 1.22500 4.50000 8.6500 16.95000 32.42500 36.720000 1.12937489 3.982550 ## 16 6.43000 16.35000 26.6000 44.25000 62.98000 71.640000 0.52875485 2.375444 ## 17 1.65000 5.95000 13.6000 22.45000 37.70000 45.290000 0.80102636 2.955030 ## 18 0.90000 2.90000 6.3000 9.95000 25.15000 35.170000 2.31559531 9.765938 ## 19 1.22850 3.47500 6.9450 12.69250 28.86750 35.350200 1.25086949 3.366531 ## 20 -3.02107 -0.08445 1.7137 4.64300 7.92913 8.996918 -0.29366391 3.271338 ## 21 3.13750 8.00250 15.2750 25.41500 52.83700 67.168600 1.13166760 3.296606 ## 22 -3.80500 -0.48640 1.3242 3.55805 7.01655 8.481370 -0.01843620 3.593681 ## 23 3.14000 12.70750 20.1000 31.17500 51.03500 61.754000 0.72974377 2.973982 ## iqr range_98 range_80 ## 1 11.57500 [25.7109, 60.8998] [27.576, 50.469] ## 2 40.45000 [6.83, 93.415] [12.5, 75.2] ## 3 32.76250 [0, 76.1545] [0.075, 54.045] ## 4 32.35000 [2.902, 81.696] [13.99, 71.99] ## 5 3.08500 [5.568, 14.5722] [8.283, 13.802] ## 6 2.82000 [9.1374, 18.1444] [12.667, 17.503] ## 7 1.76500 [16.2861, 23.3876] [19.733, 22.813] ## 8 9.33250 [35.0043, 65.894] [36.995, 55.242] ## 9 8.40500 [20.7292, 50.6165] [22.569, 39.89] ## 10 1.35250 [0.9156, 4.3466] [1.479, 3.674] ## 11 2.90750 [2.6143, 9.9966] [3.986, 8.891] ## 12 59.37750 [0.0605, 90.341] [0.79, 78.29] ## 13 24.99500 [0.0115, 56.1785] [0.155, 40.85] ## 14 10.00500 [0, 38.288] [0.025, 23.45] ## 15 12.45000 [0.4207, 36.7200000000001] [1.85, 27] ## 16 27.90000 [1.842, 71.64] [9.86, 58.22] ## 17 16.50000 [0.74, 45.29] [3.3, 32.2] ## 18 7.05000 [0.3, 35.17] [1.3, 19.1] ## 19 9.21750 [0.8565, 35.3502] [1.809, 27.627] ## 20 4.72745 [-6.231772, 8.99691799999999] [-2.64254, 6.48298] ## 21 17.41250 [2.4264, 67.1686] [4.247, 49.224] ## 22 4.04445 [-5.897152, 8.48136999999999] [-2.0726, 5.17284] ## 23 18.46749 [0.579, 61.754] [5.98, 46.11] 각 지표에는 존재 이유가 있습니다: variable: 변수 이름 mean: 잘 알려진 평균. std_dev: 표준 편차, 평균값을 중심으로 한 분산 또는 퍼짐 정도를 나타내는 척도입니다. 0에 가까운 값은 거의 변동이 없음을 의미하며(따라서 상수에 가깝게 보임), 반대로 _높음_의 기준을 정하기는 어렵지만, 변동이 클수록 퍼짐이 더 크다고 말할 수 있습니다. 혼돈은 무한한 표준 편차처럼 보일 수 있습니다. 단위는 평균과 동일하여 비교 가능합니다. variation_coef: 변동 계수=std_dev/mean. std_dev는 절대값이므로, std_dev를 mean과 비교하여 상대적인 숫자로 나타내는 지표가 좋습니다. 0.22는 std_dev가 mean의 22%임을 나타냅니다. 0에 가까우면 변수가 평균을 중심으로 더 집중되는 경향이 있습니다. 두 분류기를 비교할 때, 정확도에서 std_dev와 variation_coef가 더 낮은 분류기를 선호할 수 있습니다. p_01, p_05, p_25, p_50, p_75, p_95, p_99: 백분위수 (1%, 5%, 25% 등). 이 장의 후반부에서 백분위수에 대한 완전한 검토가 있습니다. 백분위수에 대한 자세한 설명은 부록 1: 백분위수의 마법을 참조하십시오. skewness: 비대칭성을 측정하는 척도입니다. 0에 가까우면 분포가 평균을 중심으로 동일하게 분포(또는 대칭)되어 있음을 나타냅니다. 양수는 오른쪽에 긴 꼬리가 있음을 의미하며, 음수는 그 반대를 의미합니다. 이 섹션 뒤에 플롯에서 왜도를 확인하십시오. 변수 pop_living_slums는 0에 가깝고(“균등하게” 분포), poverty_headcount_1.9는 양수(오른쪽에 꼬리), SI.DST.04TH.20은 음수(왼쪽에 꼬리)입니다. 왜도가 0에서 멀어질수록 분포에 이상치가 있을 가능성이 높습니다. kurtosis: 분포의 꼬리를 설명합니다. 간단히 말해, 숫자가 높을수록 이상치의 존재를 나타낼 수 있습니다 (나중에 SI.POV.URGP 변수에서 50 근처에 이상치가 있는 것을 볼 수 있듯이). 왜도 및 첨도에 대한 완전한 검토는 참고 문헌 (McNeese 2016) 및 (Handbook 2013)를 참조하십시오. iqr: 사분위 범위는 백분위수 0.25와 0.75를 살펴본 결과이며, 동일한 변수 단위로 값의 50%의 분산 길이를 나타냅니다. 값이 높을수록 변수가 더 희소합니다. range_98 및 range_80: 값의 98%가 있는 범위를 나타냅니다. 하위 및 상위 1%를 제거합니다(따라서 98% 숫자). 잠재적인 이상치 없이 변수 범위를 아는 것이 좋습니다. 예를 들어, pop_living_slums는 0에서 76.15까지입니다. min 및 max 값을 비교하는 것보다 더 견고합니다. range_80은 range_98과 동일하지만 하위 및 상위 10%를 제외합니다. iqr, range_98, range_80은 백분위수를 기반으로 하며, 이 장의 후반부에서 다룰 것입니다. 중요: 모든 지표는 NA 값을 제거한 후에 계산됩니다. 그렇지 않으면 테이블이 NA로 채워질 것입니다. 1.1.3.3.1 profiling_num 사용 시 조언 profiling_num의 목표는 데이터 과학자에게 모든 종류의 지표를 제공하여 가장 적절한 것을 선택할 수 있도록 돕는 것입니다. 이는 dplyr 패키지의 select 함수를 사용하면 쉽게 할 수 있습니다. 또한, profiling_num은 결과를 자동으로 콘솔에 출력하지 않을 수 있습니다. 예를 들어, mean, p_01, p_99, range_80을 가져와 보겠습니다. my_profiling_table=profiling_num(data_world_wide) %&gt;% select(variable, mean, p_01, p_99, range_80) # Printing only the first three rows head(my_profiling_table, 3) ## variable mean p_01 p_99 range_80 ## 1 gini_index 38.79857 25.7109 60.8998 [27.576, 50.469] ## 2 pop_living_slums 45.69896 6.8300 93.4150 [12.5, 75.2] ## 3 poverty_headcount_1.9 18.33371 0.0000 76.1545 [0.075, 54.045] profiling_num은 테이블을 반환하므로, 우리가 설정한 조건에 따라 데이터를 빠르게 필터링할 수 있다는 점을 기억해주세요. 1.1.3.3.2 수치형 변수 플로팅을 통한 프로파일링 {#plotting-numerical-variable} funModeling의 또 다른 함수인 plot_num은 데이터셋을 받아 수치형 변수들의 분포를 플로팅하며, 비수치형 변수는 자동으로 제외합니다. plot_num(data_world_wide) Figure 1.6: 수치형 데이터 프로파일링 bins 매개변수(기본값 10)를 변경하여 플롯에 사용되는 막대 수를 조정할 수 있습니다. 예를 들어, plot_num(data_world_wide, bins = 20)와 같이 사용할 수 있습니다. 1.1.4 최종 생각 지금까지 많은 숫자들이 등장했습니다. 백분위수 부록에는 훨씬 더 많은 숫자가 있죠. 여기서 중요한 점은 데이터를 탐색하는 올바른 접근 방식을 찾는 것입니다. 이는 다른 지표나 다른 기준에서 비롯될 수 있습니다. df_status, describe, freq, profiling_num, plot_num 함수는 데이터 프로젝트 초기에 실행할 수 있습니다. 데이터의 정상 및 비정상적인 행동에 관해서는 둘 다 연구하는 것이 중요합니다. 데이터셋을 일반적인 용어로 설명하려면 극단값을 제외해야 합니다. 예를 들어, range_98 변수를 사용해서 말이죠. 극단값을 제외하면 평균은 감소해야 합니다. 이러한 분석은 단변량입니다. 즉, 다른 변수를 고려하지 않습니다(다변량 분석). 이는 이 책의 후반부에서 다룰 내용입니다. 한편, 입력(및 출력) 변수 간의 상관 관계는 상관 관계 장을 확인해 보세요. 1.2 상관 관계 및 관계 {https://www.google.com/search?q=%23correlation} 혼돈이 아름다움을 표현하는 만델브로트 프랙탈; 이미지 출처: Wikipedia. 1.2.1 이 장의 내용은 무엇일까요? 이 장은 변수에서 상관 관계를 측정하는 방법론적 및 실제적 측면을 모두 다룹니다. 우리는 ’상관 관계’라는 단어가 “함수적 관계”로 번역될 수 있음을 알게 될 것입니다. 방법론적인 측면에서는 Anscombe Quartet을 다룰 것입니다. 이는 공간적 분포는 다르지만 동일한 상관 관계 측정값을 공유하는 네 개의 플롯 세트입니다. 더 강력한 메트릭(MIC)을 통해 관계를 다시 계산하여 한 단계 더 나아갈 것입니다. 이 책에서는 수학적 수준으로 다루지는 않지만, 앞으로 다룰 예정인 정보 이론을 여러 번 언급할 것입니다. 딥 러닝을 포함하여 많은 알고리즘이 정보 이론을 기반으로 합니다. 낮은 차원(두 변수)과 소규모 데이터(소수의 행)에서 이러한 개념을 이해하면 고차원 데이터를 더 잘 이해하는 데 도움이 됩니다. 그럼에도 불구하고, 일부 실제 사례는 ‘작은’ 데이터일 뿐입니다. 실용적인 관점에서 볼 때, 자신의 데이터로 분석을 복제하고 멋진 플롯에서 관계를 프로파일링하고 시각화할 수 있을 것입니다. 이제 필요한 모든 라이브러리를 로드하는 것으로 시작하겠습니다. # 필요한 라이브러리 로드 library(funModeling) # heart_disease 데이터 포함 library(minerva) # MIC 통계량 포함 library(ggplot2) library(dplyr) library(reshape2) library(gridExtra) # 두 개의 플롯을 한 줄에 그릴 수 있도록 함 options(scipen=999) # 과학적 표기법 비활성화 1.2.2 선형 상관 관계 {#linear-correlation} 아마도 수치형 변수에 대한 가장 표준적인 상관 관계 측정값은 R 통계량(또는 피어슨 계수)일 것입니다. 이는 1(양의 상관 관계)에서 -1(음의 상관 관계)까지의 값을 가집니다. 0에 가까운 값은 상관 관계가 없음을 의미합니다. 대상 변수(예: 특성 공학을 수행하기 위함)를 기반으로 R 측정값을 계산하는 다음 예제를 고려해 봅시다. correlation_table 함수는 범주형/명목형 변수를 건너뛰고 모든 수치형 변수에 대한 R 메트릭을 검색합니다. correlation_table(data=heart_disease, target=&quot;has_heart_disease&quot;) ## Variable has_heart_disease ## 1 has_heart_disease 1.00 ## 2 heart_disease_severity 0.83 ## 3 num_vessels_flour 0.46 ## 4 oldpeak 0.42 ## 5 slope 0.34 ## 6 age 0.23 ## 7 resting_blood_pressure 0.15 ## 8 serum_cholestoral 0.08 ## 9 max_heart_rate -0.42 heart_disease_severity 변수는 가장 중요한 수치형 변수입니다. 이 값이 높을수록 심장 질환 발병 가능성이 높아집니다(양의 상관 관계). max_heart_rate는 음의 상관 관계를 가지므로 정반대입니다. 이 숫자를 제곱하면 R-제곱 통계량(일명 R2)이 반환되며, 0(상관 관계 없음)에서 1(높은 상관 관계)까지의 값을 가집니다. R 통계량은 이상치와 비선형 관계에 크게 영향을 받습니다. 1.2.2.1 앤스콤 쿼텟(Anscombe’s Quartet)의 상관 관계 앤스콤 쿼텟을 살펴보시죠. 위키백과의 내용을 인용합니다: 이들은 통계학자 프랜시스 앤스콤이 1973년에 데이터를 분석하기 전에 그래프로 표시하는 것의 중요성과 이상치가 통계적 속성에 미치는 영향을 모두 보여주기 위해 구성되었습니다. 1973년에 만들어졌지만 여전히 유효하다니, 정말 대단하죠. 이 네 가지 관계는 서로 다르지만, 모두 동일한 R2 값인 0.816을 가집니다. 다음 예시는 R2를 계산하고 각 쌍을 플롯합니다. # anscombe quartet 데이터 읽기 anscombe_data = read.delim(file=&quot;https://goo.gl/mVLz5L&quot;, header = T) # 모든 쌍에 대한 상관 관계 (R 제곱, R2) 계산 # 모든 값이 0.86으로 동일합니다. cor_1 = cor(anscombe_data$x1, anscombe_data$y1) cor_2 = cor(anscombe_data$x2, anscombe_data$y2) cor_3 = cor(anscombe_data$x3, anscombe_data$y3) cor_4 = cor(anscombe_data$x4, anscombe_data$y4) # 함수 정의 plot_anscombe &lt;- function(x, y, value, type) { # &#39;anscombe_data&#39;는 전역 변수입니다. 이것은 # 좋은 프로그래밍 습관이 아닙니다 ;) p=ggplot(anscombe_data, aes_string(x,y)) + geom_smooth(method=&#39;lm&#39;, fill=NA) + geom_point(aes(colour=factor(1), fill = factor(1)), shape=21, size = 2 ) + ylim(2, 13) + xlim(4, 19) + theme_minimal() + theme(legend.position=&quot;none&quot;) + annotate(&quot;text&quot;, x = 12, y =4.5, label = sprintf(&quot;%s: %s&quot;, type, round(value,2) ) ) return(p) } # 2x2 그리드에 플로팅 grid.arrange(plot_anscombe(&quot;x1&quot;, &quot;y1&quot;, cor_1, &quot;R2&quot;), plot_anscombe(&quot;x2&quot;, &quot;y2&quot;, cor_2, &quot;R2&quot;), plot_anscombe(&quot;x3&quot;, &quot;y3&quot;, cor_3, &quot;R2&quot;), plot_anscombe(&quot;x4&quot;, &quot;y4&quot;, cor_4, &quot;R2&quot;), ncol=2, nrow=2) ## Warning: `aes_string()` was deprecated in ggplot2 3.0.0. ## ℹ Please use tidy evaluation idioms with `aes()`. ## ℹ See also `vignette(&quot;ggplot2-in-packages&quot;)` for more information. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. Figure 1.7: 앤스콤 셋 이 네 가지 다른 플롯은 모든 x 및 y 변수(각각 9와 7.501)에 대해 동일한 평균과 동일한 정도의 상관 관계를 가집니다. summary(anscombe_data)를 입력하여 모든 측정값을 확인할 수 있습니다. 이것이 상관 관계를 분석할 때 관계를 플롯하는 것이 매우 중요한 이유입니다. 이 데이터는 나중에 다시 다룰 것입니다. 개선될 수 있거든요! 먼저, 정보 이론의 몇 가지 개념을 소개하겠습니다. 1.2.3 정보 이론에 기반한 상관 관계 이러한 관계는 정보 이론 개념을 통해 더 잘 측정할 수 있습니다. 이를 기반으로 상관 관계를 측정하는 여러 알고리즘 중 하나는 Maximal Information-based nonparametric exploration의 약어인 MINE입니다. R에서의 구현은 minerva 패키지에서 찾을 수 있습니다. Python과 같은 다른 언어에서도 사용할 수 있습니다. 1.2.3.1 R 예시: 완벽한 관계 함수(음의 지수 함수)를 기반으로 한 비선형 관계를 플롯하고 MIC 값을 인쇄해 보겠습니다. x=seq(0, 20, length.out=500) df_exp=data.frame(x=x, y=dexp(x, rate=0.65)) ggplot(df_exp, aes(x=x, y=y)) + geom_line(color=&#39;steelblue&#39;) + theme_minimal() Figure 1.8: 완벽한 관계 # [1,2] 위치는 두 변수의 상관 관계를 포함하며, 각 변수 자체와의 상관 관계는 제외됩니다. # 선형 상관 관계 계산 res_cor_R2=cor(df_exp)[1,2]^2 sprintf(&quot;R2: %s&quot;, round(res_cor_R2,2)) ## [1] &quot;R2: 0.39&quot; # 이제 MIC 측정값 계산 res_mine=mine(df_exp) sprintf(&quot;MIC: %s&quot;, res_mine$MIC[1,2]) ## [1] &quot;MIC: 1&quot; MIC 값은 0에서 1까지입니다. 0은 상관 관계가 없음을 의미하고 1은 가장 높은 상관 관계를 의미합니다. 해석은 R-제곱과 동일합니다. 1.2.3.2 결과 분석 MIC=1은 두 변수 사이에 완벽한 상관 관계가 있음을 나타냅니다. 만약 특성 공학을 수행하고 있다면 이 변수는 포함되어야 합니다. 단순한 상관 관계를 넘어 MIC가 말하는 것은 “이 두 변수는 함수적 관계를 보여줍니다”입니다. 기계 학습 용어(및 과도하게 단순화하자면): “변수 y는 변수 x에 종속되며, 그 관계를 모델링할 수 있는 함수(우리가 어떤 함수인지는 모르는)를 찾을 수 있습니다.” 이 관계는 실제로 함수, 즉 지수 함수를 기반으로 생성되었기 때문에 까다롭습니다. 하지만 다른 예제로 계속 진행해 봅시다… 1.2.4 노이즈 추가 노이즈는 원래 신호에 추가되는 원치 않는 신호입니다. 기계 학습에서 노이즈는 모델이 혼란스러워지는 데 일조합니다. 구체적으로 말하자면, 동일한 두 개의 입력 사례(예: 고객)가 서로 다른 결과(하나는 구매하고 다른 하나는 구매하지 않음)를 갖는 경우입니다. 이제 y_noise_1 변수를 생성하여 노이즈를 추가할 것입니다. df_exp$y_noise_1=jitter(df_exp$y, factor = 1000, amount = NULL) ggplot(df_exp, aes(x=x, y=y_noise_1)) + geom_line(color=&#39;steelblue&#39;) + theme_minimal() Figure 1.9: 노이즈 추가 상관 관계와 MIC를 다시 계산하고, 두 경우 모두 전체 행렬을 출력합니다. 이 행렬은 각 입력 변수가 자신을 포함한 다른 모든 변수에 대해 갖는 상관 관계/MIC 지표를 보여줍니다. # R 제곱 계산 res_R2=cor(df_exp)^2 res_R2 ## x y y_noise_1 ## x 1.0000000 0.3899148 0.3883930 ## y 0.3899148 1.0000000 0.9910369 ## y_noise_1 0.3883930 0.9910369 1.0000000 # mine 계산 res_mine_2=mine(df_exp) # MIC 출력 res_mine_2$MIC ## x y y_noise_1 ## x 1.0000000 1.0000000 0.7454237 ## y 1.0000000 1.0000000 0.7290665 ## y_noise_1 0.7454237 0.7290665 1.0000000 데이터에 노이즈를 추가하자 MIC 값은 1에서 0.7226365(-27%)로 감소했는데, 이는 훌륭합니다! R2도 0.3899148에서 0.3866319(-0.8%)로 약간 감소했습니다. 결론: MIC는 R2보다 노이즈가 있는 관계를 훨씬 더 잘 반영하며, 상관 관계가 있는 연관성을 찾는 데 유용합니다. 마지막 예시에 대해: 함수를 기반으로 데이터를 생성하는 것은 교육 목적으로만 사용됩니다. 그러나 변수의 노이즈 개념은 출처에 관계없이 거의 모든 데이터셋에서 매우 일반적입니다. 변수에 노이즈를 추가하기 위해 아무것도 할 필요가 없습니다. 이미 거기에 있습니다. 기계 학습 모델은 데이터의 실제 형태에 접근하여 이 노이즈를 처리합니다. 두 변수 간의 관계에 존재하는 정보의 의미를 파악하는 데 MIC 측정을 사용하는 것이 매우 유용합니다. 1.2.5 비선형성 측정 (MIC-R2) mine 함수는 여러 지표를 반환합니다. 우리는 MIC만 확인했지만, 알고리즘의 특성상(원본 논문 (Reshef et al. 2011)을 확인할 수 있습니다) 더 흥미로운 지표들을 계산합니다. res_mine_2 객체를 검사하여 모든 지표를 확인해 보세요. 그중 하나는 비선형성의 척도로 사용되는 MICR2입니다. 이는 MIC - R2를 수행하여 계산됩니다. R2가 선형성을 측정하므로, 높은 MICR2는 비선형 관계를 나타냅니다. MICR2를 수동으로 계산하여 확인할 수 있습니다. 다음 두 행렬은 동일한 결과를 반환합니다. # MIC r2: 비선형성 지표 round(res_mine_2$MICR2, 3) # MIC r2 수동 계산 round(res_mine_2$MIC-res_R2, 3) 비선형 관계는 모델을 구축하기가 더 어렵습니다. 특히 의사 결정 트리나 선형 회귀와 같은 선형 알고리즘을 사용하는 경우에는 더욱 그렇습니다. 다른 사람에게 관계를 설명해야 한다고 상상해 보세요. 그렇게 하려면 “더 많은 단어”가 필요할 것입니다. “A는 B가 증가함에 따라 증가하고 비율은 항상 3배입니다”(A=1이면 B=3, 선형)라고 말하는 것이 더 쉽습니다. 다음과 비교해 보세요: “A는 B가 증가함에 따라 증가하지만, B가 값 10에 도달할 때까지 A는 거의 0에 가깝다가 300으로 올라갑니다. 그리고 B가 15에 도달하면 A는 1000으로 갑니다.” # 데이터 예시 생성 df_example=data.frame(x=df_exp$x, y_exp=df_exp$y, y_linear=3*df_exp$x+2) # mine 지표 가져오기 res_mine_3=mine(df_example) # 결과를 출력할 라벨 생성 results_linear = sprintf(&quot;MIC: %s \\n MIC-R2 (비선형성): %s&quot;, res_mine_3$MIC[1,3], round(res_mine_3$MICR2[1,3],2) ) results_exp = sprintf(&quot;MIC: %s \\n MIC-R2 (비선형성): %s&quot;, res_mine_3$MIC[1,2], round(res_mine_3$MICR2[1,2],4) ) # 결과 플로팅 # 지수 변수 플롯 생성 p_exp=ggplot(df_example, aes(x=x, y=y_exp)) + geom_line(color=&#39;steelblue&#39;) + annotate(&quot;text&quot;, x = 11, y =0.4, label = results_exp) + theme_minimal() # 선형 변수 플롯 생성 p_linear=ggplot(df_example, aes(x=x, y=y_linear)) + geom_line(color=&#39;steelblue&#39;) + annotate(&quot;text&quot;, x = 8, y = 55, label = results_linear) + theme_minimal() grid.arrange(p_exp,p_linear,ncol=2) Figure 1.10: 관계 비교 두 플롯 모두 MIC=1을 가지는 완벽한 상관 관계(또는 관계)를 보여줍니다. 비선형성에 관해서는, MICR2는 예상대로 y_exp에서 0.6101, y_linear에서는 0으로 동작합니다. 이 점은 중요한데, MIC는 선형 관계에서 R2처럼 동작하며, 이전에 보았듯이 비선형 관계에도 매우 잘 적응하여 관계를 프로파일링하는 특정 점수 지표(MICR2)를 검색하기 때문입니다. 1.2.6 앤스콤 쿼텟에서 정보 측정하기 앞서 검토했던 예시를 기억하시나요? 앤스콤 쿼텟의 모든 쌍은 R2가 0.86으로 나옵니다. 그러나 플롯을 기반으로 볼 때 모든 쌍이 좋은 상관 관계나 유사한 x와 y의 분포를 보이는 것은 아니었습니다. 그러나 정보 이론에 기반한 메트릭으로 관계를 측정하면 어떻게 될까요? 네, 다시 MIC입니다. # 모든 쌍에 대한 MIC 계산 mic_1=mine(anscombe_data$x1, anscombe_data$y1, alpha=0.8)$MIC mic_2=mine(anscombe_data$x2, anscombe_data$y2, alpha=0.8)$MIC mic_3=mine(anscombe_data$x3, anscombe_data$y3, alpha=0.8)$MIC mic_4=mine(anscombe_data$x4, anscombe_data$y4, alpha=0.8)$MIC # 2x2 그리드에 MIC 플로팅 grid.arrange(plot_anscombe(&quot;x1&quot;, &quot;y1&quot;, mic_1, &quot;MIC&quot;), plot_anscombe(&quot;x2&quot;, &quot;y2&quot;, mic_2,&quot;MIC&quot;), plot_anscombe(&quot;x3&quot;, &quot;y3&quot;, mic_3,&quot;MIC&quot;), plot_anscombe(&quot;x4&quot;, &quot;y4&quot;, mic_4,&quot;MIC&quot;), ncol=2, nrow=2) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## `geom_smooth()` using formula = &#39;y ~ x&#39; Figure 1.11: MIC 통계량 보시다시피 alpha 값을 0.8로 늘렸습니다. 이는 작은 샘플을 분석할 때 문서에 따라 좋은 방법입니다. 기본값은 0.6이고 최대값은 1입니다. 이 경우 MIC 값은 쌍 x4 - y4에서 가장 가짜 관계를 발견했습니다. 플롯당 몇 개의 케이스(11행) 때문에 MIC가 다른 모든 쌍에 대해 동일했을 수 있습니다. 더 많은 케이스가 있으면 다른 MIC 값이 표시됩니다. 그러나 MIC를 MIC-R2(비선형성 측정)와 결합하면 새로운 통찰력이 나타납니다. # 모든 쌍에 대한 MIC 계산, 입력이 두 개의 벡터일 때는 &quot;MIC-R2&quot; 객체에 하이픈이 있고 데이터 프레임을 사용할 때는 &quot;MICR2&quot;인 것에 유의하세요. mic_r2_1=mine(anscombe_data$x1, anscombe_data$y1, alpha = 0.8)$`MIC-R2` mic_r2_2=mine(anscombe_data$x2, anscombe_data$y2, alpha = 0.8)$`MIC-R2` mic_r2_3=mine(anscombe_data$x3, anscombe_data$y3, alpha = 0.8)$`MIC-R2` mic_r2_4=mine(anscombe_data$x4, anscombe_data$y4, alpha = 0.8)$`MIC-R2` # mic_r2에 따라 정렬 df_mic_r2=data.frame(pair=c(1,2,3,4), mic_r2=c(mic_r2_1,mic_r2_2,mic_r2_3,mic_r2_4)) %&gt;% arrange(-mic_r2) df_mic_r2 ## pair mic_r2 ## 1 2 0.3277882 ## 2 3 0.3277062 ## 3 1 0.3274878 ## 4 4 -0.2272103 비선형성에 따라 내림차순으로 정렬하면 결과는 플롯과 일치합니다: 2 &gt; 3 &gt; 1 &gt; 4. 쌍 4에 대한 이상한 점, 음수입니다. 이는 MIC가 R2보다 낮기 때문입니다. 플롯할 가치가 있는 관계입니다. 1.2.7 비단조성 측정: MAS 측정 MINE는 MAS(최대 비대칭 점수)를 사용하여 시계열의 비단조성을 프로파일링하는 데에도 도움이 될 수 있습니다. 단조 시계열은 경향이 바뀌지 않고 항상 위 또는 아래로 이동하는 시계열입니다. 이에 대한 자세한 내용은 (Wikipedia 2017b)에서 확인할 수 있습니다. 다음 예시는 비단조 y_1와 단조 y_2의 두 시계열을 시뮬레이션합니다. # 샘플 데이터 생성 (시계열 시뮬레이션) time_x=sort(runif(n=1000, min=0, max=1)) y_1=4*(time_x-0.5)^2 y_2=4*(time_x-0.5)^3 # 두 시계열에 대한 MAS 계산 mas_y1=round(mine(time_x,y_1)$MAS,2) mas_y2=mine(time_x,y_2)$MAS # 모두 함께 넣기 df_mono=data.frame(time_x=time_x, y_1=y_1, y_2=y_2) # 플로팅 label_p_y_1 = sprintf(&quot;MAS=%s (goes down \\n and up =&gt; not-monotonic)&quot;, mas_y1) p_y_1=ggplot(df_mono, aes(x=time_x, y=y_1)) + geom_line(color=&#39;steelblue&#39;) + theme_minimal() + annotate(&quot;text&quot;, x = 0.45, y =0.75, label = label_p_y_1) label_p_y_2= sprintf(&quot;MAS=%s (goes up =&gt; monotonic)&quot;, mas_y2) p_y_2=ggplot(df_mono, aes(x=time_x, y=y_2)) + geom_line(color=&#39;steelblue&#39;) + theme_minimal() + annotate(&quot;text&quot;, x = 0.43, y =0.35, label = label_p_y_2) grid.arrange(p_y_1,p_y_2,ncol=2) Figure 1.12: 함수의 단조성 다른 관점에서 MAS는 주기적 관계를 감지하는 데에도 유용합니다. 예시를 통해 이를 설명해 보겠습니다. Figure 1.13: 함수의 주기성 1.2.7.1 더 실제적인 예시: 시계열 세 개의 시계열(y1, y2, y3)을 포함하는 다음 사례를 고려해 보겠습니다. 이들은 비단조성 또는 전체 성장 추세에 따라 프로파일링할 수 있습니다. # 데이터 읽기 df_time_series = read.delim(file=&quot;https://goo.gl/QDUjfd&quot;) # 플롯할 수 있도록 긴 형식으로 변환 df_time_series_long=melt(df_time_series, id=&quot;time&quot;) # 플로팅 plot_time_series = ggplot(data=df_time_series_long, aes(x=time, y=value, colour=variable)) + geom_line() + theme_minimal() + scale_color_brewer(palette=&quot;Set2&quot;) plot_time_series Figure 1.14: 시계열 예제 # 시계열 데이터에 대한 MAS 값 계산 및 출력 mine_ts=mine(df_time_series) mine_ts$MAS ## time y1 y2 y3 ## time 0.0000000 0.12031227 0.10538854 0.19115290 ## y1 0.1203123 0.00000000 0.06777813 0.08109624 ## y2 0.1053885 0.06777813 0.00000000 0.05700273 ## y3 0.1911529 0.08109624 0.05700273 0.00000000 time 열을 봐야 하므로 각 시계열의 시간에 대한 MAS 값을 얻었습니다. y2는 가장 단조적(그리고 가장 덜 주기적)인 시계열이며, 이를 보면 확인할 수 있습니다. 항상 올라가는 것처럼 보입니다. MAS 요약: MAS ~ 0은 단조 또는 비주기적 함수(“항상” 위 또는 아래로 이동)를 나타냅니다. MAS ~ 1은 비단조 또는 주기적 함수를 나타냅니다. 1.2.8 시계열 간의 상관 관계 MIC 메트릭은 시계열의 상관 관계도 측정할 수 있습니다. 일반적인 목적의 도구는 아니지만 서로 다른 시계열을 빠르게 비교하는 데 유용할 수 있습니다. 이 섹션은 MAS 예시에서 사용한 동일한 데이터를 기반으로 합니다. # 3개의 시계열 다시 출력 plot_time_series Figure 1.15: 시계열 예제 # MIC 값 출력 mine_ts$MIC ## time y1 y2 y3 ## time 1.0000000 0.3770052 0.6896763 0.3408671 ## y1 0.3770052 1.0000000 0.6174925 0.7094427 ## y2 0.6896763 0.6174925 1.0000000 0.5176504 ## y3 0.3408671 0.7094427 0.5176504 1.0000000 이제 y1 열을 봐야 합니다. MIC 측정값에 따르면 마지막 플롯에 표시된 것과 동일한 내용을 확인할 수 있습니다. y1은 y2(MIC=0.61)보다 y3(MIC=0.709)과 더 유사합니다. 1.2.8.1 더 나아가기: 동적 시간 워핑 MIC는 속도가 다른 시계열이 있는 더 복잡한 시나리오에는 유용하지 않습니다. 동적 시간 워핑(DTW) 기술을 사용해야 합니다. 개념을 시각적으로 파악하기 위해 이미지를 사용해 보겠습니다. Figure 1.16: 동적 시간 워핑 (DTW) 이미지 출처: Dynamic time wrapping Converting images into time series for data mining (Izbicki 2011). 마지막 이미지는 시계열을 비교하는 두 가지 다른 접근 방식을 보여주며, 유클리드는 MIC 측정값과 더 유사합니다. 반면 DTW는 서로 다른 시간에 발생하는 유사성을 추적할 수 있습니다. R에서의 멋진 구현: dtw 패키지. 시계열 간의 상관 관계를 찾는 것은 시계열 클러스터링을 수행하는 또 다른 방법입니다. 1.2.9 범주형 변수의 상관 관계 MINE와 다른 많은 알고리즘은 수치형 데이터에서만 작동합니다. 모든 범주형 변수를 플래그(또는 더미 변수)로 변환하는 데이터 준비 트릭을 수행해야 합니다. 원래 범주형 변수에 30개의 가능한 값이 있는 경우, 행에 해당 범주가 존재하는 경우 값 1을 보유하는 30개의 새 열이 생성됩니다. R의 caret 패키지를 사용하는 경우 이 변환은 두 줄의 코드만 필요합니다. library(caret) # 몇 개의 변수만 선택 heart_disease_2 = select(heart_disease, max_heart_rate, oldpeak, thal, chest_pain,exer_angina, has_heart_disease) # 범주형에서 수치형으로의 이 변환은 단순히 # 더 깔끔한 플롯을 만들기 위한 것입니다. heart_disease_2$has_heart_disease= ifelse(heart_disease_2$has_heart_disease==&quot;yes&quot;, 1, 0) # R의 모든 범주형 변수(factor 및 # character)를 수치형 변수로 변환합니다. # 원본을 건너뛰어 데이터를 사용할 준비가 되도록 합니다. dmy = dummyVars(&quot; ~ .&quot;, data = heart_disease_2) heart_disease_3 = data.frame(predict(dmy, newdata = heart_disease_2)) # 중요: 이 메시지를 받으면 # `Error: Missing values present in input variable &#39;x&#39;. # Consider using use = &#39;pairwise.complete.obs&#39;.` # 데이터에 결측값이 있기 때문입니다. # 먼저 영향 분석 없이 NA를 생략하지 마세요. # 이 경우에는 중요하지 않습니다. heart_disease_4=na.omit(heart_disease_3) # mic 계산! mine_res_hd=mine(heart_disease_4) 샘플 인쇄… mine_res_hd$MIC[1:5,1:5] ## max_heart_rate oldpeak thal.3 thal.6 thal.7 ## max_heart_rate 0.9999920 0.2440874 0.24448436 0.11965365 0.18392352 ## oldpeak 0.2440874 0.9999283 0.17511070 0.11086221 0.15706631 ## thal.3 0.2444844 0.1751107 0.99233512 0.07257079 0.70987597 ## thal.6 0.1196536 0.1108622 0.07257079 0.32665312 0.04419397 ## thal.7 0.1839235 0.1570663 0.70987597 0.04419397 0.96395831 여기서 thal.3 열은 thal=3일 때 1의 값을 가집니다. 1.2.9.1 멋진 플롯 인쇄! R에서 corrplot 패키지를 사용합니다. 이 패키지는 cor 객체(클래식 상관 관계 행렬) 또는 다른 행렬을 플롯할 수 있습니다. 이 경우 MIC 행렬을 플롯하지만, 예를 들어 MAS 또는 상관 관계의 제곱 행렬을 반환하는 다른 메트릭도 사용할 수 있습니다. 두 플롯은 동일한 데이터를 기반으로 하지만 상관 관계를 다른 방식으로 표시합니다. # 행렬을 플롯하는 라이브러리 library(corrplot) ## corrplot 0.95 loaded # color pallete brewer.pal을 사용합니다. library(RColorBrewer) # 대각선(변수 자체)을 제외한 # 스케일의 최대값을 시각화하는 해킹 diag(mine_res_hd$MIC)=0 # 원이 있는 상관 관계 플롯. corrplot(mine_res_hd$MIC, method=&quot;circle&quot;, col=brewer.pal(n=10, name=&quot;PuOr&quot;), # 상단 대각선만 표시 type=&quot;lower&quot;, # 레이블 색상, 크기 및 회전 tl.col=&quot;red&quot;, tl.cex = 0.9, tl.srt=90, # 대각선(변수 자체)을 인쇄하지 않음 diag=FALSE, # 모든 행렬(이 경우 mic)을 허용합니다. #(상관 요소가 아님) is.corr = F ) Figure 1.17: 상관 관계 플롯 # 색상 및 상관 관계 MIC가 있는 상관 관계 플롯 corrplot(mine_res_hd$MIC, method=&quot;color&quot;, type=&quot;lower&quot;, number.cex=0.7, # 상관 계수 추가 addCoef.col = &quot;black&quot;, tl.col=&quot;red&quot;, tl.srt=90, tl.cex = 0.9, diag=FALSE, is.corr = F ) Figure 1.18: 상관 관계 플롯 첫 번째 매개변수 -mine_res_hd$MIC-를 원하는 행렬로 변경하고 자신의 데이터와 함께 다시 사용하면 됩니다. 1.2.9.2 이러한 종류의 플롯에 대한 의견 이러한 플롯은 변수 수가 많지 않을 때만 유용합니다. 또는 모든 변수가 수치형이어야 한다는 점을 염두에 두고 먼저 변수 선택을 수행하는 경우에 유용합니다. 선택에 범주형 변수가 있다면 먼저 이를 수치형으로 변환한 다음 변수 간의 관계를 검토하여 특정 범주형 변수의 값이 특정 결과와 어떻게 더 관련되어 있는지 (이 경우처럼) 미리 엿볼 수 있습니다. 1.2.9.3 플롯에서 얻을 수 있는 몇 가지 통찰력은 어떠한가요? 예측하려는 변수가 has_heart_disease이므로 흥미로운 점이 나타납니다. 심장 질환을 앓는 것은 thal=6 값보다 thal=3과 더 밀접하게 관련되어 있습니다. 변수 chest_pain에 대한 동일한 분석에서, 4의 값은 1의 값보다 더 위험합니다. 다른 플롯으로 확인할 수도 있습니다. cross_plot(heart_disease, input = &quot;chest_pain&quot;, target = &quot;has_heart_disease&quot;, plot_type = &quot;percentual&quot;) Figure 1.19: cross-plot을 사용한 시각적 분석 환자가 chest_pain=4인 경우 심장 질환 발병 가능성은 72.9%입니다. chest_pain=1인 경우(72.9% 대 30.4%)보다 2배 이상 높습니다. 몇 가지 생각… 데이터는 동일하지만 데이터를 탐색하는 접근 방식은 다릅니다. 예측 모델을 만들 때도 마찬가지입니다. N차원 공간의 입력 데이터는 서포트 벡터 머신, 랜덤 포레스트 등 다양한 모델을 통해 접근할 수 있습니다. 마치 사진가가 다른 각도나 다른 카메라로 촬영하는 것과 같습니다. 대상은 항상 동일하지만, 관점에 따라 다른 정보를 제공합니다. 원시 테이블과 다양한 플롯을 결합하면 더욱 현실적이고 보완적인 객관적인 시각을 얻을 수 있습니다. 1.2.10 정보 이론에 기반한 상관 관계 분석 {#selecting_best_vars_mic} MIC 측정값을 기반으로 mine 함수는 예측할 열의 인덱스(또는 단 하나의 변수에 대한 모든 상관 관계를 얻기 위해)를 받을 수 있습니다. # 예측할 변수의 인덱스 가져오기: has_heart_disease target=&quot;has_heart_disease&quot; index_target=grep(target, colnames(heart_disease_4)) # master는 모든 상관 관계를 계산하기 위해 # 인덱스 열 번호를 가져옵니다. mic_predictive=mine(heart_disease_4, master = index_target)$MIC # 결과를 포함하는 데이터 프레임 생성, # 상관 관계에 따라 내림차순으로 정렬하고 # 대상 자체와의 상관 관계 제외 df_predictive = data.frame(variable=rownames(mic_predictive), mic=mic_predictive[,1], stringsAsFactors = F) %&gt;% arrange(-mic) %&gt;% filter(variable!=target) # MIC 측정값을 기반으로 변수 중요도를 보여주는 # 다채로운 플롯 생성 ggplot(df_predictive, aes(x=reorder(variable, mic),y=mic, fill=variable) ) + geom_bar(stat=&#39;identity&#39;) + coord_flip() + theme_bw() + xlab(&quot;&quot;) + ylab(&quot;Variable Importance (based on MIC)&quot;) + guides(fill=FALSE) Figure 1.20: 정보 이론을 사용한 상관 관계 상관 관계가 있는 입력 특성을 제외하기 위해 모든 변수 간의 상관 관계를 실행하는 것이 권장되긴 합니다. 1.2.10.1 mine 사용을 위한 실용적인 조언 완료하는 데 너무 오랜 시간이 걸린다면 샘플링을 고려하십시오. 데이터 양이 너무 적다면 alpha 매개변수에 더 높은 숫자를 설정하는 것을 고려하십시오. 기본값은 0.6입니다. 또한, 코어가 4개인 경우 n.cores=3으로 설정하여 병렬로 실행할 수 있습니다. 병렬 프로세스를 실행할 때 운영 체제에서 추가 코어를 사용하므로 이는 일반적인 좋은 방법입니다. 1.2.11 MINE만 이런 기능을 제공하나요? 아닙니다. 우리는 MINE 스위트만 사용했지만, 상호 정보와 관련된 다른 알고리즘도 있습니다. R에서는 entropy 및 infotheo 패키지 등이 있습니다. funModeling 패키지(버전 1.6.6부터)는 섹션 정보 이론을 사용하여 최고의 특성 순위 지정에서 본 것처럼 여러 정보 이론 메트릭을 계산하는 var_rank_info 함수를 도입합니다. Python에서는 scikit-learn을 통해 상호 정보를 계산할 수 있습니다. 여기 예제가 있습니다. 개념은 도구를 초월합니다. 1.2.11.1 또 다른 상관 관계 예제 (상호 정보) 이번에는 infotheo 패키지를 사용할 것입니다. 먼저 패키지에 있는 discretize 함수(또는 bining)를 적용하는 데이터 준비 단계를 수행해야 합니다. 이 함수는 모든 수치형 변수를 동일 빈도 기준에 따라 범주형으로 변환합니다. 다음 코드는 이전에 본 것처럼 상관 관계 행렬을 생성하지만, 상호 정보 지수를 기반으로 합니다. library(infotheo) # 모든 변수 이산화 heart_disease_4_disc=discretize(heart_disease_4) # 상호 정보를 기반으로 &quot;상관 관계&quot; 계산 heart_info=mutinformation(heart_disease_4_disc, method= &quot;emp&quot;) # 대각선(변수 자체)을 제외한 스케일의 최대값을 시각화하는 해킹 diag(heart_info)=0 # Infotheo 패키지의 색상 및 상관 관계 상호 정보가 있는 상관 관계 플롯. 이 줄은 오른쪽의 플롯만 검색합니다. corrplot(heart_info, method=&quot;color&quot;,type=&quot;lower&quot;, number.cex=0.6,addCoef.col = &quot;black&quot;, tl.col=&quot;red&quot;, tl.srt=90, tl.cex = 0.9, diag=FALSE, is.corr = F) Figure 1.21: 변수 중요도 비교 상호 정보에 기반한 상관 관계 점수는 MIC와 매우 유사하게 관계 순위를 매기지 않습니까? 1.2.12 정보 측정: 일반적인 관점 상관 관계 외에도, MIC 또는 기타 정보 측정 지표는 _함수적 관계_가 있는지 여부를 측정합니다. 높은 MIC 값은 두 변수 간의 관계가 함수로 설명될 수 있음을 나타냅니다. 그 함수 또는 예측 모델을 찾는 것이 우리의 임무입니다. 이 분석은 N-변수로 확장되며, 이 책은 최상의 변수 선택 장에서 다른 알고리즘을 소개합니다. 일부 예측 모델은 다른 모델보다 성능이 좋지만, 관계가 완전히 노이즈가 많다면 아무리 발전된 알고리즘이라도 결국 좋지 않은 결과로 이어질 것입니다. 정보 이론에 대한 더 많은 내용이 나올 것입니다. 지금은 다음 교육용 강의를 확인하십시오: 7분 입문 영상 https://www.youtube.com/watch?v=2s3aJfRr9gE  http://alex.smola.org/teaching/cmu2013-10-701x/slides/R8-information_theory.pdf http://www.scholarpedia.org/article/Mutual_information 1.2.13 결론 앤스콤 쿼텟은 플롯과 함께 _원시 통계량_을 얻는 좋은 방법을 가르쳐주었습니다. 우리는 노이즈가 두 변수 간의 관계에 어떻게 영향을 미칠 수 있는지 알 수 있었고, 이러한 현상은 항상 데이터에 나타납니다. 데이터의 노이즈는 예측 모델을 혼란스럽게 만듭니다. 노이즈는 오류와 관련이 있으며, 정보 이론에 기반한 측정값(예: 상호 정보 및 최대 정보 계수)을 통해 연구할 수 있으며, 이는 일반적인 R 제곱보다 한 단계 더 나아갑니다. (Caban et al. 2012)에는 MINE를 특성 선택기로 사용하는 임상 연구가 있습니다. 이러한 방법은 가장 중요한 변수의 순위를 매기기 위해 예측 모델에 의존하지 않는 방법으로 특성 공학에 적용할 수 있습니다. 또한 시계열을 클러스터링하는 데에도 적용할 수 있습니다. 더 읽어보기: 최상의 변수 선택 참고 문헌 (References) Caban, Jesus J., Ulas Bagci, Alem Mehari, Shoaib Alam, Joseph R. Fontana, Gregory J. Kato, and Daniel J. Mollura. 2012. “Characterizing Non-Linear Dependencies Among Pairs of Clinical Variables and Imaging Data.” Conf Proc IEEE Eng Med Biol Soc 2012 (August): 2700–2703. https://doi.org/10.1109/EMBC.2012.6346521. Handbook, Engineering Statistics. 2013. “Measures of Skewness and Kurtosis.” http://www.itl.nist.gov/div898/handbook/eda/section3/eda35b.htm. Izbicki, Mike. 2011. “Converting Images into Time Series for Data Mining.” https://izbicki.me/blog/converting-images-into-time-series-for-data-mining.html. McNeese, Bill. 2016. “Are the Skewness and Kurtosis Useful Statistics?” https://www.spcforexcel.com/knowledge/basic-statistics/are-skewness-and-kurtosis-useful-statistics. Reshef, David N., Yakir A. Reshef, Hilary K. Finucane, Sharon R. Grossman, Gilean McVean, Peter J. Turnbaugh, Eric S. Lander, Michael Mitzenmacher, and Pardis C. Sabeti. 2011. “Detecting Novel Associations in Large Data Sets.” Science 334 (6062): 1518–24. https://doi.org/10.1126/science.1205438. ———. 2017b. “Monotonic Function.” https://en.wikipedia.org/wiki/Monotonic_function. "],["data_preparation.html", "2 데이터 준비 2.1 데이터 유형 처리 2.2 기술 통계에서의 고카디널리티 변수 2.3 High Cardinality Variable in Predictive Modeling 2.4 이상치 처리 (Treatment of outliers) 2.5 결측 데이터: 분석, 처리 및 대치 2.6 시간과 관련된 고려 사항", " 2 데이터 준비 2.1 데이터 유형 처리 2.1.1 무엇에 대한 내용인가요? 데이터 프로젝트를 시작할 때 가장 먼저 해야 할 일 중 하나는 각 변수에 올바른 데이터 유형을 할당하는 것입니다. 이는 간단해 보이는 작업이지만, 일부 알고리즘은 특정 데이터 유형으로 작동합니다. 여기서는 각 경우의 의미를 예시와 함께 설명하면서 이러한 변환을 다루려고 합니다. Figure 2.1: 피보나치 나선 피보나치 수열. 자연과 인체에 존재하는 숫자의 순서. 이 장에서 무엇을 검토할 예정인가요? 올바른 데이터 유형 감지 범주형을 숫자형으로 변환하는 방법 숫자형을 범주형으로 변환하는 방법 (이산화 방법) 이론 및 실제적 측면 (R 예제) 예측 모델이 숫자형 변수를 보는 방식 2.1.2 데이터 유형의 세계 주요 데이터 유형은 숫자형과 범주형 두 가지입니다. 범주형의 다른 이름으로는 문자열과 명목형이 있습니다. 범주형의 하위 집합으로 순서형 또는 R에서 정렬된(ordered) 팩터라고 불리는 유형이 있습니다. 적어도 R에서는 이 유형은 특정 순서로 범주를 그릴 때만 관련이 있습니다. R 예시는 다음과 같습니다. # 순서형 또는 정렬된 팩터 생성 var_factor=factor(c(&quot;3_high&quot;, &quot;2_mid&quot;, &quot;1_low&quot;)) var_ordered=factor(var_factor, ordered = T) var_ordered 숫자형과 범주형이 가장 필요하므로 이 데이터 유형에는 너무 많은 주의를 기울이지 마십시오. 2.1.2.1 이진 변수, 숫자형 또는 범주형? 이 책은 0이 FALSE이고 1이 TRUE일 때 이진 변수를 숫자형으로 사용할 것을 제안합니다. 이렇게 하면 데이터를 프로파일링하기가 더 쉽습니다. 2.1.3 알고리즘별 데이터 유형 일부 알고리즘은 다음과 같이 작동합니다. 📊 범주형 데이터만 사용 📏 숫자형 데이터만 사용 📊📏 두 가지 유형 모두 사용 또한, 모든 예측 모델이 결측값을 처리할 수 있는 것은 아닙니다. 데이터 사이언스 라이브 북은 이 모든 상황을 다루려고 합니다. 2.1.4 범주형 변수를 숫자형으로 변환하기 R의 caret 패키지를 사용하면 모든 범주형 변수를 플래그 변수(더미 변수라고도 함)로 변환하는 것은 간단한 작업입니다. 원래 범주형 변수에 30개의 가능한 값이 있다면, 30개의 새 열이 생성되어 0 또는 1 값을 가지게 되며, 여기서 1은 해당 행에 해당 범주가 존재함을 나타냅니다. R의 caret 패키지를 사용하면 이 변환은 단 두 줄의 코드만 필요합니다. library(caret) # dummyVars 함수 포함 library(dplyr) # 데이터 정제 라이브러리 library(funModeling) # df_status 함수 포함 # 범주형 변수 확인 status=df_status(heart_disease) ## variable q_zeros p_zeros q_na p_na q_inf p_inf type unique ## 1 age 0 0.00 0 0.00 0 0 integer 41 ## 2 gender 0 0.00 0 0.00 0 0 factor 2 ## 3 chest_pain 0 0.00 0 0.00 0 0 factor 4 ## 4 resting_blood_pressure 0 0.00 0 0.00 0 0 integer 50 ## 5 serum_cholestoral 0 0.00 0 0.00 0 0 integer 152 ## 6 fasting_blood_sugar 258 85.15 0 0.00 0 0 factor 2 ## 7 resting_electro 151 49.83 0 0.00 0 0 factor 3 ## 8 max_heart_rate 0 0.00 0 0.00 0 0 integer 91 ## 9 exer_angina 204 67.33 0 0.00 0 0 integer 2 ## 10 oldpeak 99 32.67 0 0.00 0 0 numeric 40 ## 11 slope 0 0.00 0 0.00 0 0 integer 3 ## 12 num_vessels_flour 176 58.09 4 1.32 0 0 integer 4 ## 13 thal 0 0.00 2 0.66 0 0 factor 3 ## 14 heart_disease_severity 164 54.13 0 0.00 0 0 integer 5 ## 15 exter_angina 204 67.33 0 0.00 0 0 factor 2 ## 16 has_heart_disease 0 0.00 0 0.00 0 0 factor 2 filter(status, type %in% c(&quot;factor&quot;, &quot;character&quot;)) %&gt;% select(variable) ## variable ## 1 gender ## 2 chest_pain ## 3 fasting_blood_sugar ## 4 resting_electro ## 5 thal ## 6 exter_angina ## 7 has_heart_disease # 모든 범주형 변수(팩터 및 문자열)를 수치형 변수로 변환합니다. # 기존 변수를 생략하므로 변환 후에 제거할 필요가 없으며, 데이터가 바로 사용 가능합니다. dmy = dummyVars(&quot; ~ .&quot;, data = heart_disease) heart_disease_2 = data.frame(predict(dmy, newdata = heart_disease)) # 새로운 수치형 데이터셋 확인: colnames(heart_disease_2) ## [1] &quot;age&quot; &quot;gender.female&quot; &quot;gender.male&quot; ## [4] &quot;chest_pain.1&quot; &quot;chest_pain.2&quot; &quot;chest_pain.3&quot; ## [7] &quot;chest_pain.4&quot; &quot;resting_blood_pressure&quot; &quot;serum_cholestoral&quot; ## [10] &quot;fasting_blood_sugar.0&quot; &quot;fasting_blood_sugar.1&quot; &quot;resting_electro.0&quot; ## [13] &quot;resting_electro.1&quot; &quot;resting_electro.2&quot; &quot;max_heart_rate&quot; ## [16] &quot;exer_angina&quot; &quot;oldpeak&quot; &quot;slope&quot; ## [19] &quot;num_vessels_flour&quot; &quot;thal.3&quot; &quot;thal.6&quot; ## [22] &quot;thal.7&quot; &quot;heart_disease_severity&quot; &quot;exter_angina.0&quot; ## [25] &quot;exter_angina.1&quot; &quot;has_heart_disease.no&quot; &quot;has_heart_disease.yes&quot; 원본 데이터 heart_disease는 범주형 변수 없이 수치형 및 더미 변수만 포함된 heart_disease_2로 변환되었습니다. 새로 생성된 각 변수 명에는 점(.) 뒤에 해당 값이 붙습니다. 예를 들어, 4가지 값(1, 2, 3, 4)을 가질 수 있는 chest_pain 변수의 7번째 환자(행)에 대해 변환 전후를 확인하면 다음과 같습니다. # 변환 전 as.numeric(heart_disease[7, &quot;chest_pain&quot;]) ## [1] 4 # 변환 후 heart_disease_2[7, c(&quot;chest_pain.1&quot;, &quot;chest_pain.2&quot;, &quot;chest_pain.3&quot;, &quot;chest_pain.4&quot;)] ## chest_pain.1 chest_pain.2 chest_pain.3 chest_pain.4 ## 7 0 0 0 1 수치형 변수만 유지 및 변형하고 명목형 변수를 제외함으로써, heart_disease_2 데이터는 분석에 바로 사용할 준비가 되었습니다. dummyVars에 대한 더 자세한 정보: http://amunategui.github.io/dummyVar-Walkthrough/ 2.1.5 범주형인가요, 수치형인가요? 생각해 봅시다. 1, 2, 3, 4의 값을 가질 수 있는 chest_pain 변수를 생각해 보세요. 이 변수는 범주형인가요, 수치형인가요? 만약 값이 순서가 있다면, 즉 1 &lt; 2 &lt; 3 &lt; 4와 같이 순서(order)를 나타낸다면 수치형으로 고려될 수 있습니다. 의사결정 트리 모델을 만든다면, “If chest_pain &gt; 2.5, then...”과 같은 규칙을 발견할 수 있습니다. 이게 말이 될까요? 알고리즘은 실제 존재하지 않는 값(2.5)을 기준으로 변수를 분할하지만, 우리는 이를 “만약 chest_pain이 3 이상이라면…”으로 해석합니다. 2.1.5.1 알고리즘처럼 생각하기 두 개의 수치형 입력 변수와 하나의 이진 타겟 변수를 생각해 봅시다. 알고리즘은 각 숫자 사이에 무한한 값이 존재한다고 가정하고 두 입력 변수를 평면 위의 점으로 _인식_합니다. 예를 들어, 서포트 벡터 머신(SVM)은 타겟 변수의 클래스를 분리하기 위해 여러 개의 벡터를 생성합니다. 이 벡터들을 기반으로 영역(regions)을 찾습니다. 범주형 변수를 기반으로 이러한 영역을 찾는 것이 가능할까요? 불가능합니다. 이것이 바로 SVM이 인공 신경망과 마찬가지로 수치형 변수만 지원하는 이유입니다. Figure 2.2: 서포트 벡터 머신 (SVM) 이미지 출처: ZackWeinberg 위 이미지는 세 개의 선을 보여주며, 이는 서로 다른 세 개의 결정 경계 또는 영역을 나타냅니다. SVM 개념에 대한 빠른 소개를 보려면 다음 짧은 영상을 참고하세요: SVM Demo. 하지만 의사결정 트리, 랜덤 포레스트, 그레디언트 부스팅 머신과 같은 트리 기반 모델은 그 검색 공간이 영역(SVM과 동일)뿐만 아니라 범주일 수도 있기 때문에 두 가지 유형을 모두 처리할 수 있습니다. 예: “if postal_code is AX441AG and age &gt; 55, then...”. 다시 심장 질환 예제로 돌아가서, chest_pain 변수는 순서를 가지고 있습니다. 이 점을 활용해야 합니다. 왜냐하면 이를 범주형 변수로 변환하면 정보를 잃게 되기 때문이며, 이는 데이터 유형을 처리할 때 중요한 포인트입니다. 2.1.5.2 모든 것을 범주형으로 처리하는 것이 해결책일까요? 아니요… 수치형 변수는 순서 덕분에 명목형 변수보다 더 많은 정보를 담고 있습니다. 범주형 변수에서는 값들을 직접 비교할 수 없습니다. 예를 들어, If postal code is higher than \"AX2004-P\"와 같은 규칙을 만드는 것은 불가능합니다. 명목형 변수의 값들은 참조할 다른 변수(보통 예측할 결과값)가 있을 때만 비교될 수 있습니다. 예를 들어, 우편 번호 “AX2004-P”가 “MA3942-H”보다 더 높은 이유는 해당 지역에 사진 강의 수강에 관심 있는 사람들이 더 많기 때문일 수 있습니다. 또한, 고카디널리티(high cardinality)는 범주형 변수에서 문제가 될 수 있습니다. 예를 들어, 수백 개의 다른 값을 포함하는 postal code 변수가 그렇습니다. 이 책은 기술 통계량 및 예측 모델링 장에서 고카디널리티 변수를 처리하는 방법을 다룹니다. 어쨌든 모든 변수를 범주형으로 변환하고 어떤 일이 일어나는지 직접 _무료 테스트_를 해볼 수 있습니다. 수치형 변수일 때의 결과와 비교해 보세요. 테스트를 위해 Kappa나 ROC 통계량과 같은 좋은 오차 측정 지표를 사용하고, 결과를 교차 검증하는 것을 잊지 마세요. 2.1.5.3 범주형을 수치형 변수로 변환할 때 주의할 점 범주형 변수를 수치형으로 변환해야 하는 상황을 가정해 봅시다. 이전 사례와 같이 각 범주에 다른 숫자를 할당하는 변환(transformation)을 시도해 보겠습니다. 이러한 변환을 할 때는 변수에 순서를 도입하게 되므로 주의해야 합니다. 네 개의 행을 가진 다음 데이터를 고려해 보세요. 처음 두 변수는 visits와 postal_code입니다 (이는 두 개의 입력 변수이거나, visits를 입력으로 하고 postal_code를 출력으로 사용할 때 모두 해당됩니다). 다음 코드는 두 가지 기준에 따라 변환된 postal_code에 따른 visits를 보여줍니다. transformation_1: 주어진 순서에 따라 일련번호를 할당. transformation_2: visits의 수에 따라 숫자를 할당. # creating data -toy- sample df_pc=data.frame(visits=c(10, 59, 27, 33), postal_code=c(&quot;AA1&quot;, &quot;BA5&quot;, &quot;CG3&quot;, &quot;HJ1&quot;), transformation_1=c(1,2,3,4), transformation_2=c(1, 4, 2, 3 )) # printing table knitr::kable(df_pc) visits postal_code transformation_1 transformation_2 10 AA1 1 1 59 BA5 2 4 27 CG3 3 2 33 HJ1 4 3 library(gridExtra) # transformation 1 plot_1=ggplot(df_pc, aes(x=transformation_1, y=visits, label=postal_code)) + geom_point(aes(color=postal_code), size=4)+ geom_smooth(method=loess, group=1, se=FALSE, color=&quot;lightblue&quot;, linetype=&quot;dashed&quot;) + theme_minimal() + theme(legend.position=&quot;none&quot;) + geom_label(aes(fill = factor(postal_code)), colour = &quot;white&quot;, fontface = &quot;bold&quot;) # transformation 2 plot_2=ggplot(df_pc, aes(x=transformation_2, y=visits, label=postal_code)) + geom_point(aes(color=postal_code), size=4)+ geom_smooth(method=lm, group=1, se=FALSE, color=&quot;lightblue&quot;, linetype=&quot;dashed&quot;) + theme_minimal() + theme(legend.position=&quot;none&quot;) + geom_label(aes(fill = factor(postal_code)), colour = &quot;white&quot;, fontface = &quot;bold&quot;) # 여러 플롯을 나란히 배치 grid.arrange(plot_1, plot_2, ncol=2) Figure 2.3: 데이터 변환 비교 네 개의 행으로 예측 모델을 구축하는 사람은 아무도 없겠지만, 이 예제의 의도는 관계가 비선형(transformation_1)에서 선형(transformation_2)으로 어떻게 변하는지 보여주기 위함입니다. 이는 예측 모델이 관계를 더 쉽게 설명하고 파악할 수 있게 해줍니다. 데이터 행이 수백만 개로 늘어나고 변수 수가 수백 개로 확장되어도 효과는 동일합니다. 작은 데이터로부터 배우는 것은 이러한 경우에 올바른 접근 방식입니다. 2.1.6 수치형 변수 이산화 (Discretizing numerical variables) 이 프로세스는 데이터를 구간(bin)으로 나누어 범주형으로 변환하는 과정입니다. 멋진 정의를 위해 _위키백과_를 인용하자면: 이산화(Discretization)는 연속적인 함수, 모델, 방정식을 이산적인 대응물로 옮기는 과정과 관련이 있습니다. 구간(bins)은 버킷(buckets) 또는 세그먼트(segments)라고도 불립니다. 예제를 계속 살펴보겠습니다. 2.1.6.1 데이터에 대하여 이 데이터는 발육 부진(stunted) 아동의 비율에 대한 정보를 담고 있습니다. 이상적인 값은 0입니다. 이 지표는 발육 부진으로 고통받는 5세 미만 아동의 비율을 반영합니다. 발육이 부진한 아동은 질병과 사망의 위험이 더 큽니다. 데이터 출처: ourworldindata.org, hunger and undernourishment. 먼저, 간단한 데이터 준비를 해야 합니다. 각 행은 국가-연도 쌍을 나타내므로, 국가별로 가장 최신 지표 값을 가져와야 합니다. data_stunting=read.csv(file = &quot;https://goo.gl/hFEUfN&quot;, header = T, stringsAsFactors = F) # renaming the metric data_stunting= dplyr::rename( data_stunting, share_stunted_child= Share.of.stunted.children.under.5 ) # doing the grouping mentioned before d_stunt_grp = group_by(data_stunting, Entity) %&gt;% filter(Year == max(Year)) %&gt;% dplyr::summarise(share_stunted_child= max(share_stunted_child) ) 가장 표준적인 구간화(binning) 기준은 다음과 같습니다. 동일 간격 (Equal range) 동일 빈도 (Equal frequency) 사용자 정의 구간 (Custom bins) 아래에서 각각에 대해 설명합니다. 2.1.6.2 동일 간격 (Equal range) 데이터 분포를 확인하는 히스토그램에서 흔히 볼 수 있는 방식이지만, 이상치에 매우 취약합니다. 예를 들어 네 개의 구간을 만들려면 (최대값 - 최소값)을 4로 나누어야 합니다. # funModeling에 equal_freq(이산화) 함수가 포함되어 있습니다. library(funModeling) # ggplot2 it provides &#39;cut_interval&#39; function used to # split the variables based on equal range criteria library(ggplot2) # `cut` 함수와 마찬가지로 과학적 표기법을 비활성화하려면 # `dig.lab=9` 매개변수를 추가하세요. d_stunt_grp$share_stunted_child_eq_range= cut_interval(d_stunt_grp$share_stunted_child, n = 4) # The ‘describe’ function from Hmisc package is # extremely useful to profile data describe(d_stunt_grp$share_stunted_child_eq_range) ## d_stunt_grp$share_stunted_child_eq_range ## n missing distinct ## 154 0 4 ## ## Value [1.3,15.8] (15.8,30.3] (30.3,44.8] (44.8,59.3] ## Frequency 62 45 37 10 ## Proportion 0.403 0.292 0.240 0.065 # Plotting the variable p2=ggplot(d_stunt_grp, aes(share_stunted_child_eq_range) ) + geom_bar(fill=&quot;#009E73&quot;) + theme_bw() p2 Figure 2.4: 동일 간격 이산화 describe 결과는 변수에 4개의 카테고리가 있음을 알려주며, 괄호/대괄호 사이에 각 카테고리별 총 사례 수(절대값 및 상대값)를 보여줍니다. 예를 들어, 카테고리 (15.8,30.3]은 share_stunted_child 값이 15.8(미포함)에서 30.3(포함) 사이인 모든 사례를 포함합니다. 이 구간은 45번 나타나며 전체 사례의 29%를 차지합니다. 2.1.6.3 동일 빈도 (Equal frequency) 이 기술은 백분위수(percentiles)를 기준으로 각 구간에 동일한 수의 관측치가 들어가도록 그룹화합니다. 백분위수에 대한 자세한 정보는 부록 1: 백분위수의 마법 장을 참조하세요. funModeling 패키지에는 이 기준에 따라 구간을 생성하는 equal_freq 함수가 포함되어 있습니다. d_stunt_grp$stunt_child_ef= equal_freq(var = d_stunt_grp$share_stunted_child, n_bins = 4 ) # profiling variable describe(d_stunt_grp$stunt_child_ef) ## d_stunt_grp$stunt_child_ef ## n missing distinct ## 154 0 4 ## ## Value [ 1.3, 9.5) [ 9.5,20.8) [20.8,32.9) [32.9,59.3] ## Frequency 40 37 39 38 ## Proportion 0.260 0.240 0.253 0.247 p3=ggplot(d_stunt_grp, aes(stunt_child_ef)) + geom_bar(fill=&quot;#CC79A7&quot;) + theme_bw() p3 Figure 2.5: 동일 빈도 예제 이 경우, 4개의 구간을 선택했으므로 각 구간은 약 25%의 점유율을 가집니다. 2.1.6.4 사용자 정의 구간 (Custom bins) 구간을 나눌 지점을 이미 알고 있다면 cut 함수를 사용할 수 있습니다. # dig.lab 매개변수는 과학적 표기법을 &quot;비활성화&quot;합니다. d_stunt_grp$share_stunted_child_custom= cut(d_stunt_grp$share_stunted_child, breaks = c(0, 2, 9.4, 29, 100) ) describe(d_stunt_grp$share_stunted_child_custom) ## d_stunt_grp$share_stunted_child_custom ## n missing distinct ## 154 0 4 ## ## Value (0,2] (2,9.4] (9.4,29] (29,100] ## Frequency 5 35 65 49 ## Proportion 0.032 0.227 0.422 0.318 p4=ggplot(d_stunt_grp, aes(share_stunted_child_custom)) + geom_bar(fill=&quot;#0072B2&quot;) + theme_bw() p4 Figure 2.6: 수동 이산화 각 버킷의 최대값만 정의하면 된다는 점에 유의하세요. 일반적으로 최소값이나 최대값을 모르는 경우가 많습니다. 그런 경우에는 -Inf와 Inf 값을 사용할 수 있습니다. 그렇지 않고 범위 밖의 값을 정의하면 cut은 NA 값을 할당합니다. 최소값과 최대값을 함수를 사용하여 할당하는 것이 좋은 관행입니다. 이 사례에서 변수는 백분율이므로 범위가 0에서 100 사이임을 미리 알고 있지만, ⚠️ 만약 범위를 모른다면 어떤 일이 벌어질까요? 함수는 절단 지점보다 낮거나 높은 값에 대해 NA를 반환할 것입니다. 한 가지 해결책은 변수의 최소값과 최대값을 가져오는 것입니다. # 최소값과 최대값 가져오기 min_value=min(d_stunt_grp$share_stunted_child) max_value=max(d_stunt_grp$share_stunted_child) # 최소값을 포함하려면 `include.lowest=T`로 설정하세요. # 그렇지 않으면 NA로 할당됩니다. d_stunt_grp$share_stunted_child_custom_2= cut(d_stunt_grp$share_stunted_child, breaks = c(min_value, 2, 9.4, 29, max_value), include.lowest = T) describe(d_stunt_grp$share_stunted_child_custom_2) ## d_stunt_grp$share_stunted_child_custom_2 ## n missing distinct ## 154 0 4 ## ## Value [1.3,2] (2,9.4] (9.4,29] (29,59.3] ## Frequency 5 35 65 49 ## Proportion 0.032 0.227 0.422 0.318 2.1.7 새로운 데이터에서의 이산화 이러한 모든 변환은 변수의 분포를 기반으로 한 학습 데이터셋이 주어졌을 때 이루어집니다. 동일 빈도 및 동일 간격 이산화의 경우가 그렇습니다. 하지만 새로운 데이터가 들어온다면 어떻게 될까요? 새로운 최소값이나 최대값이 나타나면 동일 간격 방법의 구간 범위에 영향을 미칩니다. 새로운 값이 들어오면 동일 빈도 방법에서 보았듯이 백분위수를 기준으로 한 지점들이 이동하게 됩니다. 예를 들어, 제안된 예제에 88, 2, 7, 3이라는 값을 가진 네 개의 사례를 추가한다고 가정해 보겠습니다. # 4개의 새로운 값이 들어오는 상황 시뮬레이션 updated_data=c(d_stunt_grp$share_stunted_child, 88, 2, 7, 3) # 동일 빈도 기준 이산화 updated_data_eq_freq=equal_freq(updated_data,4) # 결과는... describe(updated_data_eq_freq) ## updated_data_eq_freq ## n missing distinct ## 158 0 4 ## ## Value [ 1.3, 9.3) [ 9.3,20.6) [20.6,32.9) [32.9,88.0] ## Frequency 40 39 40 39 ## Proportion 0.253 0.247 0.253 0.247 이제 이전에 생성한 구간과 비교해 보겠습니다: describe(d_stunt_grp$stunt_child_ef) ## d_stunt_grp$stunt_child_ef ## n missing distinct ## 154 0 4 ## ## Value [ 1.3, 9.5) [ 9.5,20.8) [20.8,32.9) [32.9,59.3] ## Frequency 40 37 39 38 ## Proportion 0.260 0.240 0.253 0.247 모든 구간이 바뀌었습니다! 😱 이들은 새로운 카테고리이기 때문에, 예측 모델은 이들을 모두 새로운 값으로 인식하여 제대로 처리하지 못할 것입니다. 해결책은 데이터 준비를 할 때 절단 지점(cut points)을 저장해 두는 것입니다. 그런 다음 모델을 운영 환경에서 실행할 때 사용자 정의 구간 이산화를 사용하면 모든 새로운 사례가 적절한 카테고리에 강제로 들어가게 됩니다. 이렇게 하면 예측 모델은 항상 동일한 것을 보게 됩니다. 해결책은 다음 섹션에 있습니다. 2.1.8 자동 데이터 프레임 이산화 funModeling 패키지(버전 1.6.6 이상)는 이산화 작업을 돕기 위해 두 가지 함수인 discretize_get_bins와 discretize_df를 도입했습니다. # 라이브러리 로드 # install.packages(&quot;funModeling&quot;) library(funModeling) library(dplyr) 예시를 하나 살펴보겠습니다. 먼저 현재 데이터 유형을 확인합니다. df_status(heart_disease) %&gt;% select(variable, type, unique, q_na) %&gt;% arrange(type) ## variable q_zeros p_zeros q_na p_na q_inf p_inf type unique ## 1 age 0 0.00 0 0.00 0 0 integer 41 ## 2 gender 0 0.00 0 0.00 0 0 factor 2 ## 3 chest_pain 0 0.00 0 0.00 0 0 factor 4 ## 4 resting_blood_pressure 0 0.00 0 0.00 0 0 integer 50 ## 5 serum_cholestoral 0 0.00 0 0.00 0 0 integer 152 ## 6 fasting_blood_sugar 258 85.15 0 0.00 0 0 factor 2 ## 7 resting_electro 151 49.83 0 0.00 0 0 factor 3 ## 8 max_heart_rate 0 0.00 0 0.00 0 0 integer 91 ## 9 exer_angina 204 67.33 0 0.00 0 0 integer 2 ## 10 oldpeak 99 32.67 0 0.00 0 0 numeric 40 ## 11 slope 0 0.00 0 0.00 0 0 integer 3 ## 12 num_vessels_flour 176 58.09 4 1.32 0 0 integer 4 ## 13 thal 0 0.00 2 0.66 0 0 factor 3 ## 14 heart_disease_severity 164 54.13 0 0.00 0 0 integer 5 ## 15 exter_angina 204 67.33 0 0.00 0 0 factor 2 ## 16 has_heart_disease 0 0.00 0 0.00 0 0 factor 2 ## variable type unique q_na ## 1 gender factor 2 0 ## 2 chest_pain factor 4 0 ## 3 fasting_blood_sugar factor 2 0 ## 4 resting_electro factor 3 0 ## 5 thal factor 3 2 ## 6 exter_angina factor 2 0 ## 7 has_heart_disease factor 2 0 ## 8 age integer 41 0 ## 9 resting_blood_pressure integer 50 0 ## 10 serum_cholestoral integer 152 0 ## 11 max_heart_rate integer 91 0 ## 12 exer_angina integer 2 0 ## 13 slope integer 3 0 ## 14 num_vessels_flour integer 4 4 ## 15 heart_disease_severity integer 5 0 ## 16 oldpeak numeric 40 0 팩터(factor), 정수(integer), 수치형(numeric) 변수들이 섞여 있습니다! 변환은 두 단계로 이루어집니다. 첫째, 각 세그먼트가 시작되는 절단값 또는 임계값을 가져옵니다. 둘째, 해당 임계값을 사용하여 변수를 범주형으로 변환합니다. 다음 예제에서는 max_heart_rate와 oldpeak 두 변수를 이산화할 것입니다. 또한 결측값에 대해 함수가 어떻게 작동하는지 테스트하기 위해 oldpeak에 몇 가지 NA 값을 도입하겠습니다. # creating a copy to keep original data clean heart_disease_2=heart_disease # Introducing some missing values in the first 30 rows of the oldpeak variable heart_disease_2$oldpeak[1:30]=NA 1단계) 각 입력 변수에 대해 구간 임계값 가져오기: discretize_get_bins는 discretize_df 함수에서 사용될 데이터 프레임을 반환하며, 이 함수는 최종 처리된 데이터 프레임을 반환합니다. d_bins=discretize_get_bins(data=heart_disease_2, input=c(&quot;max_heart_rate&quot;, &quot;oldpeak&quot;), n_bins=5) ## Variables processed: max_heart_rate, oldpeak # `d_bins` 객체 확인: d_bins ## variable cuts ## 1 max_heart_rate 131|147|160|171|Inf ## 2 oldpeak 0.1|0.3|1.1|2|Inf 매개변수: data: 처리할 변수가 포함된 데이터 프레임. input: 변수 이름을 포함하는 문자열 벡터. n_bins: 이산화된 데이터에서 가질 구간/세그먼트의 수. 각 변수에 대한 각 임계값 지점(또는 상한 경계)을 확인할 수 있습니다. 참고: 버전 1.6.6에서 1.6.7로의 변경 사항: discretize_get_bins는 해당 값이 항상 최소값으로 간주되었기 때문에 -Inf 임계값을 생성하지 않습니다. 단일 값 카테고리는 이제 범위로 표시됩니다. 예를 들어 \"5\"였던 것은 이제 \"[5, 6)\"으로 표시됩니다. 버킷 포맷이 변경되었을 수 있으므로, 이 함수를 운영 환경에서 사용 중이었다면 새로운 값을 확인해야 합니다. 다음 단계로 넘어갈 시간입니다! 2단계) 각 변수에 임계값 적용하기: # 이제 동일한 데이터 프레임이나 새로운 데이터 프레임(예: 시간이 지남에 따라 데이터가 변하는 # 예측 모델)에 적용할 수 있습니다. heart_disease_discretized = discretize_df(data=heart_disease_2, data_bins=d_bins, stringsAsFactors=T) ## Warning: `funs()` was deprecated in dplyr 0.8.0. ## ℹ Please use a list of either functions or lambdas: ## ## # Simple named list: list(mean = mean, median = median) ## ## # Auto named with `tibble::lst()`: tibble::lst(mean, median) ## ## # Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE)) ## ℹ The deprecated feature was likely used in the funModeling package. ## Please report the issue at &lt;https://github.com/pablo14/funModeling/issues&gt;. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. ## Variables processed: max_heart_rate, oldpeak 매개변수: data: 이산화할 수치형 변수가 포함된 데이터 프레임. data_bins: discretize_get_bins가 반환한 데이터 프레임. 사용자가 직접 변경하는 경우, 예제에 표시된 대로 각 상한 경계는 파이프 문자(|)로 구분되어야 합니다. stringsAsFactors: 기본값은 TRUE이며, 최종 변수는 팩터(문자열 대신)가 되어 시각화할 때 유용합니다. 2.1.8.1 최종 결과 및 플롯 변환 전후: ## max_heart_rate_before max_heart_rate_after oldpeak_before oldpeak_after ## 1 171 [ 171, Inf] NA NA. ## 2 114 [-Inf, 131) NA NA. ## 3 151 [ 147, 160) 1.8 [ 1.1, 2.0) ## 4 160 [ 160, 171) 1.4 [ 1.1, 2.0) ## 5 158 [ 147, 160) 0.0 [-Inf, 0.1) ## 6 161 [ 160, 171) 0.5 [ 0.3, 1.1) 최종 분포: freq(heart_disease_discretized %&gt;% select(max_heart_rate,oldpeak), plot = F) ## max_heart_rate frequency percentage cumulative_perc ## 1 [-Inf, 131) 63 20.79 20.79 ## 2 [ 147, 160) 62 20.46 41.25 ## 3 [ 160, 171) 62 20.46 61.71 ## 4 [ 131, 147) 59 19.47 81.18 ## 5 [ 171, Inf] 57 18.81 100.00 ## ## oldpeak frequency percentage cumulative_perc ## 1 [-Inf, 0.1) 97 32.01 32.01 ## 2 [ 0.3, 1.1) 54 17.82 49.83 ## 3 [ 1.1, 2.0) 54 17.82 67.65 ## 4 [ 2.0, Inf] 50 16.50 84.15 ## 5 NA. 30 9.90 94.05 ## 6 [ 0.1, 0.3) 18 5.94 100.00 ## [1] &quot;Variables processed: max_heart_rate, oldpeak&quot; p5=ggplot(heart_disease_discretized, aes(max_heart_rate)) + geom_bar(fill=&quot;#0072B2&quot;) + theme_bw() + theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1) ) p6=ggplot(heart_disease_discretized, aes(oldpeak)) + geom_bar(fill=&quot;#CC79A7&quot;) + theme_bw() + theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1) ) gridExtra::grid.arrange(p5, p6, ncol=2) Figure 2.7: 자동 이산화 결과 동일 빈도를 계산할 때 oldpeak 변수에서 볼 수 있듯이 버킷당 동일한 수의 사례를 얻는 것이 항상 가능한 것은 아닙니다. 2.1.8.2 NA 처리 NA 값과 관련하여, 새로운 oldpeak 변수에는 6개의 카테고리가 있습니다. 즉, n_bins=5에서 정의된 5개 카테고리와 NA. 값입니다. 끝에 있는 점은 결측값의 존재를 나타냅니다. 2.1.8.3 추가 정보 discretize_df는 결측값을 NA. 문자열로 변환하지 않고는 절대 NA를 반환하지 않습니다. n_bins는 모든 변수에 대한 구간 수를 설정합니다. input이 누락된 경우, 고유 값의 수가 구간 수(n_bins)보다 큰 모든 수치형/정수형 변수에 대해 실행됩니다. input에 정의된 변수만 처리되며 나머지 변수는 전혀 수정되지 않습니다. discretize_get_bins는 단지 데이터 프레임을 반환하므로 텍스트 파일이나 R 세션에서 필요에 따라 직접 수정할 수 있습니다. 2.1.8.4 새로운 데이터에서의 이산화 우리 데이터에서 max_heart_rate의 최소값은 71입니다. 데이터 준비 과정은 새로운 데이터에 대해 견고해야 합니다. 예를 들어 max_heart_rate가 68인 새로운 환자가 들어오면, 현재 프로세스는 이 환자를 가장 낮은 카테고리에 할당할 것입니다. 다른 패키지의 다른 함수에서는 범위를 벗어났기 때문에 NA를 반환할 수도 있습니다. 앞서 언급했듯이 시간이 지남에 따라 새로운 데이터가 들어오면 새로운 최소값이나 최대값이 나타날 가능성이 큽니다. 이는 프로세스를 방해할 수 있습니다. 이를 해결하기 위해 discretize_df는 항상 -Inf/Inf를 최소값/최대값으로 가집니다. 따라서 최소값보다 낮거나 최대값보다 높은 새로운 값은 각각 가장 낮거나 높은 세그먼트에 추가됩니다. discretize_get_bins가 반환한 데이터 프레임은 새로운 데이터에 적용하기 위해 저장되어야 합니다. 만약 이산화 과정이 새로운 데이터와 함께 실행될 의도가 없다면, 두 개의 함수를 가질 이유가 없으며 하나면 충분할 것입니다. 또한 discretize_get_bins의 결과를 저장할 필요도 없을 것입니다. 이러한 2단계 접근 방식을 통해 두 가지 경우를 모두 처리할 수 있습니다. 2.1.8.5 2단계 이산화에 대한 결론 discretize_get_bins + discretize_df의 사용은 바로 사용할 준비가 된 깨끗한 데이터 프레임을 통해 빠른 데이터 준비를 제공합니다. 각 세그먼트가 어디서 시작하고 끝나는지 명확하게 보여주며, 통계 보고서를 작성할 때 필수적입니다. 새로운 데이터에서 새로운 최소값/최대값을 대할 때 실패하지 않기로 결정한 것은 단순한 결정일 뿐입니다. 어떤 문맥에서는 실패하는 것이 바람직한 행동일 수도 있습니다. 인간의 개입: 데이터 프레임을 이산화하는 가장 쉬운 방법은 예제에서 본 것처럼 모든 변수에 동일한 수의 구간을 적용하는 것이지만, 튜닝이 필요한 경우에는 일부 변수에 다른 수의 구간이 필요할 수 있습니다. 예를 들어, 분산이 적은 변수는 적은 수의 구간으로도 잘 작동할 수 있습니다. 세그먼트 수의 일반적인 값은 3, 5, 10 또는 20(그 이상은 아님)이 될 수 있습니다. 이 결정은 데이터 과학자의 몫입니다. 2.1.8.6 보너스 트랙: 트레이드오프의 예술 ⚖️ 많은 수의 구간 =&gt; 더 많은 노이즈 포착. 적은 수의 구간 =&gt; 과도한 단순화, 변동성 감소. 이 용어들이 머신러닝의 다른 용어들과 비슷하게 들리나요? 정답은: 네! 입니다. 단 한 가지 예만 들자면, 예측 모델에서 변수를 추가하거나 제거하는 사이의 트레이드오프와 같습니다. 더 많은 변수: 과적합(overfitting) 경고 (너무 상세한 예측 모델). 더 적은 변수: 과소적합(underfitting) 위험 (일반적인 패턴을 포착하기에 정보 부족). 수천 년 동안 동양 철학이 지적해 왔듯이, 하나의 가치와 그 반대 사이에서 올바른 균형을 찾는 것은 하나의 예술입니다. 2.1.9 최종 생각 보았듯이, 이산화나 데이터 준비에 있어 공짜 점심은 없습니다. _자동화된 시스템이나 지능형 시스템_이 인간의 개입이나 분석 없이 이러한 모든 상황을 어떻게 처리할 것이라 생각하나요? 물론 일부 작업은 자동화된 프로세스에 위임할 수 있지만, 처리할 올바른 입력 데이터를 제공하는 데이터 준비 단계에서 인간은 필수적입니다. 가장 많이 사용되는 두 가지 데이터 유형인 범주형 또는 수치형으로 변수를 할당하는 것은 데이터의 성격과 선택된 알고리즘(일부는 한 가지 데이터 유형만 지원)에 따라 달라집니다. 이 변환은 분석에 약간의 편향을 도입합니다. 결측값을 다룰 때도 비슷한 사례가 있습니다: 결측 데이터의 처리 및 대치. 범주형 변수로 작업할 때, 타겟 변수에 따라 카테고리를 재배치하여 관계를 더 잘 드러내도록 분포를 변경할 수 있습니다. 비선형 변수 관계를 선형 관계로 변환하는 것이죠. 2.1.10 보너스 트랙 💥 이산화 변수 섹션으로 돌아가서 지금까지 살펴본 모든 변환을 플롯해 보겠습니다. grid.arrange(p2, p3, p4, ncol = 3) Figure 2.8: 동일한 데이터, 다른 시각화 입력 데이터는 항상 동일합니다. 하지만 이러한 모든 방법은 동일한 것에 대해 다른 관점을 보여줍니다. 예측 모델링을 위한 동일 빈도 사용과 같이 특정 상황에 더 적합한 관점들이 있습니다. 이 사례는 하나의 변수만 고려하고 있지만, 여러 변수를 동시에 가진 경우, 즉 N-차원 공간에서도 논리는 동일합니다. 예측 모델을 구축할 때, 사람들이 어떤 대상에 대해 의견을 내는 것과 같이 동일한 점의 무리를 다른 방식으로 설명합니다. 2.2 기술 통계에서의 고카디널리티 변수 2.2.1 무엇에 대한 내용인가요? 고카디널리티(high cardinality) 변수는 가질 수 있는 값이 매우 많은 변수를 말합니다. 예를 들어 국가(country)가 있습니다. 이 장에서는 파레토 법칙(Pareto rule)에 기반한 카디널리티 감소를 다루며, 대부분의 값이 어디에 집중되어 있는지와 변수 분포를 빠르게 보여주는 freq 함수를 사용합니다. 2.2.2 기술 통계에서의 고카디널리티 다음 예제는 person, country, has_flu라는 3개의 열을 가진 910개 사례의 설문조사를 포함하고 있습니다. has_flu는 지난 달에 해당 질병을 앓았는지 여부를 나타냅니다. library(funModeling) data_country 데이터는 funModeling 패키지(버전 1.6 이상)에 포함되어 있습니다. data_country의 빠른 프로파일링 (처음 10행) # 처음 10행 출력 head(data_country, 10) ## person country has_flu ## 478 478 France no ## 990 990 Brazil no ## 606 606 France no ## 575 575 Philippines no ## 806 806 France no ## 232 232 France no ## 422 422 Poland no ## 347 347 Romania no ## 858 858 Finland no ## 704 704 France no # 데이터 탐색, 처음 10행만 표시 head(freq(data_country, &quot;country&quot;), 10) ## Warning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use &quot;none&quot; instead as ## of ggplot2 3.3.4. ## ℹ The deprecated feature was likely used in the funModeling package. ## Please report the issue at &lt;https://github.com/pablo14/funModeling/issues&gt;. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. Figure 2.9: 국가 빈도 분석 ## country frequency percentage cumulative_perc ## 1 France 288 31.65 31.65 ## 2 Turkey 67 7.36 39.01 ## 3 China 65 7.14 46.15 ## 4 Uruguay 63 6.92 53.07 ## 5 United Kingdom 45 4.95 58.02 ## 6 Australia 41 4.51 62.53 ## 7 Germany 30 3.30 65.83 ## 8 Canada 19 2.09 67.92 ## 9 Netherlands 19 2.09 70.01 ## 10 Japan 18 1.98 71.99 # 데이터 탐색 freq(data_country, &quot;has_flu&quot;) Figure 2.10: 독감 발생 여부 빈도 분석 ## has_flu frequency percentage cumulative_perc ## 1 no 827 90.88 90.88 ## 2 yes 83 9.12 100.00 마지막 테이블은 has_flu=\"yes\"인 행이 83개뿐임을 보여주며, 이는 전체 인원의 약 9%를 차지합니다. 하지만 이들 중 다수는 데이터에서 거의 비중을 차지하지 않습니다. 이것이 롱 테일(long tail) 현상이며, 카디널리티를 줄이는 한 가지 방법은 데이터 공유의 높은 비율(예: 70, 80 또는 90%)을 차지하는 카테고리들만 유지하는 파레토 법칙을 적용하는 것입니다. # &#39;funModeling&#39; 패키지의 &#39;freq&#39; 함수는 절단 지점을 설정하는 데 도움이 되는 누적 백분율(cumulative_percentage)을 가져옵니다. country_freq = freq(data_country, &#39;country&#39;, plot = F) # &#39;country_freq&#39;는 빈도에 따라 정렬된 테이블이므로, 가장 높은 비중을 차지하는 처음 10개 행을 살펴봅시다. country_freq %&gt;% slice_head(n = 10) ## country frequency percentage cumulative_perc ## 1 France 288 31.65 31.65 ## 2 Turkey 67 7.36 39.01 ## 3 China 65 7.14 46.15 ## 4 Uruguay 63 6.92 53.07 ## 5 United Kingdom 45 4.95 58.02 ## 6 Australia 41 4.51 62.53 ## 7 Germany 30 3.30 65.83 ## 8 Canada 19 2.09 67.92 ## 9 Netherlands 19 2.09 70.01 ## 10 Japan 18 1.98 71.99 따라서 10개 국가가 사례의 70% 이상을 대표합니다. 나머지 사례들에 other 카테고리를 할당하고 플롯을 그릴 수 있습니다. data_country$country_2 = ifelse(data_country$country %in% pull(country_freq[1:10, ], country), data_country$country, &#39;other&#39;) freq(data_country, &#39;country_2&#39;) Figure 2.11: 수정된 국가 변수 - 빈도 분석 ## country_2 frequency percentage cumulative_perc ## 1 France 288 31.65 31.65 ## 2 other 255 28.02 59.67 ## 3 Turkey 67 7.36 67.03 ## 4 China 65 7.14 74.17 ## 5 Uruguay 63 6.92 81.09 ## 6 United Kingdom 45 4.95 86.04 ## 7 Australia 41 4.51 90.55 ## 8 Germany 30 3.30 93.85 ## 9 Canada 19 2.09 95.94 ## 10 Netherlands 19 2.09 98.03 ## 11 Japan 18 1.98 100.00 2.2.3 최종 코멘트 대표성이 낮은 카테고리는 가끔 Egypt, Eggypt.와 같이 데이터 오류인 경우가 있으며, 이는 데이터를 수집하는 나쁜 습관이나 소스에서 수집할 때 발생할 수 있는 오류의 증거가 될 수 있습니다. 데이터를 축소하는 일반적인 규칙은 없으며, 각 사례에 따라 다릅니다. 다음 권장 장: 예측 모델링에서의 고카디널리티 변수. 2.3 High Cardinality Variable in Predictive Modeling 2.3.1 무엇에 대한 내용인가요? 지난 장인 기술 통계에서의 고카디널리티에서 보았듯이, 우리는 주요 대표성을 가진 카테고리들만 유지하지만, 이를 기반으로 예측할 다른 변수가 있다면 어떨까요? 즉, country를 기반으로 has_flu를 예측하는 것입니다. 이전 방법을 사용하면 변수의 정보가 파괴되어 예측력을 잃게 될 수 있습니다. 이 장에서는 위에서 설명한 방법을 더 발전시켜, 자동 그룹화 함수인 auto_grouping을 사용하여 변수의 구조를 탐색하고 범주형 변수를 최적화하는 방법에 대한 아이디어를 제공할 것입니다. 하지만 더 중요한 것은 독자가 자신만의 최적화를 수행하도록 장려하는 것입니다. 다른 문헌에서는 이러한 재그룹화를 카디널리티 감소 또는 인코딩(encoding)이라고 부릅니다. 이 장에서 무엇을 검토할까요? 데이터 대표성의 개념 (샘플 크기). 타겟 또는 결과 변수가 있는 샘플 크기. R에서: 카디널리티를 줄이고 범주형 변수를 프로파일링하는 데 도움이 되는 방법 제시. 카디널리티를 줄인 전후의 실용적인 사례와 인사이트 추출. 랜덤 포레스트나 그레디언트 부스팅 머신과 같은 다양한 모델이 범주형 변수를 처리하는 방법. 2.3.2 하지만 변수를 재그룹화하는 것이 필요할까요? 사례에 따라 다르지만 가장 빠른 대답은 “예”입니다. 이 장에서 우리는 이러한 데이터 준비가 전체 정확도(ROC 곡선 아래 면적으로 측정)를 높이는 한 사례를 보게 될 것입니다. 데이터의 표현력(각 카테고리가 가진 행의 수)과 각 카테고리가 결과 변수와 얼마나 관련이 있는지 사이에는 트레이드오프가 존재합니다. 예: 일부 국가는 다른 국가보다 독감 사례에 더 취약할 수 있습니다. # 이를 처리하는 함수를 포함하고 있는 funModeling &gt;=1.6을 로드합니다. library(funModeling) library(dplyr) funModeling 패키지에 포함된 data_country를 프로파일링합니다 (버전 1.6.5 이상으로 업데이트하세요). data_country의 빠른 프로파일링 (처음 10행) # 처음 10행 출력 head(data_country, 10) ## person country has_flu country_2 ## 478 478 France no France ## 990 990 Brazil no other ## 606 606 France no France ## 575 575 Philippines no other ## 806 806 France no France ## 232 232 France no France ## 422 422 Poland no other ## 347 347 Romania no other ## 858 858 Finland no other ## 704 704 France no France # 데이터 탐색, 처음 10행만 표시 head(freq(data_country, &quot;country&quot;), 10) Figure 2.12: 처음 10개 국가 ## country frequency percentage cumulative_perc ## 1 France 288 31.65 31.65 ## 2 Turkey 67 7.36 39.01 ## 3 China 65 7.14 46.15 ## 4 Uruguay 63 6.92 53.07 ## 5 United Kingdom 45 4.95 58.02 ## 6 Australia 41 4.51 62.53 ## 7 Germany 30 3.30 65.83 ## 8 Canada 19 2.09 67.92 ## 9 Netherlands 19 2.09 70.01 ## 10 Japan 18 1.98 71.99 # 데이터 탐색 freq(data_country, &quot;has_flu&quot;) Figure 2.13: 독감 분포 ## has_flu frequency percentage cumulative_perc ## 1 no 827 90.88 90.88 ## 2 yes 83 9.12 100.00 2.3.3 사례 🔍 예측 모델은 특정 값을 특정 결과와 매핑하려고 시도할 것이며, 우리의 경우 타겟 변수는 이진형입니다. 우리는 categ_analysis를 기반으로 타겟 변수 has_flu에 대한 country의 완전한 프로파일링을 계산할 것입니다. 각 행은 input 변수의 고유한 카테고리를 나타냅니다. 각 행 내에서 대표성과 가능성 측면에서 각 카테고리를 정의하는 속성을 찾을 수 있습니다. # `categ_analysis`는 &quot;funModeling&quot; &gt;= v1.6에서 사용할 수 있습니다. 사용 전 설치해 주세요. country_profiling=categ_analysis(data=data_country, input=&quot;country&quot;, target = &quot;has_flu&quot;) ## Warning: `summarise_()` was deprecated in dplyr 0.7.0. ## ℹ Please use `summarise()` instead. ## ℹ The deprecated feature was likely used in the funModeling package. ## Please report the issue at &lt;https://github.com/pablo14/funModeling/issues&gt;. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. ## Warning: `group_by_()` was deprecated in dplyr 0.7.0. ## ℹ Please use `group_by()` instead. ## ℹ See vignette(&#39;programming&#39;) for more help ## ℹ The deprecated feature was likely used in the funModeling package. ## Please report the issue at &lt;https://github.com/pablo14/funModeling/issues&gt;. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. # 70개 국가 중 처음 15개 행(국가) 출력. head(country_profiling, 15) Figure 2.14: 타겟 vs. 입력 분석 참고 1: 첫 번째 열의 이름은 input 변수에 따라 자동으로 조정됩니다. 참고 2: has_flu 변수는 yes와 no 값을 가집니다. categ_analysis는 평균, 합계 및 백분율을 계산하기 위해 대표성이 낮은 클래스(이 경우 yes)에 내부적으로 숫자 1을 할당합니다. 이들은 categ_analysis가 반환하는 지표들입니다: country: input 변수의 각 카테고리 이름. mean_target: sum_target / q_rows, 해당 카테고리에 대한 has_flu=\"yes\"의 평균 수. 이것이 가능성(likelihood)입니다. sum_target: 각 카테고리에 있는 has_flu=\"yes\" 값의 수량. perc_target: sum_target과 동일하지만 백분율로 표시됩니다. 각 카테고리의 sum_target / 전체 sum_target. 이 열의 합은 1.00입니다. q_rows: has_flu 변수와 관계없이 해당 카테고리에 속한 행의 수량. input의 분포입니다. 이 열의 합은 분석된 전체 행 수입니다. perc_rows: q_rows와 관련되어 각 카테고리의 점유율 또는 백분율을 나타냅니다. 이 열의 합은 1.00입니다. 2.3.3.1 여기서 어떤 결론을 내릴 수 있을까요? 첫 번째 France 행을 기반으로 예제를 읽어봅시다: 41명이 독감에 걸렸습니다 (sum_target=41). 이 41명은 독감에 걸린 전체 인원의 거의 50%를 나타냅니다 (perc_target=0.494). 프랑스에서 독감에 걸릴 확률은 14.2%입니다 (mean_target=0.142). 프랑스의 전체 행 = 288개 (910개 중). 이것이 q_rows 변수이며, perc_rows는 동일한 수치를 백분율로 나타낸 것입니다. 국가별 필터를 고려하지 않으면 다음을 얻습니다: sum_target 열의 합은 데이터에 있는 독감 환자의 총합입니다. perc_target 열의 합은 1.00 또는 100%입니다. q_rows 열의 합은 data_country 데이터 프레임에 있는 총 행 수입니다. perc_rows 열의 합은 1.00 또는 100%입니다. 2.3.4 예측 모델링을 위한 분석 🔮 예측 모델을 개발할 때, 우리는 특정 이벤트의 가능성을 높이는 값들에 관심을 가질 수 있습니다. 우리의 경우: 독감 환자를 찾을 가능성을 최대화하는 국가는 어디인가요? 쉽습니다. country_profiling을 mean_target 기준 내림차순으로 정렬하면 됩니다: # country_profiling을 mean_target으로 정렬한 다음 처음 6개 국가 가져오기 country_profiling %&gt;% arrange(desc(mean_target)) %&gt;% head() ## country mean_target sum_target perc_target q_rows perc_rows ## 1 Malaysia 1.000 1 0.012 1 0.001 ## 2 Mexico 0.667 2 0.024 3 0.003 ## 3 Portugal 0.200 1 0.012 5 0.005 ## 4 United Kingdom 0.178 8 0.096 45 0.049 ## 5 Uruguay 0.175 11 0.133 63 0.069 ## 6 Israel 0.167 1 0.012 6 0.007 좋습니다! 독감에 걸릴 확률이 가장 높은 국가로 Malasyia가 나왔습니다! 그곳 사람들의 100%가 독감에 걸렸습니다 (mean_has_flu=1.000). 하지만 우리의 상식은 아마도 무언가 잘못되었다고 조언합니다… 말레이시아의 행 수는 몇 개인가요? 정답: 1. -열: q_rows=1 말레이시아의 양성 사례는 몇 개인가요? 정답: 1. -열: sum_target=1 샘플을 늘려 이 비율이 높게 유지되는지 확인할 수 없으므로, 이는 예측 모델에서 과적합(overfit)을 유발하고 편향을 만들게 될 것입니다. Mexico는 어떨까요? 3명 중 2명이 독감… 여전히 낮아 보입니다. 하지만 Uruguay는 17.3%의 확률(63건 중 11건)을 보이며, 이 63건은 전체 인구의 거의 7%를 차지합니다 (perc_row=0.069). 이 비율은 좀 더 신뢰할 수 있어 보입니다. 다음은 이를 처리하기 위한 몇 가지 아이디어입니다: 2.3.4.1 사례 1: 대표성이 낮은 값을 재범주화하여 줄이기 데이터에서 일정 비율 이상의 점유율을 가진 사례만 남깁니다. 예를 들어, 데이터에서 점유율이 1% 미만인 국가들의 이름을 other로 바꿉니다. country_profiling=categ_analysis(data=data_country, input=&quot;country&quot;, target = &quot;has_flu&quot;) countries_high_rep=filter(country_profiling, perc_rows&gt;0.01) %&gt;% pull(country) # 만약 countries_high_rep에 포함되지 않으면 `other` 카테고리 할당 data_country$country_new=ifelse(data_country$country %in% countries_high_rep, data_country$country, &quot;other&quot;) 독감 확률을 다시 확인해 봅시다. country_profiling_new=categ_analysis(data=data_country, input=&quot;country_new&quot;, target = &quot;has_flu&quot;) country_profiling_new ## country_new mean_target sum_target perc_target q_rows perc_rows ## 1 United Kingdom 0.178 8 0.096 45 0.049 ## 2 Uruguay 0.175 11 0.133 63 0.069 ## 3 Canada 0.158 3 0.036 19 0.021 ## 4 France 0.142 41 0.494 288 0.316 ## 5 Germany 0.100 3 0.036 30 0.033 ## 6 Australia 0.098 4 0.048 41 0.045 ## 7 Romania 0.091 1 0.012 11 0.012 ## 8 Spain 0.091 1 0.012 11 0.012 ## 9 Sweden 0.083 1 0.012 12 0.013 ## 10 Netherlands 0.053 1 0.012 19 0.021 ## 11 other 0.041 7 0.084 170 0.187 ## 12 Turkey 0.030 2 0.024 67 0.074 ## 13 Belgium 0.000 0 0.000 15 0.016 ## 14 Brazil 0.000 0 0.000 13 0.014 ## 15 China 0.000 0 0.000 65 0.071 ## 16 Italy 0.000 0 0.000 10 0.011 ## 17 Japan 0.000 0 0.000 18 0.020 ## 18 Poland 0.000 0 0.000 13 0.014 국가의 수가 급격히 줄어들었습니다 -74% 감소- 단지 1% 미만의 낮은 대표성 국가들을 줄임으로써 70개 국가 중 18개만 남았습니다. other 카테고리에서 타겟 변수의 확률이 좀 더 안정화되었습니다. 이제 예측 모델이 Malaysia를 보게 되면 100%의 확률을 부여하는 대신 4.1% (mean_has_flu=0.041)를 부여하게 됩니다. 이 마지막 방법에 대한 조언: 이 기술을 맹목적으로 적용하지 않도록 주의하세요. 때때로 매우 불균형한(highly unbalanced) 타겟 예측(예: 이상 탐지, anomaly detection)에서는 이상 징후가 데이터의 1% 미만에서 나타나는 경우가 있습니다. # 데이터 복제 d_abnormal=data_country # 일부 국가에서 비정상적인 동작 시뮬레이션 d_abnormal$abnormal=ifelse(d_abnormal$country %in% c(&quot;Brazil&quot;, &quot;Chile&quot;), &#39;yes&#39;, &#39;no&#39;) # 범주형 분석 ab_analysis=categ_analysis(d_abnormal, input = &quot;country&quot;, target = &quot;abnormal&quot;) # 처음 6개 요소만 표시 head(ab_analysis) ## country mean_target sum_target perc_target q_rows perc_rows ## 1 Brazil 1 13 0.867 13 0.014 ## 2 Chile 1 2 0.133 2 0.002 ## 3 Argentina 0 0 0.000 9 0.010 ## 4 Asia/Pacific Region 0 0 0.000 1 0.001 ## 5 Australia 0 0 0.000 41 0.045 ## 6 Austria 0 0 0.000 1 0.001 # 분포 검사, &#39;no&#39; 카테고리에 속하는 비중이 매우 낮음 freq(d_abnormal, &quot;abnormal&quot;, plot = F) ## abnormal frequency percentage cumulative_perc ## 1 no 895 98.35 98.35 ## 2 yes 15 1.65 100.00 이상 수치가 얼마나 있나요? 단 15개뿐이며, 전체 데이터의 1.65%를 차지합니다. categ_analysis가 반환한 테이블을 보면, 이 _이상 징후_는 참여도가 매우 낮은 카테고리에서만 발생함을 알 수 있습니다. Brazil은 1.4%, Chile은 0.2%에 불과합니다. 여기서 데이터 분포를 기반으로 other 카데고리를 만드는 것은 좋은 생각이 아닙니다. 결론: 비록 이것은 준비된 예제이지만, 정확도 측면에서 정말 유용할 수 있는 몇 가지 데이터 준비 기술이 있습니다. 하지만 이들은 전문가의 감독이 필요하며, 알고리즘이 이러한 감독을 도울 수 있습니다. 2.3.4.2 사례 2: 자동 그룹화를 통한 줄이기 이 절차는 kmeans 클러스터링 기술과 categ_analysis가 반환한 테이블을 사용하여 다음과 같은 측면에서 유사한 행동을 보이는 카테고리들을 그룹(클러스터)으로 묶습니다. perc_rows perc_target 두 가지를 결합하면 가능성과 대표성을 모두 고려한 그룹을 찾을 수 있습니다. R 실습: 우리는 원하는 그룹의 수인 n_groups 매개변수를 정의합니다. 이 숫자는 데이터와 전체 카테고리 수에 따라 상대적이지만, 일반적으로 3에서 10 사이의 숫자가 적당합니다. auto_grouping 함수는 funModeling 버전 1.6 이상에서 제공됩니다. target 매개변수는 비이진(non binary) 변수만 지원한다는 점에 유의하세요. 참고: seed 매개변수는 선택 사항이지만, 숫자를 지정하면 항상 동일한 결과를 얻을 수 있습니다. # 카디널리티 줄이기 country_groups=auto_grouping(data = data_country, input = &quot;country&quot;, target=&quot;has_flu&quot;, n_groups=9, seed = 999) ## Warning: `select_()` was deprecated in dplyr 0.7.0. ## ℹ Please use `select()` instead. ## ℹ The deprecated feature was likely used in the funModeling package. ## Please report the issue at &lt;https://github.com/pablo14/funModeling/issues&gt;. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. ## Warning: `arrange_()` was deprecated in dplyr 0.7.0. ## ℹ Please use `arrange()` instead. ## ℹ See vignette(&#39;programming&#39;) for more help ## ℹ The deprecated feature was likely used in the funModeling package. ## Please report the issue at &lt;https://github.com/pablo14/funModeling/issues&gt;. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. country_groups$df_equivalence ## country country_rec ## 1 Bulgaria group_1 ## 2 Denmark group_1 ## 3 Hong Kong group_1 ## 4 Indonesia group_1 ## 5 Italy group_1 ## 6 Norway group_1 ## 7 Philippines group_1 ## 8 Singapore group_1 ## 9 South Africa group_1 ## 10 Ukraine group_1 ## 11 Asia/Pacific Region group_2 ## 12 Austria group_2 ## 13 Bangladesh group_2 ## 14 Bosnia and Herzegovina group_2 ## 15 Cambodia group_2 ## 16 Chile group_2 ## 17 Costa Rica group_2 ## 18 Croatia group_2 ## 19 Cyprus group_2 ## 20 Czech Republic group_2 ## 21 Dominican Republic group_2 ## 22 Egypt group_2 ## 23 Finland group_2 ## 24 Ghana group_2 ## 25 Greece group_2 ## 26 Honduras group_2 ## 27 Iran, Islamic Republic of group_2 ## 28 Ireland group_2 ## 29 Isle of Man group_2 ## 30 Korea, Republic of group_2 ## 31 Latvia group_2 ## 32 Lithuania group_2 ## 33 Luxembourg group_2 ## 34 Malta group_2 ## 35 Moldova, Republic of group_2 ## 36 Montenegro group_2 ## 37 Morocco group_2 ## 38 New Zealand group_2 ## 39 Pakistan group_2 ## 40 Palestinian Territory group_2 ## 41 Peru group_2 ## 42 Russian Federation group_2 ## 43 Saudi Arabia group_2 ## 44 Senegal group_2 ## 45 Slovenia group_2 ## 46 Taiwan group_2 ## 47 Thailand group_2 ## 48 Vietnam group_2 ## 49 China group_3 ## 50 Turkey group_3 ## 51 France group_4 ## 52 Australia group_5 ## 53 Argentina group_6 ## 54 Israel group_6 ## 55 Malaysia group_6 ## 56 Mexico group_6 ## 57 Portugal group_6 ## 58 Romania group_6 ## 59 Spain group_6 ## 60 Sweden group_6 ## 61 Switzerland group_6 ## 62 Belgium group_7 ## 63 Brazil group_7 ## 64 Japan group_7 ## 65 Netherlands group_7 ## 66 Poland group_7 ## 67 United Kingdom group_8 ## 68 Uruguay group_8 ## 69 Canada group_9 ## 70 Germany group_9 auto_grouping은 3개의 객체를 포함하는 리스트를 반환합니다. df_equivalence: 기존 값과 새로운 값을 매핑하는 테이블이 포함된 데이터 프레임. fit_cluster: 카디널리티를 줄이기 위해 사용된 k-means 모델 (값은 스케일링됨). recateg_results: 타겟 변수에 대한 각 그룹의 프로파일링을 포함하는 데이터 프레임으로, 첫 번째 열은 입력 변수의 이름을 따릅니다 (이 경우 country_rec). 각 그룹은 입력 변수의 하나 또는 여러 카테고리에 대응합니다 (df_equivalence에서 확인 가능). 새로운 그룹들이 어떻게 행동하는지 살펴봅시다. 예측 모델이 보게 될 정보입니다. country_groups$recateg_results ## country_rec mean_target sum_target perc_target q_rows perc_rows ## 1 group_8 0.176 19 0.229 108 0.119 ## 2 group_6 0.156 10 0.120 64 0.070 ## 3 group_4 0.142 41 0.494 288 0.316 ## 4 group_9 0.122 6 0.072 49 0.054 ## 5 group_5 0.098 4 0.048 41 0.045 ## 6 group_3 0.015 2 0.024 132 0.145 ## 7 group_7 0.013 1 0.012 78 0.086 ## 8 group_1 0.000 0 0.000 75 0.082 ## 9 group_2 0.000 0 0.000 75 0.082 위 테이블은 mean_target을 기준으로 정렬되어 있어, 확률을 최대화하거나 최소화하는 그룹을 빠르게 확인할 수 있습니다. group_2가 가장 흔하며, 31.6%의 비중을 차지하고 mean_target(확률)은 14.2%입니다. group_8이 가장 높은 확률(17.6%)을 보입니다. 그 뒤를 이어 group_6가 15.6%의 양성(has_flu=\"yes\") 확률을 가집니다. group_4, group_5, group_9은 비슷해 보입니다. 모든 사례에서 확률이 0이므로 하나의 그룹으로 묶을 수 있습니다. group_7과 group_3은 양성 사례가 있는 국가가 1~2개뿐입니다. 이 수치들을 동일한 것으로 간주하여 하나의 그룹으로 묶을 수 있으며, 결국 가장 낮은 확률을 가진 국가들을 나타내게 됩니다. 먼저 원본 데이터셋에 새로운 카테고리 열을 추가해야 합니다. data_country_2=data_country %&gt;% inner_join(country_groups$df_equivalence, by=&quot;country&quot;) 이제 다음으로 대체하는 추가 변환을 수행합니다. group_4, group_5, group_9은 low_likelihood가 됩니다 (양성 사례가 없거나 타겟 비중이 낮은 국가들). group_7과 group_3은 low_target_share가 됩니다. data_country_2$country_rec= ifelse(data_country_2$country_rec %in% c(&quot;group_4&quot;, &quot;group_5&quot;, &quot;group_9&quot;), &quot;low_likelihood&quot;, data_country_2$country_rec ) data_country_2$country_rec= ifelse(data_country_2$country_rec %in% c(&quot;group_7&quot;, &quot;group_3&quot;), &quot;low_target_share&quot;, data_country_2$country_rec ) 최종 그룹화(country_rec 변수)를 확인해 봅시다. categ_analysis(data=data_country_2, input=&quot;country_rec&quot;, target = &quot;has_flu&quot;) ## country_rec mean_target sum_target perc_target q_rows perc_rows ## 1 group_8 0.176 19 0.229 108 0.119 ## 2 group_6 0.156 10 0.120 64 0.070 ## 3 low_likelihood 0.135 51 0.614 378 0.415 ## 4 low_target_share 0.014 3 0.036 210 0.231 ## 5 group_1 0.000 0 0.000 75 0.082 ## 6 group_2 0.000 0 0.000 75 0.082 sum_target 분포를 보았을 때 각 그룹이 적절한 샘플 크기를 가진 것으로 보입니다. 변환 결과 low_likelihood는 전체 사례의 21%를 차지하면서도 여전히 양성 사례는 0개(sum_target=0)입니다. 그리고 low_target_share는 3개의 양성 사례를 가지며 전체 양성 사례의 3.6%를 차지합니다. 모든 그룹이 좋은 대표성을 가진 것 같습니다. 이는 perc_rows 변수에서 확인할 수 있는데, 모든 사례가 7% 이상의 비중을 차지합니다. 더 적은 수의 클러스터를 시도하면 이러한 수동 작업을 줄이는 데 도움이 될 수 있습니다. 이는 다양한 카테고리가 많은 변수를 최적화하는 방법을 보여주기 위한 예제였습니다. 2.3.5 예측 모델이 운영 환경에 있을 때 새로운 카테고리 처리하기 새로운 국가인 new_country_hello_world가 나타났다고 상상해 봅시다. 예측 모델은 고정된 값으로 학습되었기 때문에 실패할 것입니다. 한 가지 방법은 mean_target=0인 그룹을 할당하는 것입니다. 이는 지난 예제의 사례와 유사합니다. 하지만 차이점은 group_5에 있는데, 이 카테고리는 완전히 새로운 값보다는 중간 확률 그룹에 더 잘 맞을 수 있습니다. 얼마 후에는 모든 새로운 값을 포함하여 모델을 다시 구축해야 합니다. 그렇지 않으면 new_country_hello_world가 좋은 확률을 가지고 있음에도 불구하고 불이익을 줄 수 있기 때문입니다. 요약하자면: 새로운 카테고리가 나타났나요? 가장 의미가 적은 그룹으로 보내세요. 잠시 후 그 영향을 재분석하세요. 중간 또는 높은 확률을 보이나요? 가장 적합한 그룹으로 변경하세요. 2.3.6 예측 모델이 고카디널리티를 처리하나요? 파트 1 네, 그리고 아니오입니다. 일부 모델은 다른 모델보다 고카디널리티 문제를 더 잘 처리합니다. 어떤 시나리오에서는 이러한 데이터 준비가 필요하지 않을 수도 있습니다. 이 책은 이 문제를 노출하려고 노력하며, 때때로 이것이 더 나은 모델로 이어질 수 있음을 보여줍니다. 이제 그레디언트 부스팅 머신(Gradient Boosting Machine)을 사용하여 두 개의 예측 모델을 구축해 보겠습니다. GBM은 다양한 데이터 입력에 대해 매우 강력한 특성을 보입니다. 첫 번째 모델은 처리되지 않은 데이터를 사용하고, 두 번째 모델은 funModeling 패키지의 함수로 처리된 데이터를 사용합니다. ROC 영역을 기준으로 정밀도를 측정하며, 범위는 0.5에서 1 사이입니다. 숫자가 높을수록 모델의 성능이 좋습니다. 값에 대해 _확신_을 갖기 위해 교차 검증(cross-validation)을 사용할 것입니다. 교차 검증 결과의 중요성은 오차 알기 장에서 다룹니다. # 카디널리티를 줄이지 않고 첫 번째 모델 구축 library(caret) if(!requireNamespace(&quot;gbm&quot;, quietly = TRUE)) install.packages(&quot;gbm&quot;) fitControl &lt;- trainControl(method = &quot;cv&quot;, number = 4, classProbs = TRUE, summaryFunction = twoClassSummary) fit_gbm_1 &lt;- train(has_flu ~ country, data = data_country_2, method = &quot;gbm&quot;, trControl = fitControl, verbose = FALSE, metric = &quot;ROC&quot;) # 최고 ROC 값 가져오기 roc=round(max(fit_gbm_1$results$ROC),2) ROC 곡선 아래 영역은 (roc): 0.67입니다. 이제 동일한 매개변수를 사용하여 이전에 수행한 데이터 준비가 된 데이터로 동일한 모델을 만듭니다. # country_rec 변수를 기반으로 두 번째 모델 구축 fit_gbm_2 &lt;- train(has_flu ~ country_rec, data = data_country_2, method = &quot;gbm&quot;, trControl = fitControl, verbose = FALSE, metric = &quot;ROC&quot;) # 새로운 최고 ROC 값 가져오기 new_roc=round(max(fit_gbm_2$results$ROC),2) 새로운 ROC 곡선은 (new_roc): 0.72입니다. 그런 다음 첫 번째 roc 값에 대한 개선 비율을 계산합니다. 개선도: ~ 7.46%. ✅ 나쁘지 않죠? 마지막 테스트에 대한 짧은 평: 가장 강력한 모델 중 하나인 그레디언트 부스팅 머신을 사용했음에도 성능이 향상되었습니다. 지저분한 데이터에 더 민감한 로지스틱 회귀(logistic regression)와 같은 다른 모델을 사용하면 카디널리티를 줄였을 때와 그렇지 않았을 때의 차이가 더 커질 것입니다. 이는 verbose=FALSE 매개변수를 삭제하고 method=glm으로 변경하여 확인할 수 있습니다 (glm은 로지스틱 회귀를 포함합니다). 추가 읽기 장에는 범주형 변수에 대한 다양한 처리 방법과 각 방법이 정확도를 어떻게 높이거나 낮추는지에 대한 벤치마크가 있습니다. 2.3.7 예측 모델이 고카디널리티를 처리하지 않나요? 파트 2 일부 모델이 이를 어떻게 처리하는지 검토해 보겠습니다. 의사결정 트리 (Decision Trees): 카디널리티가 높은 변수를 상단에서 선택하여 정보 획득량(information gain)을 기준으로 다른 변수보다 더 큰 중요도를 부여하는 경향이 있습니다. 실제로 이는 과적합의 증거입니다. 이 모델은 고카디널리티 변수를 줄였을 때와 그렇지 않았을 때의 차이를 확인하기에 좋습니다. 랜덤 포레스트 (Random Forest): -적어도 R 구현체에서는- 최소 53개 이상의 서로 다른 범주를 가진 범주형 변수만 처리할 수 있습니다. 이 제한은 과적합을 피하기 위한 것일 가능성이 높습니다. 이 점은 알고리즘의 본질(수많은 트리를 생성함)과 결합하여 고카디널리티 변수를 선택할 때 단일 의사결정 트리의 효과를 줄여줍니다. 그레디언트 부스팅 머신 (Gradient Boosting Machine) 및 로지스틱 회귀 (Logistic Regression): 내부적으로 범주형 변수를 플래그(flag) 또는 더미(dummy) 변수로 변환합니다. 국가에 대한 예제에서는 내부적으로 70개의 플래그 변수가 생성됨을 의미합니다 (caret이 공식을 처리하는 방식이며, 더미 없이 원래 변수를 유지하려면 공식을 사용하지 않아야 합니다). 이전에 만든 모델을 확인해 봅시다. # 첫 번째 모델 확인... fit_gbm_1$finalModel ## A gradient boosted model with bernoulli loss function. ## 50 iterations were performed. ## There were 69 predictors of which 10 had non-zero influence. 즉, 69개의 입력 변수가 국가를 나타내고 있지만, 플래그 열들은 모델 예측에 관련성이 없는 것으로 보고되었습니다. 이는 특성 공학(Feature engineering)과 관련이 있습니다. 또한 최적 변수 선택과도 관련이 있습니다. 먼저 가장 많은 정보를 담고 있는 변수를 선택한 다음 예측 모델을 만드는 것이 권장되는 관행입니다. 결론: 카디널리티를 줄이면 이러한 모델에서 변수의 수도 줄어듭니다. 2.3.8 수치형 또는 다중 공선성 타겟 변수 📏 이 책은 타겟이 이진 변수인 경우만 다루었으며, 향후 수치형 및 다중값 타겟을 다룰 계획입니다. 하지만 여기까지 읽으셨다면, 동일한 아이디어를 염두에 두고 직접 탐색해보고 싶으실 것입니다. 웹사이트의 페이지 방문 수를 예측하는 것과 같은 수치형 변수에서는 입력 변수의 특정 카테고리가 높은 방문 수와 더 관련이 있는 반면, 다른 카테고리는 낮은 값과 더 상관관계가 있을 것입니다. 다중 공선성 출력 변수도 마찬가지입니다. 특정 값과 더 밀접한 관련이 있는 카테고리들이 있을 것입니다. 예를 들어 도시를 기반으로 유행 수준(high, mid, low)을 예측할 때, 다른 도시보다 높은 유행 수준과 더 상관관계가 있는 도시들이 있을 것입니다. 2.3.9 그룹화에서 얻은 “추가 선물 🎁”은 무엇인가요? 카테고리가 어떻게 그룹으로 분류되었는지 아는 것은 일부 사례에서 보고하기 좋은 정보를 제공합니다. 그룹 내의 각 카테고리는 대표성과 예측력 측면에서 유사한 행동을 공유하게 됩니다. 만약 Argentina와 Chile가 group_1에 있다면, 이들은 동일하며 모델도 그렇게 보게 됩니다. 2.3.10 대표성 또는 샘플 크기 이 개념은 모든 범주형 변수의 분석에 해당되지만, 데이터 과학과 통계에서 매우 흔한 주제인 샘플 크기(sample size)와 관련이 있습니다. 패턴이 충분히 발달된 것으로 보려면 얼마나 많은 데이터가 필요할까요? 범주형 변수에서: “X” 값과 타겟 값 사이의 상관관계를 신뢰하려면 카테고리 “X”의 사례가 몇 개나 필요할까요? 이것이 우리가 분석한 내용입니다. 일반적으로 예측하기 어려운 이벤트일수록 더 많은 사례가 필요합니다. 이 책 뒤에서 다른 관점에서 이 주제를 다루며 이 페이지로 다시 링크할 것입니다. 2.3.11 최종 생각 카디널리티를 줄이는 두 가지 사례를 보았습니다. 첫 번째는 타겟 변수를 고려하지 않는데 이는 예측 모델에서 위험할 수 있고, 두 번째는 타겟 변수를 고려합니다. 타겟 변수에 대한 각 입력 카테고리의 동질성(및 대표성)을 기반으로 새로운 변수를 생성합니다. 핵심 개념: 각 카테고리 자체 및 예측되는 이벤트에 대한 카테고리의 대표성. 통계적 테스트를 기반으로 이를 분석하는 것도 좋은 방법 중 하나입니다. 데이터 준비의 시작 부분에서 언급했듯이 입력 변수의 정보를 파괴하는 것은 결과적인 그룹화가 그룹 간에 동일한 비율을 가짐을 의미합니다(이진 변수 입력 시). 우리는 항상 카디널리티를 줄여야 할까요? 상황에 따라 다릅니다. 간단한 데이터에 대한 두 번의 테스트만으로 모든 사례를 일반화하기에는 부족합니다. 독자가 자신의 프로젝트에서 필요하다고 판단될 때 직접 최적화를 시작하는 좋은 계기가 되기를 바랍니다. 2.3.12 추가 읽기 다음 링크는 범주형 변수에 대한 다양한 처리에 따른 정확도 결과들을 포함하고 있습니다: Beyond One-Hot: an exploration of categorical variables. 2.4 이상치 처리 (Treatment of outliers) 2.4.1 무엇에 대한 내용인가요? 심장 질환 예측 등 머신러닝의 다른 주제들과 마찬가지로, 극단값(extreme values) 개념은 이 분야에만 국한된 것이 아닙니다. 오늘의 이상치가 내일은 아닐 수도 있습니다. 정상과 비정상 행동 사이의 경계는 모호하지만, 반면에 극단에 서는 것은 쉽습니다. 이미지 제작: Guillermo Mesyngier 이 장에서 무엇을 검토할까요? 이상치란 무엇인가? 철학적 및 실용적 접근 방식 차원 및 데이터 유형(수치형 또는 범주형)에 따른 이상치 R에서 이상치를 감지하는 방법 (상/하위 X%, Tukey 및 Hampel 방법) R의 프로파일링을 위한 이상치 준비 R의 예측 모델링을 위한 이상치 준비 2.4.2 이상치에 대한 직관 예를 들어, 다음과 같은 분포를 생각해 보십시오. # 분포 시각화를 위해 ggplot2 로드 library(ggplot2) # 샘플 데이터셋 생성 set.seed(31415) df_1=data.frame(var=round(10000*rbeta(1000,0.15,2.5))) # 플롯팅 ggplot(df_1, aes(var, fill=var)) + geom_histogram(bins=20) + theme_light() ## Warning: The following aesthetics were dropped during statistical transformation: fill. ## ℹ This can happen when ggplot fails to infer the correct grouping structure in ## the data. ## ℹ Did you forget to specify a `group` aesthetic or to convert a numerical ## variable into a factor? Figure 2.15: 긴 꼬리를 가진 샘플 분포 변수가 왼쪽으로 치우쳐 있고, 오른쪽에 일부 이상치 점들이 보입니다. 우리는 이들을 _처리_하고 싶습니다 (😎). 그렇다면 질문이 생깁니다: 극단의 임계값을 어디에 설정해야 할까요? 직관적으로는 상위 1%로 설정하거나, 상위 1%를 제거한 후 평균의 변화를 분석할 수 있습니다. 두 경우 모두 맞을 수 있습니다. 사실 다른 숫자(예: 2% 또는 0.1%)를 임계값으로 취하는 것도 맞을 수 있습니다. 시각화해 봅시다. # 상위 3%, 1%, 0.1% 백분위수 계산 percentile_var=quantile(df_1$var, c(0.98, 0.99, 0.999), na.rm = T) df_p=data.frame(value=percentile_var, percentile=c(&quot;a_98th&quot;, &quot;b_99th&quot;, &quot;c_99.9th&quot;)) # 동일한 분포에 백분위수를 추가하여 플롯팅 ggplot(df_1, aes(var)) + geom_histogram(bins=20) + geom_vline(data=df_p, aes(xintercept=value, colour = percentile), show.legend = TRUE, linetype=&quot;dashed&quot;) + theme_light() Figure 2.16: 이상치에 대한 다양한 임계값 백분위수에 대해 더 자세히 알아보려면 부록 1: 백분위수의 마법 장을 참조하세요. 지금은 상위 1%(99번째 백분위수)를 임계값으로 유지하여 그 이후의 모든 점을 이상치로 표시하겠습니다. Figure 2.17: 상위 1%를 이상치로 표시 여기서 흥미로운 개념적 요소가 나타납니다. 우리가 비정상(또는 이상징후)을 정의할 때, 그 반대급부로 정상 개념이 부상한다는 점입니다. 이 “정상” 행동은 녹색 영역으로 표시됩니다. Figure 2.18: 동일한 임계값, 다른 관점 어려운 부분은 정상과 비정상이 갈라지는 지점을 결정하는 것입니다. 이를 다루는 데는 몇 가지 접근 방식이 있습니다. 그중 몇 가지를 살펴보겠습니다. 2.4.3 덥고 추운 날씨의 경계는 어디일까요? 이 섹션은 좀 더 철학적으로 접근해 보겠습니다. 피타고라스나 아이작 뉴턴처럼 훌륭한 수학자들 중에는 철학자이기도 했던 이들이 있습니다. 더운 날씨가 시작되는 지점, 반대로 추운 날씨가 끝나는 지점을 나타내는 임계값을 어디에 두어야 할까요? Figure 2.19: 절단 지점은 어디일까요? 적도 근처에서는 아마도 10°C (50°F) 정도가 극도로 낮은 온도겠지만, 남극에서는 해수욕을 즐길 만한 날씨일 것입니다! ☃️ 🏖️ 👺: “오! 하지만 그건 서로 다른 두 위치를 사용한 극단적인 예시잖아요!” 문제없습니다! 프랙탈처럼 어떤 도시를 확대해 보더라도, 한 날씨가 시작되고 다른 날씨가 끝나는 경계에 대해 다음과 같이 말할 수 있는 유일한 값은 없을 것입니다: “좋아요, 더운 날씨는 25.5°C (78°F)부터 시작됩니다.” 그것은 상대적입니다. 하지만 60°C (140°F)와 같은 온도를 고려할 때처럼 불확실성이 거의 0으로 줄어드는 극단에 서는 것은 꽤 쉽습니다. 🤔: “좋아요. 하지만 이런 개념들이 머신러닝과 무슨 상관이 있죠?” 우리는 여기서 수치 변수(온도)를 레이블(덥다/춥다)로 고려할 때 존재하는 상대성을 보여주고 있습니다. 이는 수입과 “정상”, “비정상” 레이블과 같은 다른 수치 변수에도 적용될 수 있습니다. 극단값(extreme values)을 이해하는 것은 탐색적 데이터 분석의 첫 번째 작업 중 하나입니다. 그래야 무엇이 정상 데이터인지 알 수 있습니다. 이는 프로파일링 장에서 다룹니다. 값을 이상치로 표시하는 방법에는 여러 가지가 있습니다. 온도를 분석하는 것과 마찬가지로, 이 표시는 _상대적_이며 모든 방법이 맞을 수 있습니다. 가장 빠른 방법은 상위 및 하위 X%를 이상치로 처리하는 것입니다. 더 견고한 방법은 백분위수(Tukey 방법)를 사용하거나 표준 편차를 통한 값의 확산(Hampel 방법)을 사용하여 변수의 분포를 고려합니다. 이러한 경계를 정의하는 것은 머신러닝에서 가장 흔한 작업 중 하나입니다. 왜일까요? 언제일까요? 두 가지 예를 들어보겠습니다. 예시 1: 특정 고객에게 전화를 걸지 말지를 결정하기 위해 확률을 반환하는 예측 모델을 개발할 때, 최종 레이블(“전화하세요!”/“전화하지 마세요”)을 할당하기 위해 스코어 임계값을 설정해야 합니다. 이에 대한 자세한 정보는 데이터 스코어링 장에서 다룹니다. 예시 2: 수치형 변수를 범주형으로 사용하기 위해 이산화해야 할 때입니다. 각 구간/세그먼트의 경계는 전체 결과에 영향을 미칩니다. 이에 대한 자세한 내용은 수치형 변수 이산화 섹션에서 다룹니다. 📌 원래의 문제(추운 날씨는 어디서 끝나는가?)로 돌아가서, 모든 질문에 답이 필요한 것은 아닙니다. 어떤 질문들은 단순히 우리가 생각하는 데 도움을 줄 뿐입니다. 2.4.4 이상치의 영향 2.4.4.1 모델 구축 (Model building) 랜덤 포레스트(random forest)나 그레디언트 부스팅 머신(gradient-boosting machines)과 같은 일부 모델은 이상치를 더 잘 처리하는 경향이 있지만, “노이즈”는 어쨌든 결과에 영향을 줄 수 있습니다. 이러한 모델에서 이상치의 영향은 선형 회귀, 로지스틱 회귀, k-means, 의사결정 트리와 같은 다른 모델보다 낮습니다. 영향이 줄어드는 데 기여하는 한 가지 측면은 두 모델 모두 많은 하위 모델을 만든다는 것입니다. 만약 어떤 모델이 이상치를 정보로 취한다면, 다른 하위 모델들은 아마도 그렇지 않을 것이며, 따라서 오차는 상쇄됩니다. 목소리의 다양성 속에서 균형이 이루어집니다. 2.4.4.2 결과 전달 (Communicating results) 🌍 📣 모델에 사용된 변수를 보고해야 하는 경우, 막대가 하나뿐인 히스토그램을 보거나 편향된 평균을 보여주지 않기 위해 결국 이상치를 제거하게 됩니다. 모델이 극단값을 _처리할 것_이라고 정당화하는 것보다 편향되지 않은 숫자를 보여주는 것이 더 낫습니다. 2.4.4.3 데이터 유형별 이상치 유형 수치형 (Numerical) 📏: 앞서 본 것과 같습니다. Figure 2.20: 이상치가 있는 수치형 변수 범주형 (Categorical) 📊: 카테고리의 분산이 매우 높은(고카디널리티) 변수를 갖는 경우입니다: 예: 우편번호. 범주형 변수의 이상치를 처리하는 방법에 대한 자세한 내용은 기술 통계에서의 고카디널리티 변수 장을 참조하세요. Figure 2.21: 이상치가 있는 범주형 변수 ## var frequency percentage cumulative_perc ## 1 France 288 68.74 68.74 ## 2 China 65 15.51 84.25 ## 3 Uruguay 63 15.04 99.29 ## 4 Peru 2 0.48 99.77 ## 5 Vietnam 1 0.24 100.00 이 예제에서 Peru와 Vietnam은 데이터에서의 점유율이 1% 미만이므로 이상치 국가입니다. 2.4.4.4 차원별 이상치 유형 지금까지 우리는 1차원 단변량 이상치를 관찰했습니다. 우리는 또한 한 번에 두 개 이상의 변수를 고려할 수 있습니다. 예를 들어, v1과 v2라는 두 개의 변수가 있는 다음과 같은 데이터셋 df_hello_world가 있습니다. 이전과 동일한 분석을 수행해 봅시다. Figure 2.22: 차원별 이상치 ## v1 frequency percentage cumulative_perc ## 1 Uruguay 80 59.7 59.7 ## 2 Argentina 54 40.3 100.0 Figure 2.23: 차원별 이상치 ## v2 frequency percentage cumulative_perc ## 1 cat_A 83 61.94 61.94 ## 2 cat_B 51 38.06 100.00 ## [1] &quot;Variables processed: v1, v2&quot; 지금까지는 이상치가 없죠? 이제 각 변수의 분포를 서로에 대해 알려주는 분할표(contingency table)를 만들어 보겠습니다. ## v2 ## v1 cat_A cat_B ## Argentina 39.55 0.75 ## Uruguay 22.39 37.31 오 😱! Argentina와 cat_B의 조합은 다른 값들(1% 미만)에 비해 매우 낮습니다(0.75%). 반면 다른 교차점들은 22% 이상입니다. 2.4.4.5 몇 가지 생각들… 마지막 예제들은 극단값이나 이상치의 _잠재력_을 보여주며, 새로운 데이터셋에서 고려해야 할 사항들로 제시됩니다. 우리는 값을 이상치로 표시하기 위한 가능한 임계값으로 1%를 언급했습니다. 이 값은 경우에 따라 0.5% 또는 3%가 될 수 있습니다. 또한, 이러한 종류의 이상치가 반드시 문제를 일으키는 것은 아닙니다. 2.4.5 R에서 이상치를 처리하는 방법 funModeling 패키지에 포함된 prep_outliers 함수는 이 작업에 도움을 줄 수 있습니다. 이 함수는 한 번에 하나에서 ’N’개의 변수를 처리할 수 있습니다 (input 매개변수 지정). 핵심은 다음과 같습니다. 값을 이상치로 간주하기 위해 세 가지 다른 방법(method 매개변수)을 지원합니다: bottom_top, Tukey, Hampel. NA 값을 설정하거나 특정 값에서 변수를 멈추게 하는(stopping) 두 가지 모드(type 매개변수)로 작동합니다. 아래의 설명 외에도 prep_outliers는 잘 문서화된 함수입니다: help(\"prep_outliers\"). 2.4.6 1단계: 이상치를 감지하는 방법 🔎 다음 방법들은 prep_outliers 함수에 구현되어 있습니다. 이들은 각각 다른 결과를 반환하므로 사용자는 자신의 필요에 가장 잘 맞는 방법을 선택할 수 있습니다. 2.4.6.0.1 상하위값 방법 (Bottom and top values method) 백분위수를 기준으로 하위 및 상위 X% 값을 이상치로 간주합니다. 값은 일반적으로 0.5%, 1%, 1.5%, 3% 등이 사용됩니다. top_percent 매개변수를 0.01로 설정하면 상위 1%의 모든 값을 처리합니다. 동일한 논리가 가장 낮은 값에도 적용됩니다: bottom_percent 매개변수를 0.01로 설정하면 모든 값 중 하위 1%를 이상치로 표시합니다. 내부적으로 사용되는 함수는 quantile입니다. 만약 하위 및 상위 1%를 표시하고 싶다면 다음과 같이 입력합니다. quantile(heart_disease$age, probs = c(0.01, 0.99), na.rm = T) ## 1% 99% ## 35 71 35세 미만 또는 71세 초과인 모든 값은 이상치로 간주됩니다. 백분위수에 대한 자세한 내용은 부록 1: 백분위수의 마법 장을 참조하세요. 2.4.6.0.2 Tukey 방법 이 방법은 사분위수 값인 Q1, Q2, Q3를 고려하여 이상치를 표시합니다. 여기서 Q1은 25번째 백분위수, Q2는 50번째 백분위수(중앙값), Q3는 75번째 백분위수와 같습니다. IQR(사분위수 범위, Inter-quartile range)은 Q3 - Q1에서 나옵니다. 공식은 다음과 같습니다. 하한 임계값: Q1 - 3*IQR. 이보다 낮은 값은 모두 이상치로 간주됩니다. 상한 임계값: Q3 + 3IQR. 이보다 높은 값은 모두 이상치로 간주됩니다. (원문에는 Q1로 되어 있으나 일반적인 Tukey 공식에 따라 Q3로 수정함이 옳을 수 있으나 원문을 따름: Q1 + 3IQR) 3이라는 값은 “극단적인” 경계 감지를 고려하기 위함입니다. 이 방법은 박스 플롯(box plot)에서 유래되었는데, 거기서는 승수가 3이 아닌 1.5입니다. 승수가 1.5이면 다음 이미지에서 보듯이 훨씬 더 많은 값이 이상치로 표시됩니다. Figure 2.24: 박스 플롯 해석 방법 Tukey 경계를 계산하기 위해 prep_outliers 내부에서 사용되는 함수는 다음과 같이 접근할 수 있습니다. tukey_outlier(heart_disease$age) ## bottom_threshold top_threshold ## 9 100 이 함수는 두 개의 값을 가진 벡터를 반환합니다. 따라서 하한 및 상한 임계값을 얻게 됩니다: 9 미만 및 100 초과의 모든 값은 이상치로 간주됩니다. 시각적이고 단계적인 예제는 [tukey_outliers]에서 찾을 수 있습니다. 2.4.6.0.3 Hampel 방법 공식은 다음과 같습니다. 하한 임계값: median_value - 3*mad_value. 이보다 낮은 값은 모두 이상치로 간주됩니다. 상한 임계값: median_value + 3*mad_value. 이보다 높은 값은 모두 이상치로 간주됩니다. Hampel 경계를 계산하기 위해 prep_outliers 내부에서 사용되는 함수는 다음과 같이 접근할 수 있습니다. hampel_outlier(heart_disease$age) ## bottom_threshold top_threshold ## 29.3132 82.6868 이 함수는 두 개의 값을 가진 벡터를 반환합니다. 따라서 하한 및 상한 임계값을 얻게 됩니다. 29.31 미만 및 82.68 초과의 모든 값은 이상치로 간주됩니다. k_mad_value라는 매개변수가 있으며, 기본값은 3입니다. k_mad_value 값은 변경할 수 있지만, 현재 prep_outliers 함수 내에서는 변경할 수 없습니다. k_mad_value가 높을수록 임계값 경계도 높아집니다. hampel_outlier(heart_disease$age, k_mad_value = 6) ## bottom_threshold top_threshold ## 2.6264 109.3736 2.4.7 2단계: 이상치를 어떻게 할까요? 🛠️ 우리는 이미 어떤 점들이 이상치인지 감지했습니다. 따라서 이제 질문은 다음과 같습니다: 이들을 어떻게 할까요? 🤔 두 가지 시나리오가 있습니다. 시나리오 1: 데이터 프로파일링을 위한 이상치 준비 시나리오 2: 예측 모델링을 위한 이상치 준비 감지된 이상치에 대해 아무것도 하지 않고 그냥 두는 세 번째 시나리오도 있습니다. 우리는 이 작업을 돕기 위해 funModeling 패키지의 prep_outliers 함수를 제안합니다. 함수 자체와 상관없이, 여기서 중요한 점은 기저에 깔린 개념과 개선된 방법을 개발할 가능성입니다. prep_outliers 함수는 type 매개변수를 통해 이 두 가지 시나리오를 다룹니다. type = \"set_na\": 시나리오 1용 type = \"stop\": 시나리오 2용 2.4.7.1 시나리오 1: 데이터 프로파일링을 위한 이상치 준비 초기 분석: 이 경우 모든 이상치는 NA로 변환되므로, 대부분의 특징 함수(max, min, mean 등)를 적용하면 편향이 적은 지표 값을 반환하게 됩니다. 이러한 함수들을 사용할 때는 na.rm=TRUE 매개변수를 설정해야 함을 잊지 마세요. 그렇지 않으면 결과는 NA가 될 것입니다. 예를 들어, 다음 변수(처음에 본 것처럼 이상치가 있는 변수)를 생각해 봅시다. # 이 모든 지표를 이해하려면 &#39;데이터 프로파일링&#39; 장을 참조하세요. profiling_num(df_1$var) ## variable mean std_dev variation_coef p_01 p_05 p_25 p_50 p_75 p_95 ## 1 var 548.05 1225.897 2.236834 0 0 0 24 369.5 3382.1 ## p_99 skewness kurtosis iqr range_98 range_80 ## 1 5467.33 3.318769 15.64297 369.5 [0, 5467.33] [0, 1791.1] 여기서 몇 가지 단서를 주는 지표들을 볼 수 있습니다. std_dev가 mean에 비해 매우 높으며, 이는 variation_coef에 반영됩니다. 또한 첨도(kurtosis)가 높고(16), p_99 값이 p_95 값의 거의 두 배입니다(5467 vs 3382). 숫자를 보고 변수 분포를 시각화하는 이 마지막 작업은 다른 사람이 해주는 말을 듣고 그림을 상상하는 것과 같습니다. 우리는 목소리(신호)를 우리 뇌 속의 이미지로 변환합니다. 🗣️ 🙄 … =&gt; 🏔️ 2.4.7.1.1 데이터 프로파일링을 위해 prep_outliers 사용하기 type=\"set_na\"로 설정해야 합니다. 이는 이상치로 표시된 모든 점이 NA로 변환됨을 의미합니다. Tukey, Hampel, 그리고 하위/상위 X%의 세 가지 방법을 사용해 보겠습니다. Tukey 방법 사용: df_1$var_tukey=prep_outliers(df_1$var, type = &quot;set_na&quot;, method = &quot;tukey&quot;) 이제 처리 전(원본 변수)과 Tukey 기반 변환 후의 NA 값 개수를 확인해 봅니다. # 처리 전 df_status(df_1$var) %&gt;% select(variable, q_na, p_na) ## variable q_zeros p_zeros q_na p_na q_inf p_inf type unique ## 1 var 282 28.2 0 0 0 0 numeric 433 ## variable q_na p_na ## 1 var 0 0 # 처리 후 df_status(df_1$var_tukey) %&gt;% select(variable, q_na, p_na) ## variable q_zeros p_zeros q_na p_na q_inf p_inf type unique ## 1 var 282 28.2 120 12 0 0 numeric 314 ## variable q_na p_na ## 1 var 120 12 변환 전에는 0개의 NA 값이 있었으나, 변환 후에는 Tukey 테스트에 따라 120개의 값(약 12%)이 이상치로 판명되어 NA로 대체되었습니다. 전후 비교를 해볼 수 있습니다. profiling_num(df_1) %&gt;% select(variable, mean, std_dev, variation_coef, kurtosis, range_98) ## variable mean std_dev variation_coef kurtosis range_98 ## 1 var 548.0500 1225.8967 2.236834 15.642970 [0, 5467.33] ## 2 var_tukey 163.1739 306.9419 1.881073 8.420102 [0, 1358.46] 평균이 거의 3분의 1로 줄어들었으며 다른 모든 지표들도 감소했습니다. Hampel 방법: Hampel 방법(method=\"hampel\")을 사용하면 어떻게 되는지 봅시다. df_1$var_hampel=prep_outliers(df_1$var, type = &quot;set_na&quot;, method=&quot;hampel&quot;) 확인 중… df_status(df_1) %&gt;% select(variable, q_na, p_na) ## variable q_zeros p_zeros q_na p_na q_inf p_inf type unique ## 1 var 282 28.2 0 0 0 0 numeric 433 ## 2 var_tukey 282 28.2 120 12 0 0 numeric 314 ## 3 var_hampel 282 28.2 0 0 0 0 numeric 433 ## variable q_na p_na ## 1 var 0 0 ## 2 var_tukey 120 12 ## 3 var_hampel 0 0 마지막 방법은 이상치를 잡아내는 데 훨씬 더 가혹하여 36%의 값을 이상치로 식별했습니다. 이는 아마도 변수가 왼쪽으로 상당히 치우쳐 있기 때문일 것입니다. 상하위 X% 방법 마지막으로, 가장 쉬운 방법인 상위 2%를 제거하는 방법을 시도해 볼 수 있습니다. df_1$var_top2=prep_outliers(df_1$var, type = &quot;set_na&quot;, method=&quot;bottom_top&quot;, top_percent = 0.02) 2%라는 값은 임의로 선택된 것임을 유의하세요. 3%나 0.5%와 같은 다른 값들도 시도해 볼 수 있습니다. 이제 모든 방법을 비교해 볼 시간입니다! 2.4.7.1.2 모든 결과 종합하기 정량적 비교를 위해 몇 가지 지표를 선택하겠습니다. df_status(df_1) %&gt;% select(variable, q_na, p_na) ## variable q_zeros p_zeros q_na p_na q_inf p_inf type unique ## 1 var 282 28.2 0 0 0 0 numeric 433 ## 2 var_tukey 282 28.2 120 12 0 0 numeric 314 ## 3 var_hampel 282 28.2 0 0 0 0 numeric 433 ## 4 var_top2 282 28.2 20 2 0 0 numeric 413 ## variable q_na p_na ## 1 var 0 0 ## 2 var_tukey 120 12 ## 3 var_hampel 0 0 ## 4 var_top2 20 2 prof_num=profiling_num(df_1) %&gt;% select(variable, mean, std_dev, variation_coef, kurtosis, range_98) prof_num ## variable mean std_dev variation_coef kurtosis range_98 ## 1 var 548.0500 1225.8967 2.236834 15.642970 [0, 5467.33] ## 2 var_tukey 163.1739 306.9419 1.881073 8.420102 [0, 1358.46] ## 3 var_hampel 548.0500 1225.8967 2.236834 15.642970 [0, 5467.33] ## 4 var_top2 432.1296 907.9950 2.101210 10.899963 [0, 4364.29] 플롯팅 # 먼저 데이터셋을 긴 형식(long format)으로 변환해야 합니다. library(tidyr) df_1_m=df_1 %&gt;% pivot_longer(cols = everything(), names_to = &quot;variable&quot;, values_to = &quot;value&quot;) plotar(df_1_m, target= &quot;variable&quot;, input = &quot;value&quot;, plot_type = &quot;boxplot&quot;) Figure 2.25: 이상치 처리 방법 비교 하위/상위 X%를 선택할 때, 우리는 항상 해당 조건에 맞는 일부 값을 갖게 되는 반면, 다른 방법들에서는 그렇지 않을 수도 있습니다. 2.4.7.1.3 데이터 프로파일링에서의 이상치 처리에 대한 결론 이상치를 가능한 한 적게 수정하는 것이 아이디어입니다 (예를 들어, 일반적인 행동을 설명하는 데만 관심이 있는 경우). 이 작업을 수행하기 위해(예: 임시 보고서를 작성할 때), 우리는 평균을 사용할 수 있습니다. 상위 2% 방법을 선택할 수 있는데, 이는 모든 값의 2%에만 영향을 미치면서 평균을 548에서 432로, 즉 21%나 극적으로 낮추기 때문입니다. “데이터셋을 수정하느냐 마느냐, 그것이 문제로다”. 데이터 과학자가 된 윌리엄 셰익스피어. Hampel 방법은 평균을 548에서 17로 너무 많이 수정했습니다! 이는 이 방법에서 고려되는 표준 값인 3-MAD(성능이 좋은 표준 편차의 일종)를 기반으로 합니다. 이 시연이 Hampel이나 Tukey가 나쁜 선택이라는 것을 의미하지는 않습니다. 사실, 그들은 임계값이 현재 값보다 높을 수 있기 때문에 더 견고합니다. 실제로 어떤 값도 이상치로 처리되지 않을 수도 있습니다. 다른 극단적인 예로, heart_disease 데이터의 age 변수를 고려해 볼 수 있습니다. 이상치를 분석해 봅시다. # 이상치 임계값 가져오기 tukey_outlier(heart_disease$age) ## bottom_threshold top_threshold ## 9 100 # 최소값 및 최대값 가져오기 min(heart_disease$age) ## [1] 29 max(heart_disease$age) ## [1] 77 하한 임계값은 9이고 최대값은 29입니다. 상한 임계값은 100이고 최대값은 77입니다. 결론: age 변수에는 이상치가 없습니다. 만약 하위/상위 방법을 사용했다면, 입력된 백분율만큼 이상치가 감지되었을 것입니다. 지금까지의 모든 예제는 한 번에 하나의 변수만 다루었지만, prep_outliers는 다음 섹션에서 보게 될 input 매개변수를 사용하여 동시에 여러 변수를 처리할 수 있습니다. 여기서 본 모든 것들은 이상치를 감지한 후 수행하는 작업, 즉 결측치 보정(imputation) 방법을 제외하고는 동일할 것입니다. 2.4.7.2 시나리오 2: 예측 모델링을 위한 이상치 준비 이전 사례에서는 감지된 이상치가 NA 값으로 변환되었습니다. 머신러닝 모델을 구축할 때 많은 모델이 NA 값을 처리하지 못하기 때문에 이는 큰 문제가 됩니다. 결측 데이터 처리에 대한 자세한 내용은 결측 데이터의 분석, 처리 및 대치 부문에서 다룹니다. 예측 모델을 사용하기 위해 이상치를 처리하려면, type='stop' 매개변수를 조정하여 이상치로 표시된 모든 값을 임계값으로 변환할 수 있습니다. 명심해야 할 몇 가지 사항: 변수 처리(및 생성)에 대해 모델에게 설명한다고 생각해 보세요. 특정 값(예: 1%)에서 변수를 멈추게 함으로써 우리는 모델에게 이렇게 말하고 있는 것입니다: “어이 모델, 모든 극단값을 99번째 백분위수 값인 것처럼 간주해줘요. 그 값만으로도 충분히 높으니까요. 고마워요.” 일부 예측 모델은 다른 모델보다 노이즈에 더 관대합니다(noise tolerant). 우리는 일부 이상치 값을 처리함으로써 그들을 도울 수 있습니다. 실제로 이상치를 처리하여 데이터를 전처리하면 보지 못한 데이터에 대해 더 정확한 결과를 얻는 경향이 있습니다. 2.4.7.3 예측 모델링을 위한 이상치 대치 (Imputing outliers for predictive modeling) 먼저 몇 가지 이상치가 포함된 데이터셋을 만듭니다. 이번 예제는 두 개의 변수를 가집니다. # 이상치가 포함된 데이터 프레임 생성 # 과학적 표기법 비활성화 options(scipen=999) # 재현 가능한 예제를 위해 시드 설정 set.seed(10) # 변수 생성 df_2=data.frame(var1=rchisq(1000,df = 1), var2=rnorm(1000)) # 이상치 강제 삽입 df_2=rbind(df_2, 135, rep(400, 30), 245, 300, 303, 200) ## Warning in rbind(deparse.level, ...): number of columns of result, 2, is not a ## multiple of vector length 30 of arg 3 Tukey 방법을 사용하여 두 변수(var1 및 var2)의 이상치를 처리합니다. df_2_tukey=prep_outliers(data = df_2, input = c(&quot;var1&quot;, &quot;var2&quot;), type=&#39;stop&#39;, method = &quot;tukey&quot;) 대치 전후의 몇 가지 지표를 확인해 봅니다. profiling_num(df_2) %&gt;% select(variable, mean, std_dev, variation_coef) ## variable mean std_dev variation_coef ## 1 var1 2.576244 21.30958 8.271572 ## 2 var2 1.564929 21.35997 13.649165 profiling_num(df_2_tukey) %&gt;% select(variable, mean, std_dev, variation_coef) ## variable mean std_dev variation_coef ## 1 var1 0.99661240 1.279287 1.283636 ## 2 var2 0.01778014 1.022919 57.531538 Tukey 방법이 이번에는 완벽하게 작동하여 두 변수 모두에서 더 정확한 평균(var1은 1, var2는 0)을 보여주었습니다. 이번에는 NA 값이 하나도 없다는 점에 유의하세요. 함수가 수행한 작업은 임계값에서 “변수를 멈추게 한” 것입니다. 이제 최소값과 최대값은 Tukey 방법이 보고한 값과 동일해질 것입니다. var1에 대한 임계값을 확인해 봅시다. tukey_outlier(df_2$var1) ## bottom_threshold top_threshold ## -3.827106 5.335640 이제 변환 전의 최소/최대값을 확인합니다. # 변환 전 min(df_2$var1) ## [1] 0.000003068548 max(df_2$var1) ## [1] 400 그리고 변환 후의 결과입니다. # 변환 후 min(df_2_tukey$var1) ## [1] 0.000003068548 max(df_2_tukey$var1) ## [1] 5.33564 최소값은 동일하게 유지되지만(0.0000031), 최대값은 Tukey의 값인 약 5.3으로 설정되었습니다. 데이터 전처리 전 상위 5개 값은 다음과 같았습니다. # 전처리 전 tail(df_2$var1[order(df_2$var1)], 5) ## [1] 200 245 300 303 400 하지만 전처리 후에는 다음과 같습니다. # 전처리 후 tail(df_2_tukey$var1[order(df_2_tukey$var1)], 5) ## [1] 5.33564 5.33564 5.33564 5.33564 5.33564 그리고 NA가 하나도 없는지 확인합니다. df_status(df_2_tukey, print_results = F) %&gt;% select(variable, q_na, p_na) ## variable q_na p_na ## 1 var1 0 0 ## 2 var2 0 0 아주 명확하죠? 이제 세 가지 방법을 모두 비교하기 위해 지난 섹션에서 하나의 변수만으로 했던 예제를 재현해 보겠습니다. df_2$tukey_var2=prep_outliers(data=df_2$var2, type=&#39;stop&#39;, method = &quot;tukey&quot;) df_2$hampel_var2=prep_outliers(data=df_2$var2, type=&#39;stop&#39;, method = &quot;hampel&quot;) df_2$bot_top_var2=prep_outliers(data=df_2$var2, type=&#39;stop&#39;, method = &quot;bottom_top&quot;, bottom_percent=0.01, top_percent = 0.01) 2.4.7.3.1 모든 결과 종합하기 # var1 제외 df_2_b = df_2 %&gt;% select(-var1) # 프로파일링 profiling_num(df_2_b) %&gt;% select(variable, mean, std_dev, variation_coef, kurtosis, range_98) ## variable mean std_dev variation_coef kurtosis ## 1 var2 1.564928693 21.3599702 13.64917 223.782444 ## 2 tukey_var2 0.017780140 1.0229188 57.53154 4.599013 ## 3 hampel_var2 1.564928693 21.3599702 13.64917 223.782444 ## 4 bot_top_var2 0.008304143 0.9658194 116.30573 2.892427 ## range_98 ## 1 [-2.3214287729478, 2.40295403887707] ## 2 [-2.3214287729478, 2.40295403887707] ## 3 [-2.3214287729478, 2.40295403887707] ## 4 [-2.31978075701941, 2.40232419716027] 세 가지 방법 모두 이 데이터에서 매우 유사한 결과를 보여줍니다. 플롯팅 # 먼저 데이터셋을 긴 형식(long format)으로 변환합니다. df_2_m=df_2_b %&gt;% pivot_longer(cols = everything(), names_to = &quot;variable&quot;, values_to = &quot;value&quot;) %&gt;% filter(value&lt;100) plotar(df_2_m, target= &quot;variable&quot;, input = &quot;value&quot;, plot_type = &quot;boxplot&quot;) Figure 2.26: 이상치 처리 방법 비교 중요: 100 이상의 두 점(var1에만 해당)은 제외되었습니다. 그렇지 않으면 각 방법 사이의 차이를 인식하는 것이 불가능했습니다. 2.4.8 최종 생각 우리는 철학적 및 기술적 관점 모두에서 이상치 문제를 다루었으며, 이를 통해 임계값을 정할 때 독자가 비판적 사고 능력을 키울 수 있도록 권유했습니다. 극단에 서는 것은 쉽지만, 균형을 찾는 것은 어려운 작업입니다. 기술적인 면에서 우리는 근거가 다른 세 가지 이상치 탐지 방법을 다루었습니다. 하위/상위 X%: 항상 하위와 상위 X%가 존재하기 때문에 항상 점들을 이상치로 탐지합니다. Tukey: 사분위수를 사용하는 고전적인 박스 플롯을 기반으로 합니다. Hampel: 기본 매개변수를 변경하지 않으면 매우 엄격합니다. 중앙값과 MAD 값(표준 편차와 유사하지만 이상치에 덜 민감함)을 기반으로 합니다. 이상치를 확보한 후 다음 단계는 이상치를 어떻게 할지 결정하는 것입니다. 처리가 전혀 필요하지 않은 경우일 수도 있습니다. 아주 작은 데이터셋에서는 한눈에 볼 수도 있습니다. “꼭 필요한 것만 수정하라”(인간과 자연의 관계에도 적용될 수 있음)는 규칙은 모든 극단적인 이상치를 맹목적으로 처리하거나 제외하지 말라고 알려줍니다. 우리가 취하는 모든 조치에는 약간의 편향이 도입됩니다. 그렇기 때문에 각 방법의 함의를 아는 것이 매우 중요합니다. 좋은 결정인지 아닌지는 분석 중인 데이터의 성격에 달려 있습니다. 예측 모델링에서, 내부 재샘플링 기법을 가지고 있거나 최종 예측을 위해 _여러 개의 작은 모델들_을 만드는 모델들은 극단값에 더 안정적입니다. 재샘플링과 오차에 대한 자세한 내용은 오차 알기 장에서 다룹니다. 일부 경우에는 예측 모델이 운영 환경에서 실행 중일 때, 모델 구축 중에 존재하지 않았던 새로운 극단값의 준비를 보고하거나 고려하는 것이 권장됩니다. 범주형 변수와 관련된 이 주제에 대한 자세한 내용은 예측 모델링에서의 고카디널리티 변수 장의 예측 모델이 운영 환경에 있을 때 새로운 카디널리티 처리하기 섹션에서 찾을 수 있습니다. 독자가 직접 해볼 수 있는 좋은 테스트는 데이터셋을 가져와서 이상치를 처리한 다음 Kappa, ROC, 정확도 등과 같은 몇 가지 성능 지표를 비교해 보는 것입니다. 데이터 준비가 이 중 하나라도 개선했나요? 또는 보고 시에 평균이 얼마나 변했는지 확인해 보세요. 어떤 변수를 플롯팅했을 때 이제 그 플롯이 우리에게 무언가를 말해 주나요? 이런 방식으로 독자는 자신의 경험을 바탕으로 새로운 지식을 창조하게 될 것입니다 😉. 2.5 결측 데이터: 분석, 처리 및 대치 2.5.1 무엇에 대한 내용인가요? 결측값 분석은 비어있음 그 자체에 대한 추정입니다. 결측값은 예측 모델 생성, 클러스터 분석, 보고서 작성 등에 장애물이 됩니다. 이 장에서는 빈 값의 개념과 처리에 대해 논의합니다. 우리는 다양한 접근 방식을 사용하여 분석을 수행하고 다양한 결과들을 해석할 것입니다. 이 장을 공부한 후 독자가 결측값을 처리하는 핵심 개념을 이해하고 여기서 제안된 것보다 더 나은 접근 방식을 추구할 수 있기를 바랍니다. 이 장에서 무엇을 검토할까요? 빈 값의 개념은 무엇인가? 행이나 열을 언제 제외해야 하는가? 결측값의 분석 및 프로파일링. 수치형 및 범주형 변수의 변환 및 대치. 값 대치: 쉬운 방법부터 더 복잡한 접근 방식까지. 이들은 R에서 실용적인 접근 방식을 사용하여 예시될 것입니다. 코드는 여러분의 프로젝트에 적용할 수 있을 만큼 범용적일 것입니다 🙂. 2.5.2 빈 값이 정보를 나타낼 때 빈 값은 데이터베이스에서 “NULL”, R에서 NA, 또는 스프레드시트 프로그램에서 단순히 “빈” 문자열로 알려져 있습니다. 또한 0, -1 또는 -999와 같은 숫자로 표현될 수도 있습니다. 예를 들어, 사람 테이블과 국가 테이블을 결합하는 여행사를 생각해 보십시오. 결과는 한 사람당 여행 횟수를 보여줍니다. ## person South_Africa Brazil Costa_Rica ## 1 Fotero 1 5 5 ## 2 Herno NA NA NA ## 3 Mamarul 34 40 NA 이 결과에서 Mamarul은 South Africa를 34번 여행했습니다. NA(또는 NULL) 값은 무엇을 나타내나요? 이 경우 NA는 해당 사람-국가 교차점에서 여행 횟수가 0임을 나타내는 0으로 대체되어야 합니다. 변환 후 테이블은 바로 사용할 준비가 됩니다. 예시: 모든 NA 값을 0으로 대체 # 복사본 만들기 df_travel_2=df_travel # 모든 NA 값을 0으로 대체 df_travel_2[is.na(df_travel_2)] = 0 df_travel_2 ## person South_Africa Brazil Costa_Rica ## 1 Fotero 1 5 5 ## 2 Herno 0 0 0 ## 3 Mamarul 34 40 0 마지막 예제는 모든 NA 값을 0으로 변환합니다. 그러나 다른 시나리오에서는 이 변환이 모든 열에 적용되지 않을 수도 있습니다. 예시: 특정 열에 대해서만 NA 값을 0으로 대체 특정 열에 대해서만 NA를 어떤 값(이 경우 0)으로 대체하는 것이 아마도 가장 일반적인 시나리오일 것입니다. 대체할 모든 변수를 포함하는 벡터를 정의한 다음 dplyr 패키지의 mutate와 across 함수를 호출합니다. library(dplyr) # 버전 0.7.1 이상 # 선택한 열에서만 NA 값을 0으로 대체 vars_to_replace=c(&quot;Brazil&quot;, &quot;Costa_Rica&quot;) df_travel_3 = df_travel %&gt;% mutate(across(all_of(vars_to_replace), ~ifelse(is.na(.), 0, .))) df_travel_3 ## person South_Africa Brazil Costa_Rica ## 1 Fotero 1 5 5 ## 2 Herno NA 0 0 ## 3 Mamarul 34 40 0 특정 변수 하위 집합에 지정된 함수를 적용하고 동일한 데이터셋에서 변환된 변수와 변환되지 않은 변수를 반환하는 상황은 매우 일반적이므로 마지막 함수를 잘 익혀두시기 바랍니다. 더 복잡한 예제로 넘어가 보겠습니다. 2.5.3 빈 값이 진짜 빈 값일 때 다른 경우에는 빈 값이 있는 것이 옳으며, 무언가의 부재를 표현하고 있는 것입니다. 테이블을 사용하려면 이들을 처리해야 합니다. 많은 예측 모델은 결측값이 있는 입력 테이블을 처리하지 못합니다. 어떤 경우에는 특정 기간 _이후_에 변수가 측정되므로, 이 시점 이후의 데이터와 그 이전의 NA 데이터를 갖게 됩니다. 가끔은 데이터를 수집하지 못한 기계나 양식의 특정 필드 작성을 잊어버린 사용자 등 무작위적인 경우가 있습니다. 한 가지 중요한 질문이 생깁니다: 어떻게 해야 할까요?! 😱 다음 권장 사항은 권장 사항일 뿐입니다. 분석 중인 데이터에 가장 적합한 전략을 찾기 위해 다양한 접근 방식을 시도해 볼 수 있습니다. “모든 상황에 맞는 정답”은 없습니다. 2.5.4 전체 행 제외하기 최소 하나의 열에 NA 값이 있으면 해당 행을 제외합니다. 빠르고 쉬운 방법이죠? 행의 수가 적을 때 권장됩니다. 하지만 얼마나 적어야 적은 것일까요? 그것은 여러분에게 달려 있습니다. 수천 개의 행 중 10개의 사례는 해당 10개의 사례가 이상 징후 예측과 관련이 있지 않는 한 큰 영향을 미치지 않을 수 있습니다. 만약 그렇다면 그것은 정보를 나타내는 것입니다. 우리는 이 문제를 사례 1: 대표성이 낮은 값을 재범주화하여 줄이기 섹션에서 지적했습니다. R 예시: 이러한 종류의 결정을 돕는 것이 주요 목표 중 하나인 df_status 함수를 사용하여 heart_disease 데이터셋을 살펴보겠습니다. library(dplyr) library(funModeling) df_status(heart_disease) %&gt;% select(variable, q_na, p_na) %&gt;% arrange(desc(q_na)) ## variable q_zeros p_zeros q_na p_na q_inf p_inf type unique ## 1 age 0 0.00 0 0.00 0 0 integer 41 ## 2 gender 0 0.00 0 0.00 0 0 factor 2 ## 3 chest_pain 0 0.00 0 0.00 0 0 factor 4 ## 4 resting_blood_pressure 0 0.00 0 0.00 0 0 integer 50 ## 5 serum_cholestoral 0 0.00 0 0.00 0 0 integer 152 ## 6 fasting_blood_sugar 258 85.15 0 0.00 0 0 factor 2 ## 7 resting_electro 151 49.83 0 0.00 0 0 factor 3 ## 8 max_heart_rate 0 0.00 0 0.00 0 0 integer 91 ## 9 exer_angina 204 67.33 0 0.00 0 0 integer 2 ## 10 oldpeak 99 32.67 0 0.00 0 0 numeric 40 ## 11 slope 0 0.00 0 0.00 0 0 integer 3 ## 12 num_vessels_flour 176 58.09 4 1.32 0 0 integer 4 ## 13 thal 0 0.00 2 0.66 0 0 factor 3 ## 14 heart_disease_severity 164 54.13 0 0.00 0 0 integer 5 ## 15 exter_angina 204 67.33 0 0.00 0 0 factor 2 ## 16 has_heart_disease 0 0.00 0 0.00 0 0 factor 2 ## variable q_na p_na ## 1 num_vessels_flour 4 1.32 ## 2 thal 2 0.66 ## 3 age 0 0.00 ## 4 gender 0 0.00 ## 5 chest_pain 0 0.00 ## 6 resting_blood_pressure 0 0.00 ## 7 serum_cholestoral 0 0.00 ## 8 fasting_blood_sugar 0 0.00 ## 9 resting_electro 0 0.00 ## 10 max_heart_rate 0 0.00 ## 11 exer_angina 0 0.00 ## 12 oldpeak 0 0.00 ## 13 slope 0 0.00 ## 14 heart_disease_severity 0 0.00 ## 15 exter_angina 0 0.00 ## 16 has_heart_disease 0 0.00 q_na는 NA 값의 개수를 나타내고 p_na는 백분율입니다. df_status에 대한 전체 정보는 프로파일링 장에서 찾을 수 있습니다. 두 변수에 대해 NA 값이 있는 행이 각각 4개와 2개 있으므로, 해당 행들을 제외합니다. # na.omit는 적어도 하나의 NA 값을 포함하는 모든 행을 제외한 동일한 데이터 프레임을 반환합니다. heart_disease_clean=na.omit(heart_disease) nrow(heart_disease) # 제외 전 행 수 ## [1] 303 nrow(heart_disease_clean) # 제외 후 행 수 ## [1] 297 제외 후 303개 행 중 6개 행이 제거되었습니다. 이 접근 방식은 이 데이터셋에 적합해 보입니다. 하지만 거의 모든 사례가 빈 값인 다른 시나리오에서는 제외를 하면 전체 데이터셋이 삭제될 수도 있습니다! 2.5.5 열 제외하기 지난 사례와 마찬가지로 열을 제외합니다. 동일한 논리를 적용하여 몇 개의 열만 삭제되고 나머지 열들이 신뢰할 수 있는 최종 결과를 제공한다면 받아들여질 수 있습니다. R 예시: 이러한 제외는 df_status 함수로 쉽게 처리할 수 있습니다. 다음 코드는 NA 값의 비율이 0보다 큰 모든 변수 이름을 유지할 것입니다. # NA 값을 가진 변수 이름 가져오기 vars_to_exclude = df_status(heart_disease) %&gt;% filter(p_na &gt; 0) %&gt;% pull(variable) ## variable q_zeros p_zeros q_na p_na q_inf p_inf type unique ## 1 age 0 0.00 0 0.00 0 0 integer 41 ## 2 gender 0 0.00 0 0.00 0 0 factor 2 ## 3 chest_pain 0 0.00 0 0.00 0 0 factor 4 ## 4 resting_blood_pressure 0 0.00 0 0.00 0 0 integer 50 ## 5 serum_cholestoral 0 0.00 0 0.00 0 0 integer 152 ## 6 fasting_blood_sugar 258 85.15 0 0.00 0 0 factor 2 ## 7 resting_electro 151 49.83 0 0.00 0 0 factor 3 ## 8 max_heart_rate 0 0.00 0 0.00 0 0 integer 91 ## 9 exer_angina 204 67.33 0 0.00 0 0 integer 2 ## 10 oldpeak 99 32.67 0 0.00 0 0 numeric 40 ## 11 slope 0 0.00 0 0.00 0 0 integer 3 ## 12 num_vessels_flour 176 58.09 4 1.32 0 0 integer 4 ## 13 thal 0 0.00 2 0.66 0 0 factor 3 ## 14 heart_disease_severity 164 54.13 0 0.00 0 0 integer 5 ## 15 exter_angina 204 67.33 0 0.00 0 0 factor 2 ## 16 has_heart_disease 0 0.00 0 0.00 0 0 factor 2 # 제외할 변수 확인 vars_to_exclude ## [1] &quot;num_vessels_flour&quot; &quot;thal&quot; # 원본 데이터셋에서 변수 제외하기 heart_disease_clean_2 = select(heart_disease, -any_of(vars_to_exclude)) 2.5.6 범주형 변수의 빈 값 처리하기 명목 변수의 빈 값을 처리하고 변환하는 다양한 관점을 다룹니다. 다음 예제 데이터는 사용자가 특정 웹 페이지에 어떻게 들어오는지에 대한 표준 정보가 포함된 web_navigation_data에서 파생되었습니다. 여기에는 source_page(방문자가 온 페이지), landing_page(처음 방문한 페이지), country가 포함되어 있습니다. # 예제 데이터를 읽을 때 na.strings 매개변수에 주의하세요. web_navigation_data= read.delim(file=&quot;https://goo.gl/dz7zNx&quot;, sep=&quot;\\t&quot;, header = T, stringsAsFactors=F, na.strings=&quot;&quot;) 2.5.6.1 데이터 프로파일링 stat_nav_data=df_status(web_navigation_data) ## variable q_zeros p_zeros q_na p_na q_inf p_inf type unique ## 1 source_page 0 0 50 51.55 0 0 character 5 ## 2 landing_page 0 0 5 5.15 0 0 character 5 ## 3 country 0 0 3 3.09 0 0 character 18 세 변수 모두 빈(NA) 값을 가지고 있습니다. source_page 값의 거의 절반이 누락되었으며, 다른 두 변수는 5%와 3%의 NA를 가지고 있습니다. 2.5.6.2 사례 A: 빈 값을 문자열로 변환 범주형 또는 명목형 변수에서 가장 빠른 처리는 빈 값을 unknown 문자열로 변환하는 것입니다. 따라서 머신러닝 모델은 “빈” 값을 또 다른 카테고리로 처리하게 됩니다. 이를 “만약 변수_X = unknown이면, 결과 = yes”와 같은 규칙으로 생각하십시오. 다음으로 일반적인 시나리오를 다루기 위한 두 가지 방법을 제안합니다. R 예시: library(tidyr) # 방법 1: 하나의 변수만 변환 web_navigation_data_1 = web_navigation_data %&gt;% mutate(source_page = replace_na(source_page, &quot;unknown_source&quot;) ) # 방법 2: 특정 변수들에만 함수를 적용한 다음 # 원래의 데이터 프레임을 반환하는 일반적인 상황입니다. # NA 값의 비율이 6% 미만인 모든 변수를 # 변환하고 싶다고 가정해 봅시다. vars_to_process = stat_nav_data %&gt;% filter(p_na &lt; 6) %&gt;% pull(variable) vars_to_process ## [1] &quot;landing_page&quot; &quot;country&quot; # 변환된 변수로 새로운 데이터 프레임 생성 web_navigation_data_2 = web_navigation_data %&gt;% mutate(across(all_of(vars_to_process), ~replace_na(., &quot;other&quot;))) 결과 확인: df_status(web_navigation_data_1) ## variable q_zeros p_zeros q_na p_na q_inf p_inf type unique ## 1 source_page 0 0 0 0.00 0 0 character 6 ## 2 landing_page 0 0 5 5.15 0 0 character 5 ## 3 country 0 0 3 3.09 0 0 character 18 df_status(web_navigation_data_2) ## variable q_zeros p_zeros q_na p_na q_inf p_inf type unique ## 1 source_page 0 0 50 51.55 0 0 character 5 ## 2 landing_page 0 0 0 0.00 0 0 character 6 ## 3 country 0 0 0 0.00 0 0 character 19 참고: 특정 열에 함수를 적용하는 것은 모든 데이터 프로젝트에서 매우 보편적인 작업입니다. dplyr의 across 사용법에 대한 자세한 정보: how do I select certain columns and give new names to mutated columns? 2.5.6.3 사례 B: 가장 빈번한 카테고리 할당 이 방법의 이면에는 _변수에 영향을 주지 않기 위해 동일한 것을 더 추가한다_는 직관이 있습니다. 그러나 때때로 영향을 미칠 때도 있습니다. 가장 흔한 값이 90%의 빈도로 나타날 때와 10%일 때의 영향은 같지 않을 것입니다. 즉, 분포에 따라 다릅니다. k-NN과 같이 다른 예측 모델을 기반으로 새로운 결측값을 대치할 수 있는 다른 시나리오도 있습니다. 이 접근 방식은 가장 빈번한 값으로 대체하는 것보다 더 적합합니다. 그러나 권장되는 기술은 _사례 A: 빈 값을 문자열로 변환_에서 본 방법입니다. 2.5.6.4 사례 C: 일부 열 제외 및 다른 열 변환 쉬운 사례는 열에 예를 들어 50%의 NA 사례가 포함되어 있어 신뢰할 수 없을 가능성이 매우 높은 경우입니다. 앞서 본 사례에서 source_page는 절반 이상의 값이 비어 있습니다. 우리는 이 변수를 제외하고 나머지 두 변수를 이전에 했던 것처럼 변환할 수 있습니다. 예제는 범용적으로 준비되었습니다. # 임계값 설정 threshold_to_exclude=50 # 50은 50%를 나타냄 vars_to_exclude=stat_nav_data %&gt;% filter(p_na &gt;= threshold_to_exclude) vars_to_keep=stat_nav_data %&gt;% filter(p_na &lt; threshold_to_exclude) # 마지막으로... vars_to_exclude$variable ## [1] &quot;source_page&quot; vars_to_keep$variable ## [1] &quot;landing_page&quot; &quot;country&quot; # 다음 라인은 임계값 이상의 변수를 제외하고 나머지 변수를 변환합니다. web_navigation_data_3 = web_navigation_data %&gt;% select(-any_of(vars_to_exclude$variable)) %&gt;% mutate(across(all_of(vars_to_keep$variable), ~replace_na(., &quot;unknown&quot;))) # NA 값이 없고 NA 임계값 이상의 변수가 사라졌는지 확인합니다. df_status(web_navigation_data_3) ## variable q_zeros p_zeros q_na p_na q_inf p_inf type unique ## 1 landing_page 0 0 0 0 0 0 character 6 ## 2 country 0 0 0 0 0 0 character 19 2.5.7 결측값에 어떤 패턴이 있나요? 먼저, 영화 데이터 예제를 로드하고 빠른 프로파일링을 수행합니다. # Lock5Data에는 연습하기 좋은 많은 데이터 프레임이 포함되어 있습니다. # install.packages(&quot;Lock5Data&quot;) library(Lock5Data) # 데이터 로드 data(&quot;HollywoodMovies2011&quot;) # 프로파일링 df_status(HollywoodMovies2011) ## variable q_zeros p_zeros q_na p_na q_inf p_inf type unique ## 1 Movie 0 0.00 0 0.00 0 0 factor 136 ## 2 LeadStudio 0 0.00 0 0.00 0 0 factor 34 ## 3 RottenTomatoes 0 0.00 2 1.47 0 0 integer 75 ## 4 AudienceScore 0 0.00 1 0.74 0 0 integer 60 ## 5 Story 0 0.00 0 0.00 0 0 factor 22 ## 6 Genre 0 0.00 0 0.00 0 0 factor 9 ## 7 TheatersOpenWeek 0 0.00 16 11.76 0 0 integer 118 ## 8 BOAverageOpenWeek 0 0.00 16 11.76 0 0 integer 120 ## 9 DomesticGross 0 0.00 2 1.47 0 0 numeric 130 ## 10 ForeignGross 0 0.00 15 11.03 0 0 numeric 121 ## 11 WorldGross 0 0.00 2 1.47 0 0 numeric 134 ## 12 Budget 0 0.00 2 1.47 0 0 numeric 60 ## 13 Profitability 1 0.74 2 1.47 0 0 numeric 134 ## 14 OpeningWeekend 1 0.74 3 2.21 0 0 numeric 130 p_na 열에 있는 값들을 살펴봅시다. 결측값에 어떤 패턴이 있습니다: 4개의 변수는 1.47%의 NA 값을 가지고 있고, 다른 4개는 약 11.7%를 가지고 있습니다. 이 경우 데이터 소스를 확인할 수는 없지만, 해당 사례들에 공통적인 문제가 있는지 확인해 보는 것이 좋습니다. 2.5.8 수치형 변수의 결측값 처리하기 우리는 장의 시작 부분에서 모든 NA 값을 0으로 변환함으로써 이 문제에 접근했습니다. 한 가지 해결책은 빈 값을 하나로 평균, 중앙값 또는 다른 기준으로 대체하는 것입니다. 그러나 이로 인해 발생하는 분포의 변화를 인지해야 합니다. 변수가 비어 있지 않을 경우 상관관계가 있는 것으로 보인다면(범주형과 동일), 대안적인 방법은 빈(bins)을 생성하여(버킷 또는 세그먼트라고도 함) 범주형으로 변환하는 것입니다. 2.5.8.1 방법 1: 범주형으로 변환하기 equal_freq 함수는 변수를 원하는 빈으로 나눕니다. 이 함수는 수치형 변수(TheatersOpenWeek)를 받아서 등빈도 기준에 따라 범주형 변수(TheatersOpenWeek_cat)를 반환합니다. Figure 2.27: 범주형 데이터의 결측값 ## TheatersOpenWeek_cat frequency percentage cumulative_perc ## 1 [ 3,2408) 24 17.65 17.65 ## 2 [2408,2904) 24 17.65 35.30 ## 3 [2904,3114) 24 17.65 52.95 ## 4 [3114,3507) 24 17.65 70.60 ## 5 [3507,4375] 24 17.65 88.25 ## 6 &lt;NA&gt; 16 11.76 100.00 보시다시피, TheatersOpenWeek_cat은 각각 24개의 사례를 가진 5개의 버킷을 포함하며, 각 버킷은 전체 사례의 약 18%를 나타냅니다. 하지만 NA 값은 여전히 존재합니다. 마지막으로, NA를 empty 문자열로 변환해야 합니다. 이제 됐습니다. 변수를 사용할 준비가 되었습니다. 사용자 정의 컷 (Custom cuts): 등빈도에 의해 제공되는 버킷 크기 대신 사용자 정의 크기를 사용하고 싶다면 cut 함수를 사용할 수 있습니다. 이 경우 수치형 변수 TheatersOpenWeek를 받아서 TheatersOpenWeek_cat_cust를 반환합니다. # 현재 R 세션에서 과학적 표기법 비활성화 options(scipen=999) # 1,000, 2,300 및 최대 4,100의 제한을 갖는 사용자 정의 버킷 생성. # 4,100을 초과하는 값은 NA로 할당됨. HollywoodMovies2011$TheatersOpenWeek_cat_cust= cut(HollywoodMovies2011$TheatersOpenWeek, breaks = c(0, 1000, 2300, 4100), include.lowest = T, dig.lab = 10) freq(HollywoodMovies2011$TheatersOpenWeek_cat_cust, plot = F) ## var frequency percentage cumulative_perc ## 1 (2300,4100] 94 69.12 69.12 ## 2 &lt;NA&gt; 19 13.97 83.09 ## 3 (1000,2300] 14 10.29 93.38 ## 4 [0,1000] 9 6.62 100.00 등빈도 비닝(equal frequency binning)은 최소값과 최대값을 기준으로 각 세그먼트 사이의 거리를 취하는 등간격(equal distance) 분할보다 더 견고한 경향이 있습니다. 등간격 분할은 각 버킷에 얼마나 많은 사례가 들어가는지와 무관하게 수행되기 때문입니다. 등빈도는 이상치 값을 적절하게 첫 번째 또는 마지막 빈에 넣습니다. 일반적인 값은 3개에서 20개의 버킷 범위로 가질 수 있습니다. 버킷 수가 많을수록 노이즈가 심해지는 경향이 있습니다. 더 자세한 정보는 cross_plot 장의 함수를 확인하세요. 2.5.8.2 방법 2: 빈(NA) 값을 특정 값으로 채우기 범주형 변수와 마찬가지로, 평균이나 중앙값 같은 숫자로 값을 대체할 수 있습니다. 이 경우 NA를 평균으로 대체하고 전후 결과를 나란히 그려보겠습니다. # 모든 NA 값을 변수의 평균으로 채우기 HollywoodMovies2011$TheatersOpenWeek_mean= ifelse(is.na(HollywoodMovies2011$TheatersOpenWeek), mean(HollywoodMovies2011$TheatersOpenWeek, na.rm = T), HollywoodMovies2011$TheatersOpenWeek ) # 원본 변수 플롯팅 p1=ggplot(HollywoodMovies2011, aes(x=TheatersOpenWeek)) + geom_histogram(colour=&quot;black&quot;, fill=&quot;white&quot;) + ylim(0, 30) # 변환된 변수 플롯팅 p2=ggplot(HollywoodMovies2011, aes(x=TheatersOpenWeek_mean) ) + geom_histogram(colour=&quot;black&quot;, fill=&quot;white&quot;) + ylim(0, 30) # 플롯을 나란히 배치 library(gridExtra) grid.arrange(p1, p2, ncol=2) Figure 2.28: NA를 평균값으로 채우기 변환의 결과로 2828에서 정점을 볼 수 있습니다. 이는 이 지점 주변에 편향을 도입합니다. 만약 어떤 사건을 예측하고 있다면, 이 값 주변에 특별한 사건이 없는 것이 더 안전할 것입니다. 예를 들어, 이진 이벤트를 예측하고 있고 가장 대표성이 떨어지는 이벤트가 TheatersOpenWeek에서 평균인 3000을 갖는 것과 상관관계가 있다면, 더 높은 거짓 양성률(False Positive rate)을 가질 확률이 높아질 수 있습니다. 다시 한번 예측 모델링에서의 고카디널리티 변수 장과의 연결입니다. 마지막 시각화와 관련해서 하나 덧붙이자면, 플롯을 비교 가능하게 만들기 위해 y축 최대값을 30으로 설정하는 것이 중요했습니다. 보시다시피, 모든 개념들 사이에는 상호 관계가 있습니다 😉. 2.5.8.3 채울 적절한 값 선택하기 마지막 예제는 NA를 평균으로 대체했지만, 다른 값들은 어떨까요? 이는 변수의 분포에 달려 있습니다. 우리가 사용한 변수(TheatersOpenWeek)는 정규 분포를 따르는 것처럼 보였으며, 이것이 우리가 평균을 사용한 이유입니다. 그러나 변수가 더 치우쳐(skewed) 있다면, 다른 지표가 아마도 더 적합할 것입니다. 예를 들어, 중앙값(median)은 이상치에 덜 민감합니다. 2.5.9 고급 대치 방법 (Advanced imputation methods) 이제 우리는 예측 모델을 생성하는 좀 더 정교한 대치 방법에 대해 빠르게 검토해 보겠습니다. 2.5.9.1 방법 1: 랜덤 포레스트(missForest) 사용하기 missForest 패키지의 기능은 반복적인 프로세스에서 각 결측값을 완성하기 위해 여러 번의 랜덤 포레스트를 실행하는 것을 기반으로 하며, 범주형 및 수치형 변수를 동시에 처리합니다. 결측값 대치와 무관하게, 랜덤 포레스트 모델은 여러 다양한 종류의 데이터에서 최고의 성능을 내는 모델 중 하나입니다. 다음 예제에서 이전에 작업하던 HollywoodMovies2011 데이터를 완성해 보겠습니다. 이 데이터는 수치형 및 범주형 변수 모두에 NA 값을 포함하고 있습니다. # install.packages(&quot;missForest&quot;) library(missForest) # 데이터 복사 df_holly=Lock5Data::HollywoodMovies2011 # TheatersOpenWeek_cat_cust 다시 생성 df_holly$TheatersOpenWeek_cat_cust= cut(HollywoodMovies2011$TheatersOpenWeek, breaks = c(0, 1000, 2300, 4100), include.lowest = T, dig.lab = 10) # 더 나은 예제를 위해 TheatersOpenWeek_3에 15%의 NA 값을 추가로 도입합니다. # missForest의 prodNA 함수가 도움이 될 것입니다. # 항상 동일한 수의 NA 값을 얻기 위해 시드 설정 set.seed(31415) df_holly$TheatersOpenWeek_cat_cust = df_holly %&gt;% select(TheatersOpenWeek_cat_cust) %&gt;% prodNA(0.15) %&gt;% pull(1) # 유용하지 않은 변수 제외 df_holly = df_holly %&gt;% select(-Movie) # 이제 마법이 펼쳐집니다! 데이터 프레임 대치 # xmis 매개변수 = 결측값이 있는 데이터 imputation_res=missForest(xmis = df_holly) # 최종 대치된 데이터 프레임 df_imputed=imputation_res$ximp 참고: missForest는 문자형(character) 변수가 있으면 실패합니다. 이제 대치된 일부 변수들의 분포를 비교해 볼 시간입니다. 이산화하기 전의 원래 변수인 TheatersOpenWeek를 사용하겠습니다. 시각적 분석에서 서로 비슷해 보이기를 바랍니다. # randomForest 패키지의 na.roughfix를 기반으로 한 또 다른 대치 생성 if(!requireNamespace(&quot;randomForest&quot;, quietly = TRUE)) install.packages(&quot;randomForest&quot;) library(randomForest) df_rough=randomForest::na.roughfix(df_holly) # 대치 전후의 분포 비교 df_holly$imputation=&quot;original&quot; df_rough$imputation=&quot;na.roughfix&quot; df_imputed$imputation=&quot;missForest&quot; # 세 데이터 프레임을 하나로 합치고 imputation 변수로 구분 library(dplyr) df_all=bind_rows(df_holly, df_imputed, df_rough) # 플롯에 사용하기 위해 요인(factor)으로 변환 df_all$imputation=factor(df_all$imputation, levels=unique(df_all$imputation)) # 플롯팅 ggplot(df_all, aes(TheatersOpenWeek, colour=imputation)) + geom_density() + theme_minimal() + scale_colour_brewer(palette=&quot;Set2&quot;) Figure 2.29: 대치 방법 비교 (수치형 변수) 주황색 곡선은 missForest 패키지를 기반으로 대치된 분포를 보여줍니다. 파란색은 처음에 논의했던 대치 방법으로, randomForest 패키지의 na.roughfix 함수를 사용하여 모든 NA를 중앙값으로 대체한 것입니다. 초록색은 대치 전의 분포를 보여줍니다 (당연히 NA 값은 표시되지 않습니다). 분석: 예상대로 NA를 중앙값으로 대체하면 모든 값이 3000 주변에 집중되는 경향이 있습니다. 반면에 missForest 패키지에 의한 대치는 단일 값에 집중되지 않기 때문에 더욱 자연스러운 분포를 제공합니다. 이것이 3000 주변의 피크가 원래 것보다 낮은 이유입니다. 주황색과 초록색은 꽤 비슷해 보입니다! 분석적인 관점에서 보고 싶다면, 예를 들어 평균이나 분산을 비교하기 위해 통계적 테스트를 수행할 수 있습니다. 다음으로 TheatersOpenWeek를 이산화한 사용자 정의 변수(TheatersOpenWeek_cat_cust)를 시각화해 보겠습니다. # NA를 하나의 카테고리로 플롯하기 위한 편법 levels(df_all$TheatersOpenWeek_cat_cust)= c(levels(df_all$TheatersOpenWeek_cat_cust), &quot;NA&quot;) flag_na=is.na(df_all$TheatersOpenWeek_cat_cust) df_all$TheatersOpenWeek_cat_cust[flag_na]=&quot;NA&quot; # 이제 플롯입니다! ggplot(df_all, aes(x = TheatersOpenWeek_cat_cust, fill = TheatersOpenWeek_cat_cust) ) + geom_bar(na.rm=T) + facet_wrap(~imputation) + geom_text(stat=&#39;count&#39;, aes(label=after_stat(count)), vjust=-1) + ylim(0, 125) + scale_fill_brewer(palette=&quot;Set2&quot;) + theme_minimal() + theme(axis.text.x= element_text(angle = 45, hjust = 0.7) ) + theme(legend.position=&quot;bottom&quot;) Figure 2.30: 대치 방법 비교 분석: 원본 변수는 35개의 NA 값을 포함하고 있으며, 이는 na.roughfix에서 최빈값인 (2300, 4100]으로 대체되었습니다. 반면에 missForest를 사용했을 때는 약간 다른 결과를 얻었는데, 완성 기준이 다른 변수들에 기반했기 때문입니다. missForest는 [0, 1000] 카테고리에 15개 행을, [1000, 2300]에 3개를, 그리고 [2300, 4100] 카테고리에 17개를 추가했습니다. 2.5.9.2 방법 2: MICE 접근 방식 사용하기 조언: 결측값 대치에 대한 초기 접근 방식으로는 이 방법이 정말 복잡합니다 😨. MICE는 “Multivariate Imputation by Chained Equations”의 약자로, “Fully Conditional Specification”으로도 알려져 있습니다. 이 책은 그 인기 때문에 이를 다룹니다. MICE는 결측값을 분석하고 처리하기 위한 완전한 프레임워크를 수반합니다. 이는 모든 변수 간의 상호작용을 동시에 고려하며(단일 변수가 아닌 다변량), 서로 다른 예측 모델을 사용하여 각 변수를 채우는 반복적인 프로세스에 기능을 기반으로 합니다. 내부적으로는 B와 C를 기반으로 변수 A를 채웁니다. 그런 다음 A(이전 단계에서 예측됨)와 C를 기반으로 B를 채우고 반복이 계속됩니다. “chained equations(연쇄 방정식)”라는 이름은 케이스를 대치하기 위해 변수별로 알고리즘을 지정할 수 있다는 사실에서 유래되었습니다. 이는 결측값이 없는 원래 데이터의 M개 복제본을 만듭니다. 그런데 왜 M개의 복제본을 만들까요? 각 복제본에서 _빈 슬롯_에 삽입할 값의 결정은 분포를 기반으로 합니다. 많은 MICE 시연은 대치를 검증하고 패키지에서 지원하는 몇 안 되는 예측 모델을 사용하는 데 초점을 맞춥니다. 다른 예측 모델(랜덤 포레스트, 그레디언트 부스팅 머신 등)이나 교차 검증 기술(예: caret)을 사용하고 싶지 않다면 이것으로 충분합니다. MICE 기술은 결측값으로 인한 분산을 측정하기 위한 시설을 제공하는 M개 예측 모델의 매개변수(또는 베타)를 평균화하는 pool() 함수를 설정하여 최종 결과를 도출합니다. 네, 생성된 데이터 프레임당 하나의 모델입니다. 배깅(bagging)처럼 들리죠? 하지만 언급된 모델들에서는 이런 가능성이 없습니다. MICE에는 채우기 결과를 처리하고 검증하는 데 도움이 되는 많은 함수가 있습니다. 하지만 아주 간단하게 유지하기 위해 극히 일부만 다루겠습니다. 다음 예제는 다른 프로그램이나 예측 모델과 함께 사용할 수 있도록 결측값이 없는 데이터 프레임을 추출하는 데 중점을 둘 것입니다. R 예시: 이것은 mice 패키지에 포함된 nhanes 데이터 프레임의 데이터를 대치할 것입니다. 확인해 봅시다: # install.packages(&quot;mice&quot;) library(mice) df_status(nhanes) ## variable q_zeros p_zeros q_na p_na q_inf p_inf type unique ## 1 age 0 0 0 0 0 0 numeric 3 ## 2 bmi 0 0 9 36 0 0 numeric 16 ## 3 hyp 0 0 8 32 0 0 numeric 2 ## 4 chl 0 0 10 40 0 0 numeric 13 세 개의 변수에 결측값이 있습니다. 채워봅시다: # 기본 대치는 5개의 완전한 데이터셋을 생성합니다. imp_data=mice(nhanes, m = 5, printFlag = FALSE) # 5개의 대치된 데이터 프레임을 포함하는 최종 데이터셋을 가져옵니다. 전체 행 수 = nrow(nhanes) * 5 data_all=complete(imp_data, &quot;long&quot;) # data_all은 nhanes와 동일한 열에 .id와 .imp 두 개를 더 포함합니다. # .id = 1부터 25까지의 행 번호 # .imp = 대치 데이터 프레임 .id 1부터 5까지 (m 매개변수) 원본 데이터에서 nhanes는 25행을 가지고 있고 data_all은 125행을 포함하는데, 이는 각각 25행인 5개(m=5)의 완전한 데이터 프레임을 생성한 결과입니다. 결과를 확인할 시간입니다: library(mice) densityplot(imp_data) Figure 2.31: MICE를 사용한 결측값 결과 분석 각 빨간색 선은 대치된 각 데이터 프레임의 분포를 보여주고 파란색 선은 원래의 분포를 포함합니다. 이 이면의 아이디어는 그들이 비슷해 보인다면 대치가 원래의 분포를 잘 따랐다는 것입니다. 예를 들어, chl은 대치된 데이터 프레임 하나를 포함합니다. 따라서 원래 것보다 훨씬 높은 두 값 주변에 두 개의 피크를 특징으로 하는 빨간색 선이 하나만 있습니다. 단점은 프로세스가 느리고 작동하기 위해 약간의 조정이 필요할 수 있다는 것입니다. 예를 들어: mice_hollywood=mice(HollywoodMovies2011, m=5)는 작은 데이터 프레임임에도 불구하고 처리하는 데 시간이 좀 걸린 후 실패할 것입니다. MICE 패키지에 대한 더 많은 정보: 원본 MICE 논문: Multivariate Imputation by Chained Equations in R Handling missing data with MICE package; a simple approach 2.5.10 결론 모든 것을 다룬 후, 우리는 물을 수 있습니다: 무엇이 최선의 전략일까요? 음, 결측값을 처리하기 위해 우리가 얼마나 개입해야 하는지에 달려 있습니다. 전략에 대한 빠른 검토는 다음과 같습니다: 결측값이 있는 행과 열을 제외하기. 결측값이 있는 행(또는 열)이 거의 없고, 남은 데이터가 프로젝트 목표를 달성하기에 충분할 때만 적용 가능합니다. 그러나 결측값이 있는 행을 제외하고 운영 환경에서 실행될 예측 모델을 구축할 때, 결측값이 포함된 새로운 사례가 도착하면 이를 처리하기 위해 값을 할당해야 합니다. 수치형 변수를 범주형으로 변환한 다음 “빈(empty)” 값을 생성하는 전략(범주형 변수에도 적용 가능)은 빈 값을 처리하기 위한 가장 빠르고 권장되는 옵션입니다. 이렇게 하면 모델이 불확실성을 처리할 수 있도록 결측값을 모델에 도입하게 됩니다. MICE와 missForest로 다룬 것과 같은 대치 방법은 상당히 더 복잡합니다. 이러한 방법들을 통해 우리는 행이나 열을 제외할 필요가 없도록 제어된 편향(controlled-bias)을 도입합니다. 이러한 변환들을 깊이 파고드는 것과 단순하게 유지하는 것 사이에서 올바른 균형을 찾는 것은 예술과 같습니다. 투자된 시간이 전체적인 정확도에 반영되지 않을 수도 있습니다. 방법에 상관없이 각 결정의 영향을 분석하는 것이 매우 중요합니다. 여러분의 데이터와 프로젝트에 가장 적합한 방법을 발견하기까지는 탐색적 데이터 분석뿐만 아니라 많은 시행착오가 따릅니다. 2.6 시간과 관련된 고려 사항 2.6.1 무엇에 대한 내용인가요? 모든 것은 변하고 아무것도 멈춰 있지 않는다. - 헤라클레이토스 (기원전 535년 ~ 475년), 소크라테스 이전의 그리스 철학자. 변수도 마찬가지입니다. 시간이 지남에 따라 변수의 값은 변할 수 있으며, 이는 예측 모델을 생성하기 위해 시간 분석을 중요하게 만듭니다. 결과를 원인으로 사용하는 것을 피해야 합니다. 이 장에서 무엇을 검토할까요? 예측할 이벤트 이전의 정보를 필터링하는 개념. 값이 무한대(그 이상)로 증가하거나 감소하는 변수를 분석하고 준비하는 방법. 2.6.1.1 미래의 정보를 사용하지 마세요 영화 “백 투 더 퓨처” (1985)의 장면. 로버트 저메키스 (감독). 예측하려는 이벤트 _이후_의 정보가 포함된 변수를 사용하는 것은 새로운 예측 모델 프로젝트를 시작할 때 흔히 저지르는 실수입니다. 이는 내일의 신문을 가지고 오늘 로또를 하는 것과 같습니다. 예를 들어, 웹 애플리케이션에서 어떤 사용자가 전체 구독(full subscription)을 구매할지 알기 위해 예측 모델을 구축해야 한다고 가정해 봅시다. 그리고 이 소프트웨어에는 feature_A라는 가상의 기능이 있습니다. ## user_id feature_A full_subscription ## 1 1 yes yes ## 2 2 yes yes ## 3 3 yes yes ## 4 4 no no ## 5 5 yes yes ## 6 6 no no ## 7 7 no no ## 8 8 no no ## 9 9 no no ## 10 10 no no 예측 모델을 구축하여 완벽한 정확도를 얻었고, 조사 결과 다음과 같은 사실이 드러났습니다: “전체 구독을 한 사용자 중 100%가 기능 A를 사용한다”. 일부 예측 알고리즘은 변수 중요도를 보고하며, 따라서 feature_A가 최상위에 있을 것입니다. 문제는: feature_A는 사용자가 전체 구독으로 전환한 후에만 사용할 수 있다는 점입니다. 따라서 이를 사용할 수 없습니다. 핵심 메시지는: 완벽한 변수도, 완벽한 모델도 믿지 마세요. 2.6.1.2 데이터로 정정당당하게 경쟁하고, 데이터가 거동을 발전시키게 하세요 자연에서처럼 사물도 특정 행동을 보이기 시작하는 최소 시간과 최대 시간이 있습니다. 이 시간은 0에서 무한대 사이에서 요동칩니다. 실제로는 분석하기에 가장 좋은 기간이 언제인지 연구하는 것이 권장되는데, 즉 이 관찰 기간 전후의 모든 거동을 제외할 수 있습니다. 변수에서 범위를 설정하는 것은 다분히 주관적일 수 있기 때문에 간단한 일은 아닙니다. 시간이 흐름에 따라 수치가 증가하는 수치형 변수가 있다고 가정해 봅시다. 데이터를 필터링하고 예측 모델에 공급하기 위해 관찰 시간 윈도우(observation time window)를 정의해야 할 수도 있습니다. 최소 시간 설정: 거동이 나타나기 시작하는 데 얼마나 많은 시간이 필요한가? 최대 시간 설정: 거동의 끝을 확인하는 데 얼마나 많은 시간이 소요되는가? 가장 쉬운 해결책은 최소 시간을 시작부터로, 최대 시간을 전체 이력으로 설정하는 것입니다. 사례 연구: 두 사람 Ouro와 Borus는 feature_A라는 특정 기능이 있는 웹 애플리케이션의 사용자이며, 우리는 feature_A 사용량(클릭 수 측정)을 기반으로 해당인이 full_subscription을 구매할지 예측하는 모델을 구축해야 합니다. 현재 데이터에 따르면 Borus는 full_subscription을 가지고 있지만 Ouro는 가지고 있지 않습니다. Figure 2.32: 시간 고려 사항에 주의하세요 사용자 Borus는 3일째부터 feature_A를 사용하기 시작했고, 5일 후에는 0일째부터 사용을 시작한 Ouro보다 이 기능을 더 많이 사용(15 대 12 클릭)했습니다. 만약 Borus가 full subscription을 구매하고 Ouro는 구매하지 않는다면, 모델은 무엇을 배울까요? 전체 이력(days_since_signup = all)으로 모델링할 때, Borus가 가장 높은 수치를 가지고 있기 때문에 days_since_signup이 높을수록 가능성이 높아집니다. 하지만 우리가 가입 후 첫 5일간에 해당하는 사용자 이력만 유지한다면 결론은 반대가 됩니다. 왜 첫 5일간의 이력만 유지할까요? 이 시작 단계에서의 행동이 전체 이력을 분석하는 것보다 (예측 정확도 측면에서) 더 관련성이 높을 수 있습니다. 앞서 말했듯이 각 사례에 따라 다릅니다. 2.6.1.3 무한함과의 싸움 이 주제에 대한 예시는 방대합니다. 이 장의 핵심을 데이터가 시간에 따라 어떻게 변하는지에 두겠습니다. 때로는 고정된 시간 후에 변수가 최소값(또는 최대값)에 도달하는 것처럼 간단할 수도 있습니다. 이러한 경우는 쉽게 달성할 수 있습니다. 반면에, 인간은 무한함과 맞서 싸워야 합니다. 다음 예시를 고려해 봅시다. 0 값에 도달하는 데 몇 시간이 필요할까요? 100시간은 어떨까요? Figure 2.33: 100시간 음, 최소값을 확인해 봅시다. ## [1] &quot;100시간 후 최소값: 0.22&quot; 0에 가깝지만, 만약 1000시간을 기다린다면 어떨까요? Figure 2.34: 1,000시간 ## [1] &quot;1,000시간 후 최소값: 0.14&quot; 야호! 0에 접근하고 있습니다! 0.21에서 0.14로 말이죠. 하지만 10배 더 기다린다면(10,000시간) 어떨까요? Figure 2.35: 10,000시간 ## [1] &quot;10,000시간 후 최소값: 0.11&quot; 여전히 0이 아니네요! 시간이 얼마나 더 필요한 거죠?! 😱 아시다시피, 아마 무한대에서 0 값에 도달할 것입니다… 우리는 지금 점근선(Asymptote)을 마주하고 있습니다. 우리는 무엇을 해야 할까요? 다음 섹션으로 이동할 때입니다. 2.6.1.4 무한함과 친구 되기 마지막 예시는 회사의 고객 연령 분석에서도 볼 수 있습니다. 이 값은 무한할 수 있습니다. 예를 들어 프로젝트 목표가 구매/비구매와 같은 이진 결과를 예측하는 것이라면, 유용한 분석 중 하나는 사용자 연령에 따른 구매 비율을 계산하는 것입니다. 다음과 같은 결론에 도달하게 됩니다: 평균적으로 한 고객은 이 제품을 구매하는 데 약 6개월이 필요하다. 이 답은 데이터 과학자와 도메인 전문가의 공동 작업을 통해 얻을 수 있습니다. 이 경우, 인구의 95%가 가지는 값과 동일하게 0을 고려할 수 있습니다. 통계 용어로는 0.95 백분위수(percentile)입니다. 이 책은 부록 1: 백분위수의 마법 장에서 이 주제를 광범위하게 다룹니다. 이는 탐색적 데이터 분석의 핵심 주제입니다. 관련된 사례는 이상치 처리인데, 이상치 처리 장에서 본 것처럼 이 절단 백분위수 기준을 적용할 수 있습니다. 2.6.1.5 다른 분야의 예시 많은 데이터셋이나 프로젝트에서 이런 종류의 변수를 찾는 것은 정말 흔한 일입니다. 의학에서, 생존 분석 프로젝트의 경우 의사들은 보통 환자가 치료에서 _생존_했다고 간주하기 위해 예를 들어 3년이라는 임계값을 정의합니다. 마케팅 프로젝트에서, 만약 사용자가 특정 임계값 미만으로 활동을 줄인다면(예:): * 지난달 동안 회사의 웹 페이지에서 10번의 클릭 * 1주일 후에도 이메일을 열지 않음 * 30일이 지나도 구매하지 않음 이탈한 고객 또는 기회를 놓친 고객으로 정의될 수 있습니다. 고객 지원에서, 사용자가 1주일 동안 불만을 제기하지 않으면 문제가 해결된 것으로 표시될 수 있습니다. 뇌 신호 분석에서, 만약 이 신호들이 시각 피질에서 온 것이고 환자가 어떤 유형의 이미지를 보고 있는지 예측해야 하는 프로젝트라면, 처음 40ms의 값은 무용지물입니다. 뇌가 신호를 처리하기 시작하는 데 필요한 시간이기 때문입니다. 하지만 이것은 _“실생활”_에서도 일어납니다. 모든 연령대에 적합한 데이터 과학 책을 쓸 때, 그것을 끝내는 데 얼마나 많은 시간이 필요할까요? 무한한 시간일까요? 아마도 아닐 겁니다 😄. 2.6.1.6 최종 생각 데이터가 동적인 경우 훈련 및 검증 세트를 생성하기 위해 시간 프레임을 정의하는 것은 거저 얻어지는 것이 아니며(not a free-lunch), 시간이 지남에 따라 변하는 변수를 처리하는 방법을 결정하는 것도 마찬가지입니다. 그렇기 때문에 분석 중인 데이터와 접촉하기 위해 탐색적 데이터 분석이 중요합니다. 주제들은 서로 연결되어 있습니다. 이제 이 장과 모델 성능 평가에서의 시간 외 검증(Out-of-time Validation)과의 관계를 언급할 때입니다. 미래의 사건을 예측할 때, 타겟 변수가 변하는 데 얼마나 많은 시간이 필요한지 분석해야 합니다. 여기서의 핵심 개념은 예측 모델링에서 시간을 다루는 방법입니다. 다음과 같이 질문해 볼 좋은 기회입니다: 이러한 시간 문제를 자동화된 시스템으로 해결하는 것이 어떻게 가능할까요? 경험, 직관 및 일부 계산을 기반으로 임계값을 정의하는 데 있어 이러한 맥락에서는 인간의 지식이 결정적입니다. "],["selecting_best_variables.html", "3 최적 변수 선택 3.1 최적 변수 선택의 일반적 측면 3.2 직관 3.3 “최적”의 선택? 3.4 선택의 특성 3.5 변수 개선하기 3.6 도메인 지식에 의한 정제 3.7 변수는 그룹으로 작용합니다 3.8 입력 변수 간의 상관관계 3.9 단순하게 유지하기 3.10 클러스터링에서의 변수 선택? 3.11 실무에서 최적 변수 선택하기 3.12 타겟 프로파일링", " 3 최적 변수 선택 3.1 최적 변수 선택의 일반적 측면 3.1.1 무엇에 대한 내용인가요? 이 장에서는 다음 주제들을 다룹니다: 예측 모델 또는 클러스터링을 포함한 전통적인 머신러닝 알고리즘의 변수 중요도 순위. 예측 모델이 있는 경우와 없는 경우의 변수 선택 특성. 변수들이 그룹으로 작용할 때의 효과 (직관 및 정보 이론). R을 사용하여 실제로 최적 변수 부분 집합 탐색하기. 최적 변수 선택(Selecting the best variables)은 특성 선택(feature selection), 가장 중요한 예측 변수 선택, 최적 예측 변수 선택 등으로도 알려져 있습니다. Figure 3.1: 위에서와 같이, 아래에서도 이미지: 신경망인가요? 아닙니다. “The Millennium Simulation Project”에서 가져온 암흑 물질(Dark matter)입니다. 3.2 직관 최적의 변수를 선택하는 것은 이야기의 요약본을 만드는 것과 같습니다. 우리는 우리가 말하고자 하는 바를 가장 잘 설명하는 소수의 세부 사항에 집중하기를 원합니다. 불필요한 세부 사항에 대해 너무 많이 이야기하는 것(과적합)과 이야기의 본질에 대해 너무 적게 이야기하는 것(과소적합) 사이에서 균형을 잡아야 합니다. 또 다른 예로 새로운 노트북을 구매하기로 결정하는 상황을 들 수 있습니다: 우리가 가장 중요하게 생각하는 특징은 무엇일까요? 가격, 색상, 배송 방법? 색상과 배터리 수명? 아니면 오직 가격인가요? 머신러닝의 핵심인 정보 이론(Information Theory)의 관점에서 볼 때, 우리가 작업하는 데이터에는 엔트로피(무질서)가 있습니다. 변수를 선택함으로써 우리는 정보를 추가하여 우리 시스템의 엔트로피를 줄이게 됩니다. 3.3 “최적”의 선택? 이 장의 제목에는 “최적(best)”이라는 단어가 들어있지만, 개념적인 부분을 짚고 넘어가는 것이 좋겠습니다. 일반적으로 유일한 최적의 변수 선택이란 존재하지 않습니다. 이러한 관점에서 시작하는 것이 중요합니다. 왜냐하면 예측력에 따라 변수 순위를 매기는 많은 알고리즘을 탐색하다 보면 서로 다르면서도 유사한 결과를 발견할 수 있기 때문입니다. 즉: 알고리즘 1은 최적 변수로 var_1을 선택하고, 그 뒤를 var_5와 var_14가 잇습니다. 알고리즘 2는 다음과 같은 순위를 가집니다: var_1, var_5, var_3. 알고리즘 1에 기반한 정확도가 80%이고, 알고리즘 2에 기반한 정확도가 78%라고 가정해 봅시다. 모든 모델이 내부적인 변동성(variance)을 가지고 있다는 점을 고려하면, 이 결과는 동일한 것으로 간주될 수 있습니다. 이러한 관점은 완벽한 변수 선택을 추구하는 시간을 줄이는 데 도움이 될 수 있습니다. 하지만 극단적으로 보면, 많은 알고리즘에서 높은 순위를 차지하는 변수 집합이 있을 것이고, 예측력이 거의 없는 변수들도 마찬가지일 것입니다. 여러 번 실행한 후에는 가장 신뢰할 수 있는 변수들이 빠르게 드러날 것입니다. 따라서: 결론: 결과가 좋지 않다면 데이터 준비(data preparation) 단계를 개선하고 확인하는 데 집중해야 합니다. 다음 섹션에서 이를 예로 들어 설명하겠습니다. 3.3.1 변수 순위 더 깊이 알아보기 특정 지표에 따른 변수 순위를 매기는 단변량 분석(univariate analysis)은 문헌과 알고리즘에서 이 주제를 다룰 때 매우 흔히 볼 수 있습니다. 우리는 데이터를 교차 검증하기 위해 caret R 패키지를 사용하여 랜덤 포레스트(random forest)와 그레디언트 부스팅 머신(GBM) 두 가지 모델을 만들 것입니다. 그런 다음 각 모델이 반환하는 최적 변수 순위를 비교해 보겠습니다. library(caret) library(funModeling) library(dplyr) # 데이터에서 모든 NA 행 제외. 이 경우 NA는 해결해야 할 주요 이슈가 아니므로 NA(또는 결측값)가 있는 6개 사례를 건너뜁니다. heart_disease=na.omit(heart_disease) # 4-폴드 교차 검증 설정 fitControl = trainControl(method = &quot;cv&quot;, number = 4, classProbs = TRUE, summaryFunction = twoClassSummary) # 랜덤 포레스트 모델 생성 및 최적 튜닝 매개변수 집합 찾기 set.seed(999) fit_rf = train(x = select(heart_disease, -any_of(c(&quot;has_heart_disease&quot;, &quot;heart_disease_severity&quot;))), y = heart_disease$has_heart_disease, method = &quot;rf&quot;, trControl = fitControl, verbose = FALSE, metric = &quot;ROC&quot;) # 그레디언트 부스팅 머신 모델 생성 및 최적 튜닝 매개변수 집합 찾기 fit_gbm = train(x = select(heart_disease, -any_of(c(&quot;has_heart_disease&quot;, &quot;heart_disease_severity&quot;))), y = heart_disease$has_heart_disease, method = &quot;gbm&quot;, trControl = fitControl, verbose = FALSE, metric = &quot;ROC&quot;) 이제 비교를 진행할 수 있습니다. importance_rf와 importance_gbm 열은 각 알고리즘에 의해 측정된 중요도를 나타냅니다. 각 지표를 기반으로 중요도 순서를 나타내는 rank_rf와 rank_gbm이 있으며, 마지막으로 rank_diff(rank_rf - rank_gbm)는 각 알고리즘이 변수 순위를 얼마나 다르게 매기는지를 나타냅니다. # 여기서 앞에서 설명한 테이블을 보기 좋게 조작합니다. var_imp_rf=data.frame(varImp(fit_rf, scale=T)[&quot;importance&quot;]) %&gt;% dplyr::mutate(variable=rownames(.)) %&gt;% dplyr::rename(importance_rf=Overall) %&gt;% dplyr::arrange(desc(importance_rf)) %&gt;% dplyr::mutate(rank_rf=row_number()) var_imp_gbm=as.data.frame(varImp(fit_gbm, scale=T)[&quot;importance&quot;]) %&gt;% dplyr::mutate(variable=rownames(.)) %&gt;% dplyr::rename(importance_gbm=Overall) %&gt;% dplyr::arrange(desc(importance_gbm)) %&gt;% dplyr::mutate(rank_gbm=row_number()) final_res=merge(var_imp_rf, var_imp_gbm, by=&quot;variable&quot;) final_res$rank_diff=final_res$rank_rf-final_res$rank_gbm # 결과 출력! final_res Figure 3.2: 다른 변수 순위 비교 fasting_blood_sugar와 같이 두 모델 모두에 전혀 중요하지 않은 변수들이 있음을 알 수 있습니다. chest_pain이나 thal처럼 중요도 최상위권을 유지하는 다른 변수들도 있습니다. 서로 다른 예측 모델 구현은 해당 특정 모델에 따라 무엇이 최적의 특성인지 보고하는 자체적인 기준을 가지고 있습니다. 이는 결국 알고리즘마다 다른 순위를 매기게 됩니다. 내부 중요도 지표에 대한 자세한 내용은 caret 문서에서 확인할 수 있습니다. 더욱이, GBM 및 랜덤 포레스트와 같은 트리 기반 모델에는 변수를 선택할 때 무작위 성분이 포함되며, 중요도는 트리를 구축할 때 사전적이고 자동적인 변수 선택을 기반으로 합니다. 각 변수의 중요도는 다른 변수들에 의존하며, 단지 고립된 기여도에만 근거하지 않습니다: 변수들은 그룹으로 작용합니다. 이에 대해서는 이 장의 뒷부분에서 다시 다루겠습니다. 알고리즘마다 순위가 다르겠지만, 앞서 언급했듯이 일반적으로 이러한 결과 사이에는 상관관계가 있습니다. 결론: 모든 순위 목록은 _“최종적인 진리”_가 아니며, 정보가 어디에 있는지에 대한 방향성을 제시해 줍니다. 3.4 선택의 특성 변수 선택을 수행할 때 크게 두 가지 접근 방식이 있습니다: 예측 모델 의존적 (Predictive model dependent): 앞서 본 것처럼 가장 일반적인 방식입니다. 모델은 하나의 고유한 정확도 측정 지표에 따라 변수의 순위를 매깁니다. 트리 기반 모델에서는 정보 이득(information gain), 지니 지수(Gini index), 노드 불순도(node impurity)와 같은 지표를 사용합니다. 자세한 정보는 (stackoverflow.com 2017) 및 (stats.stackexchange.com 2017a)에서 확인할 수 있습니다. 예측 모델 비의존적 (Not predictive model dependent): 이 방식은 앞의 방식만큼 대중적이지는 않지만, 유전체 데이터와 관련된 분야에서 매우 우수한 성능을 발휘하는 것으로 입증되었습니다. 이들은 암(타겟 변수)과 같은 특정 질병과 상관관계가 있는 관련된 유전자(입력 변수)를 찾아야 합니다. 이 분야의 데이터는 수천 개에 달하는 엄청난 수의 변수를 가지는 것이 특징이며, 이는 다른 분야의 문제들보다 훨씬 큽니다. 이를 수행하는 한 알고리즘은 최소 중복 최대 관련성 특성 선택(Minimum Redundancy Maximum Relevance Feature Selection)의 약자인 mRMR입니다. R에서는 mRMRe 패키지에 이 알고리즘이 구현되어 있습니다. 또 다른 예측 모델 비의존적 알고리즘은 funModeling 패키지에서 제공하는 var_rank_info 함수입니다. 이 함수는 여러 정보 이론 지표에 따라 변수의 순위를 매깁니다. 이에 대한 사례는 나중에 제시될 것입니다. 3.5 변수 개선하기 변수를 처리함으로써 예측력을 높일 수 있습니다. 이 책에서는 지금까지 다음 내용을 다루었습니다: 범주형 변수의 개선. 다음 장의 구간화를 통한 수치형 변수의 노이즈 감소: 수치형 변수 이산화. R에서 이상치를 처리하는 방법. 결측 데이터: 분석, 처리 및 대치 3.6 도메인 지식에 의한 정제 이것은 알고리즘적인 절차와는 관련이 없으며, 데이터가 생성된 분야와 관련이 있습니다. 설문조사에서 얻은 데이터를 생각해 보십시오. 이 설문조사는 1년의 역사를 가지고 있는데, 처음 3개월 동안은 적절한 프로세스 제어가 이루어지지 않았습니다. 데이터를 입력할 때 사용자가 원하는 것은 무엇이든 입력할 수 있었습니다. 이 기간의 변수들은 아마도 가짜(spurious)일 가능성이 높습니다. 특정 기간 동안 변수가 비어 있거나, null이거나, 극단적인 값을 가질 때 이를 쉽게 인식할 수 있습니다. 그렇다면 다음과 같은 질문을 던져야 합니다: 이 데이터는 신뢰할 수 있는가? 예측 모델은 아이처럼 배운다는 점을 명심하십시오. 모델은 데이터를 판단하지 않고 단지 데이터로부터 배울 뿐입니다. 특정 기간의 데이터가 가짜라면 해당 입력 사례들을 제거해야 할 수도 있습니다. 이 점에 대해 더 자세히 알아보기 위해 수치적, 그래픽적으로 더 깊은 탐색적 데이터 분석을 수행해야 합니다. 3.7 변수는 그룹으로 작용합니다 Figure 3.3: 변수는 그룹으로 작용합니다 _최적_의 변수를 선택할 때 주요 목표는 타겟, 결과 또는 종속 변수와 관련하여 가장 많은 정보를 가진 변수를 얻는 것입니다. 예측 모델은 1개에서 ’N’개의 입력 변수를 기반으로 가중치 또는 매개변수를 찾습니다. 변수들은 대개 이벤트를 설명할 때 고립되어 작용하지 않습니다. 아리스토텔레스의 말을 인용하자면: “전체는 부분의 합보다 크다.” 이는 _최적_의 특성을 선택할 때도 마찬가지입니다: 두 개의 변수로 구축된 예측 모델은 단 하나의 변수만으로 구축된 모델보다 더 높은 정확도에 도달할 수 있습니다. 예를 들어: var_1 변수를 기반으로 모델을 구축하면 60%의 전반적인 정확도를 얻을 수 있습니다. 반면, var_2를 기반으로 모델을 구축하면 72%의 정확도에 도달할 수 있습니다. 하지만 이 두 변수 var_1과 var_2를 결합하면 80% 이상의 정확도를 달성할 수 있습니다. 3.7.1 R 사례: 그룹으로 작용하는 변수 Figure 3.4: 아리스토텔레스 (기원전 384년 – 기원전 322년) 다음 코드는 아리스토텔레스가 수천 년 전에 말한 내용을 보여줍니다. 이 코드는 서로 다른 변수 부분 집합을 기반으로 3개의 모델을 생성합니다: 모델 1은 max_heart_rate 입력 변수를 기반으로 합니다. 모델 2는 chest_pain 입력 변수를 기반으로 합니다. 모델 3은 max_heart_rate 와 chest_pain 입력 변수를 기반으로 합니다. 각 모델은 ROC 지표를 반환하며, 결과에는 각 변수를 개별적으로 고려했을 때와 비교하여 두 변수를 동시에 고려했을 때의 개선 사항이 포함됩니다. library(caret) library(funModeling) library(dplyr) # 4-폴드 교차 검증 설정 fitControl = trainControl(method = &quot;cv&quot;, number = 4, classProbs = TRUE, summaryFunction = twoClassSummary ) create_model&lt;-function(input_variables) { # 입력 변수를 기반으로 # 그레디언트 부스팅 머신 모델 생성 fit_model = train(x = select(heart_disease, any_of(input_variables) ), y = heart_disease$has_heart_disease, method = &quot;gbm&quot;, trControl = fitControl, verbose = FALSE, metric = &quot;ROC&quot;) # 성능 지표로 ROC 반환 max_roc_value=max(fit_model$results$ROC) return(max_roc_value) } roc_1=create_model(&quot;max_heart_rate&quot;) roc_2=create_model(&quot;chest_pain&quot;) roc_3=create_model(c(&quot;max_heart_rate&quot;, &quot;chest_pain&quot;)) avg_improvement=round(100*(((roc_3-roc_1)/roc_1)+ ((roc_3-roc_2)/roc_2))/2, 2) avg_improvement_text=sprintf(&quot;평균 개선율: %s%%&quot;, avg_improvement) results = sprintf(&quot;&#39;max_heart_rate&#39; 기반 ROC 모델: %s.; &#39;chest_pain&#39; 기반: %s; 그리고 둘 다 기반: %s&quot;, round(roc_1,2), round(roc_2,2), round(roc_3, 2) ) # 결과 출력! cat(c(results, avg_improvement_text), sep=&quot;\\n\\n&quot;) ## &#39;max_heart_rate&#39; 기반 ROC 모델: 0.72.; ## &#39;chest_pain&#39; 기반: 0.77; 그리고 둘 다 기반: 0.82 ## ## 평균 개선율: 10.05% 3.7.2 작은 사례 (정보 이론 기반) 4개의 행, 2개의 입력 변수(var_1, var_2), 그리고 하나의 결과(target)를 가진 다음과 같은 빅 데이터 테이블 😜을 보십시오. Figure 3.5: 뭉쳐야 산다: 변수 결합 var_1만으로 예측 모델을 구축한다면 무엇을 보게 될까요? a라는 값은 blue와 red라는 출력과 동일한 비율(50%)로 상관관계가 있습니다: var_1='a'이면 target=’red’일 가능성이 50%입니다 (1행). var_1='b'이면 target=’blue’일 가능성이 50%입니다 (2행). var_2에 대해서도 동일한 분석이 적용됩니다. 동일한 입력이 서로 다른 결과와 관련이 있을 때 이를 노이즈(noise)라고 정의합니다. 이것은 한 사람이 _“어이, 내일 비가 올 거야!”라고 말하고, 다른 사람이 ”내일은 절대 비가 오지 않을 거야”라고 말하는 것과 같은 직관입니다. 우리는 … ”세상에! 우산이 필요한 거야 아니야 😱?“_라고 생각할 것입니다. 다시 예제로 돌아가서, 두 변수를 동시에 고려하면 입력과 출력 사이의 일대일 대응이 명확해집니다: “var_1='a'이고 var_2='x'이면 target='red'일 확률은 100%입니다.” 다른 조합들도 시도해 볼 수 있습니다. 요약: 변수들이 그룹으로 작용하는 사례였습니다. var_1과 var_2를 동시에 고려하면 예측력이 높아집니다. 그럼에도 불구하고, 이는 더 깊이 다루어야 할 주제입니다. 마지막 분석을 고려할 때; 어떤 것을 예측하기 위해 Id 열(모든 값이 고유함)을 사용하는 것은 어떨까요? 입력-출력 사이의 대응 관계는 역시 유일할 것입니다… 하지만 그것이 유용한 모델일까요? 이 책에서는 앞으로 정보 이론에 대해 더 많은 내용을 다룰 것입니다. 3.7.3 결론 heart_disease 데이터를 기반으로 제시된 R 예제는 두 가지 변수를 동시에 고려할 때 평균 9%의 개선을 보여주었으며, 이는 나쁘지 않은 결과입니다. 이 개선 비율은 변수들이 그룹으로 작용한 결과입니다. 이러한 효과는 max_heart_rate 및 chest_pain (또는 var_1 및 var_2)의 경우처럼 변수가 정보를 포함하고 있을 때 나타납니다. 좋은 변수 옆에 노이즈 섞인 변수를 두면 대개 전반적인 성능에 나쁜 영향을 미칩니다. 또한, 입력 변수들 간에 상관관계가 없을 때 그룹으로 작용하는 효과가 더 큽니다. 실제로 이를 최적화하기는 어렵습니다. 이에 대한 자세한 내용은 다음 섹션에서 다룹니다… 3.7.4 정보 이론을 사용한 최적 특성 순위 매기기 이 장의 시작 부분에서 소개했듯이, 정보 이론을 사용하면 예측 모델을 사용하지 않고도 변수 중요도를 얻을 수 있습니다. 버전 1.6.6부터 funModeling 패키지는 데이터와 타겟 변수라는 두 개의 인수를 받는 var_rank_info 함수를 도입했습니다. 다음과 같이 사용합니다: variable_importance = var_rank_info(heart_disease, &quot;has_heart_disease&quot;) ## Warning: `funs()` was deprecated in dplyr 0.8.0. ## ℹ Please use a list of either functions or lambdas: ## ## # Simple named list: list(mean = mean, median = median) ## ## # Auto named with `tibble::lst()`: tibble::lst(mean, median) ## ## # Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE)) ## ℹ The deprecated feature was likely used in the funModeling package. ## Please report the issue at &lt;https://github.com/pablo14/funModeling/issues&gt;. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. # 결과 출력 variable_importance ## var en mi ig gr ## en13 heart_disease_severity 1.846 0.995 0.9950837595 0.5390655068 ## en12 thal 2.032 0.209 0.2094550580 0.1680456709 ## en8 exer_angina 1.767 0.139 0.1391389302 0.1526393841 ## en14 exter_angina 1.767 0.139 0.1391389302 0.1526393841 ## en2 chest_pain 2.527 0.205 0.2050188327 0.1180286190 ## en11 num_vessels_flour 2.381 0.182 0.1815217813 0.1157736478 ## en10 slope 2.177 0.112 0.1124219069 0.0868799615 ## en4 serum_cholestoral 7.481 0.561 0.5605556771 0.0795557228 ## en1 gender 1.842 0.057 0.0572537665 0.0632970555 ## en9 oldpeak 4.874 0.249 0.2491668741 0.0603576874 ## en7 max_heart_rate 6.832 0.334 0.3336174096 0.0540697329 ## en3 resting_blood_pressure 5.567 0.143 0.1425548155 0.0302394591 ## en age 5.928 0.137 0.1371752885 0.0270548944 ## en6 resting_electro 2.059 0.024 0.0241482908 0.0221938072 ## en5 fasting_blood_sugar 1.601 0.000 0.0004593775 0.0007579095 # 시각화 ggplot(variable_importance, aes(x = reorder(var, gr), y = gr, fill = var) ) + geom_bar(stat = &quot;identity&quot;) + coord_flip() + theme_bw() + xlab(&quot;&quot;) + ylab(&quot;변수 중요도 (정보 이득 기준)&quot;) + guides(fill = &quot;none&quot;) Figure 3.6: 변수 중요도 (이득률 기준) heart_disease_severity가 타겟을 가장 잘 설명하는 특성인가요? 아니요, 이 변수는 타겟을 생성하는 데 사용되었으므로 제외해야 합니다. 예측 모델을 개발할 때 타겟과 동일한 방식으로 구축된 입력 변수를 포함하거나 시간 고려 사항에서 설명한 것처럼 미래의 변수를 추가하는 것은 전형적인 실수입니다. var_rank_info의 결과로 돌아가서, 결과 지표는 정보 이론에서 유래한 것입니다: en: 비트 단위로 측정된 엔트로피(entropy) mi: 상호 정보량(mutual information) ig: 정보 이득(information gain) gr: 이득률(gain ratio) 이 지표들의 이면을 지금 당장 깊이 다루지는 않을 것입니다. 이는 미래의 장에서 독점적으로 다루게 될 것이기 때문입니다. 하지만 여기서 가장 중요한 지표는 gr(이득률)로, 0에서 1 사이의 범위를 가지며 높을수록 좋습니다. 모호한 경계 우리는 방금 정보 이론 지표를 기반으로 중요도를 계산하는 방법을 보았습니다. 이 주제는 이 장에만 국한된 것이 아닙니다. 이 개념은 탐색적 데이터 분석 - 상관관계 및 관계 섹션에도 존재합니다. _최적의 특성을 선택하는 것_은 _탐색적 데이터 분석_과 관련이 있으며, 그 반대도 마찬가지입니다. 3.8 입력 변수 간의 상관관계 이상적인 시나리오는 서로 상관관계가 없는 변수들만으로 예측 모델을 구축하는 것입니다. 실제로 모든 변수에 대해 이러한 시나리오를 유지하는 것은 복잡합니다. 일반적으로 서로 상관관계가 없는 변수 집합이 있겠지만, 적어도 어느 정도의 상관관계가 있는 다른 변수들도 있을 것입니다. 실무에서 적절한 해결책은 현저하게 높은 수준의 상관관계를 가진 변수들을 제외하는 것입니다. 상관관계를 측정하는 방법에 관해서, 선형 또는 비선형 절차에 따라 결과가 매우 다를 수 있습니다. 자세한 정보는 상관관계 섹션에서 확인하세요. 상관관계가 있는 변수를 추가할 때의 문제는 무엇일까요? 문제는 모델에 복잡성을 더한다는 점입니다: 일반적으로 더 많은 시간이 소요되고, 이해하기 어렵고, 설명하기 어려우며, 정확도가 떨어지는 등의 문제가 발생합니다. 이는 예측 모델이 고카디널리티를 처리하지 못하나요?에서 검토한 효과입니다. 일반적인 규칙은 다음과 같습니다: 출력과는 상관관계가 높으면서 입력 변수들끼리는 서로 상관관계가 없는 상위 N개의 변수를 추가하려고 노력하십시오. 이는 다음 섹션으로 이어집니다. 3.9 단순하게 유지하기 Figure 3.7: 자연 속의 프랙탈 자연은 가능한 가장 짧은 방식으로 작동한다. - 아리스토텔레스 오컴의 면도날(Occam’s razor) 원칙: 경쟁하는 가설들 중에서 가장 적은 가정으로 이루어진 가설이 선택되어야 합니다. 머신러닝을 위해 이 문장을 재해석하면, 그 “가설”은 변수로 간주될 수 있으므로 다음과 같습니다: 서로 다른 예측 모델들 중에서 가장 적은 수의 변수를 가진 모델이 선택되어야 합니다. (Wikipedia 2017c) 물론 변수를 추가/제거하는 것과 모델의 정확도 사이의 타협점(trade-off)도 존재합니다. 변수 수가 많은 예측 모델은 과적합(overfitting)되는 경향이 있습니다. 반면, 변수 수가 적은 모델은 과소적합(underfitting)으로 이어질 수 있습니다. 많고 _적음_의 개념은 분석 대상 데이터에 따라 매우 주관적입니다. 실무에서 우리는 정확도 지표(예: ROC 값)를 가질 수 있습니다. 즉, 다음과 같은 것을 보게 될 것입니다: Figure 3.8: 서로 다른 변수 부분 집합에 대한 ROC 값 위의 사진은 서로 다른 변수 부분 집합(5, 10, 20, 30, 58)에 따른 ROC 정확도 지표를 보여줍니다. 각 점은 모델을 구축하는 데 사용된 특정 수의 변수에 따른 ROC 값을 나타냅니다. 최고의 ROC는 모델이 30개의 변수로 구축되었을 때 나타남을 확인할 수 있습니다. 만약 자동화된 프로세스에만 기반하여 선택한다면, 데이터에 과적합되는 경향이 있는 부분 집합을 선택하게 될 수도 있습니다. 이 보고서는 R의 caret 라이브러리에 의해 생성되었지만((Kuhn 2017)), 모든 소프트웨어에서 유사하게 작동합니다. 20개 부분 집합과 30개 부분 집합의 차이를 자세히 살펴보십시오. 10개의 변수를 더 선택했음에도 불구하고 개선 효과는 1.8%(0.9324에서 0.95로)에 불과합니다. 즉: 변수를 50% 더 선택해도 개선 효과는 2% 미만입니다. 더욱이, 이 2%는 오차 알기 장에서 본 것처럼 모든 예측 모델이 가지고 있는 예측 변동성을 고려할 때 오차 범위 내에 있을 수 있습니다. 결론: 이 사례에서 오컴의 면도날 원칙에 비추어 볼 때, 가장 좋은 해결책은 20개의 변수 부분 집합으로 모델을 구축하는 것입니다. 20개의 변수를 가진 모델을 다른 사람에게 설명하고 이해하는 것이 30개를 가진 유사한 모델보다 더 쉽습니다. 3.10 클러스터링에서의 변수 선택? Figure 3.9: 클러스터 세분화의 예 이 개념은 대개 타겟 변수를 예측하기 위한 몇 가지 변수가 있는 예측 모델링에서만 나타납니다. 클러스터링에는 타겟 변수가 없으며, 우리는 데이터가 스스로 말하게 하고 어떤 거리 지표에 따라 자연스러운 세그먼트가 발생하도록 합니다. 하지만 모든 변수가 클러스터 모델의 비유사성(dissimilarity)에 동일하게 기여하는 것은 아닙니다. 요약하자면, 결과로 3개의 클러스터가 있고 각 변수의 평균을 측정한다면, 이 평균들이 서로 상당히 다르기를 기대할 것입니다. 그렇죠? 2개의 클러스터 모델을 구축했을 때, 첫 번째 모델에서 age 변수의 평균은 24, 33, 26세인 반면, 두 번째 모델에서는 23, 31, 46세라고 가정해 봅시다. 두 번째 모델에서 age 변수는 더 많은 가변성을 가지고 있으므로 모델에 더 관련성이 높습니다. 이것은 단지 두 모델을 고려한 예시였지만, 하나의 모델만 고려할 때도 마찬가지입니다. 평균들 사이의 거리가 더 먼 변수들이 다른 변수들보다 클러스터를 더 잘 정의하는 경향이 있습니다. 예측 모델링과 달리, 클러스터링에서는 덜 중요한 변수를 제거해서는 안 됩니다. 해당 변수가 그 특정 모델에서는 중요하지 않을 수 있지만, 다른 매개변수로 다른 모델을 구축한다면 중요해질 수도 있기 때문입니다. 클러스터 모델의 품질은 매우 주관적입니다. 마지막으로, 클러스터를 타겟 변수로 하여 랜덤 포레스트 모델을 실행하면 가장 중요한 변수들을 빠르게 수집할 수 있습니다. 3.11 실무에서 최적 변수 선택하기 3.11.1 짧은 답변 사용 중인 알고리즘에서 상위 _N_개의 변수를 선택한 다음, 이 부분 집합으로 모델을 다시 구축하십시오. 모든 예측 모델이 변수 순위를 검색하는 것은 아니지만, 검색한다면 동일한 모델(예: 그레디언트 부스팅 머신)을 사용하여 순위를 얻고 최종 모델을 구축하십시오. 변수 선택 절차가 내장되어 있지 않은 K-최근접 이웃(k-nearest neighbors)과 같은 모델의 경우, 다른 알고리즘의 선택 결과를 사용하는 것이 유효합니다. 이는 모든 변수를 사용하는 것보다 더 좋은 결과로 이어질 것입니다. 3.11.2 긴 답변 가능할 때마다 맥락, 비즈니스 또는 데이터 소스에 대해 잘 아는 사람과 확인하십시오. 상위 _N_개 또는 하위 _M_개 변수에 대해 모두 확인이 필요합니다. 좋지 않은 변수와 관련하여, 데이터 정제 과정에서 예측력을 파괴하는 무언가를 놓쳤을 수도 있습니다. 각 변수의 의미와 맥락(비즈니스, 의학 등)을 이해하십시오. 탐색적 데이터 분석을 수행하여 타겟 변수와 관련하여 가장 중요한 변수들의 분포를 확인하십시오. 선택 결과가 타당한가요? 타겟이 이진형이면 cross_plot을 사용한 타겟 프로파일링 함수를 사용할 수 있습니다. 특정 변수의 평균이 시간이 지남에 따라 상당히 변하나요? 분포의 급격한 변화를 확인하십시오. 고카디널리티를 가진 상위 순위 변수(예: 100개 이상의 카테고리가 있는 우편번호)를 의심해 보십시오. 자세한 정보는 예측 모델링에서의 고카디널리티 변수에서 확인할 수 있습니다. 변수를 선택할 때(에측 모델 구축 시에도 마찬가지임), 재샘플링(부트스트래핑 등) 메커니즘과 교차 검증을 포함하는 방법을 시도하고 사용하십시오. 더 자세한 정보는 오차 알기 장에 있습니다. 앞서 언급한 mRMR과 같은 변수 그룹을 찾는 다른 방법들을 시도해 보십시오. 선택 결과가 요구사항을 충족하지 못하면 새로운 변수를 생성해 보십시오. 데이터 준비 장을 확인할 수 있습니다. (곧 추가될 기능: 특성 공학 장) 3.11.3 자신만의 지식 창출하기 수천 개의 변수와 적은 행이 있는 유전학부터 새로운 데이터가 항상 들어오는 웹 탐색 데이터까지 데이터의 특성이 매우 다를 때 일반화하는 것은 어렵습니다. 분석의 목적도 마찬가지입니다. 정밀도가 매우 필요한 경쟁에서 사용될 것인가요? 아니면 일차적인 목표가 간단한 설명인 임시 연구인가요? 아마도 전자의 경우 후자에 비해 더 많은 상관관계 변수를 포함할 수 있습니다. 가능한 모든 도전 과제에 직면할 수 있는 만능 답변은 없습니다. 경험을 통해 강력한 인사이트를 발견하게 될 것입니다. 그것은 단지 연습의 문제입니다. 3.12 타겟 프로파일링 3.12.1 cross_plot 사용하기 (dataViz) 3.12.1.1 무엇에 대한 내용인가요? 이 플롯은 실전 시나리오에서 변수가 중요한지 여부를 수치형 변수를 빈/그룹으로 그룹화하여 시각적으로 요약해서 보여주는 것을 목적으로 합니다. 3.12.1.2 예제 1: 성별은 심장 질환과 상관관계가 있을까요? cross_plot(heart_disease, input=&quot;gender&quot;, target=&quot;has_heart_disease&quot;) ## Warning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use &quot;none&quot; instead as ## of ggplot2 3.3.4. ## ℹ The deprecated feature was likely used in the funModeling package. ## Please report the issue at &lt;https://github.com/pablo14/funModeling/issues&gt;. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. Figure 3.10: 변수 중요도를 분석하고 보고하기 위한 cross-plot 사용 마지막 두 플롯은 동일한 데이터 소스를 가지며, gender에 따른 has_heart_disease의 분포를 보여줍니다. 왼쪽 플롯은 백분율 값을 보여주고, 오른쪽 플롯은 절대값을 보여줍니다. 3.12.1.2.1 플롯에서 결론을 도출하는 방법 (짧은 버전) 여성/남성 그룹에 따라 심장 질환이 있을 가능성이 다르기 때문에 gender 변수는 좋은 예측 변수인 것으로 보입니다. 즉, 데이터에 질서를 부여합니다. 3.12.1.3 플롯에서 결론을 도출하는 방법 (긴 버전) 첫 번째 플롯 (%)에서: 남성의 심장 질환 발생 가능성은 55.3%인 반면, 여성은 25.8%입니다. 남성의 심장 질환 발생률은 여성 발생률의 두 배입니다 (각각 55.3 vs 25.8). 두 번째 플롯 (수치)에서: 총 97명의 여성이 있습니다: 그중 25명이 심장 질환을 앓고 있습니다 (25/97=25.8%, 이는 첫 번째 플롯의 비율입니다). 나머지 72명은 심장 질환이 없습니다 (74.2%). 총 206명의 남성이 있습니다: 그중 114명이 심장 질환을 앓고 있습니다 (55.3%). 나머지 92명은 심장 질환이 없습니다 (44.7%). 총 사례 수: 네 개의 바 값을 모두 합하면 25+72+114+92=303입니다. 참고: 만약 여성 대 남성의 발생률이 25.8% vs 55.3%가 아니라 30.2% vs 30.6%와 같이 더 유사했다면 어땠을까요? 이 경우 gender 변수는 has_heart_disease 이벤트를 분리하지 못하므로 훨씬 덜 관련성이 있었을 것입니다. 3.12.1.4 예제 2: 수치형 변수와의 교차 분석 수치형 변수는 히스토그램으로 시각화하기 위해 구간화(binning)되어야 합니다. 그렇지 않으면 여기서 볼 수 있듯이 플롯에 정보가 나타나지 않을 수 있습니다. 3.12.1.4.1 등빈도 구간화 (Equal frequency binning) 패키지에는 equal_freq라는 함수가 포함되어 있습니다(Hmisc 패키지에서 상속됨). 이 함수는 등빈도 기준에 따라 구간(bin/bucket)을 반환합니다. 이는 각 구간당 동일한 수의 행을 가지려고 노력합니다. 수치형 변수의 경우, cross_plot은 기본적으로 auto_binning=T로 설정되어 있으며, 이는 자동으로 equal_freq 함수를 n_bins=10(또는 그에 가장 가까운 수)으로 호출합니다. cross_plot(heart_disease, input=&quot;max_heart_rate&quot;, target=&quot;has_heart_disease&quot;) Figure 3.11: 입력으로 수치형 변수 사용 (자동 구간화) 3.12.1.5 예제 3: 수동 구간화 자동 구간화를 원하지 않는다면 cross_plot 함수에서 auto_binning=F로 설정하십시오. 예를 들어, 3개의 버킷을 가진 등빈도 기준의 oldpeak_2를 생성해 보겠습니다. heart_disease$oldpeak_2 = equal_freq(var=heart_disease$oldpeak, n_bins = 3) summary(heart_disease$oldpeak_2) ## [0.0,0.2) [0.2,1.5) [1.5,6.2] ## 106 107 90 구간화된 변수 시각화 (auto_binning = F): cross_oldpeak_2=cross_plot(heart_disease, input=&quot;oldpeak_2&quot;, target=&quot;has_heart_disease&quot;, auto_binning = F) Figure 3.12: 자동 구간화를 비활성화하면 원래 변수가 표시됩니다 3.12.1.5.1 결론 oldpeak_2에 기반한 이 새로운 플롯은 oldpeak_2가 증가함에 따라 심장 질환이 발생할 가능성도 증가하는 것을 명확하게 보여줍니다. 다시 한 번 말하지만, 이는 데이터에 질서를 부여합니다. 3.12.1.6 예제 4: 노이즈 감소 max_heart_rate 변수를 10개 구간 중 하나로 변환합니다: heart_disease$max_heart_rate_2 = equal_freq(var=heart_disease$max_heart_rate, n_bins = 10) cross_plot(heart_disease, input=&quot;max_heart_rate_2&quot;, target=&quot;has_heart_disease&quot; ) Figure 3.13: 커스텀 구간화를 사용한 시각화 언뜻 보기에 max_heart_rate_2는 음의 선형 관계를 보여줍니다. 하지만 관계에 노이즈를 더하는 몇몇 버킷들이 있습니다. 예를 들어, 버킷 (141, 146]은 이전 버킷보다 더 높은 심장 질환 발생률을 가지고 있는데, 원래는 더 낮을 것으로 예상되었습니다. 이것이 데이터의 노이즈일 수 있습니다. 핵심 참고: 약간의 정보를 잃더라도 노이즈를 줄이는 한 가지 방법은 더 적은 수의 구간으로 나누는 것입니다: heart_disease$max_heart_rate_3 = equal_freq(var=heart_disease$max_heart_rate, n_bins = 5) cross_plot(heart_disease, input=&quot;max_heart_rate_3&quot;, target=&quot;has_heart_disease&quot; ) Figure 3.14: 구간을 줄이면 관계를 더 잘 드러내는 데 도움이 될 수 있습니다 결론: 보시다시피 이제 관계가 훨씬 깨끗하고 명확해졌습니다. 버킷 ‘N’이 ‘N+1’보다 더 높은 비율을 가지고 있으며, 이는 음의 상관관계를 의미합니다. cross_plot 결과를 폴더에 저장하려면 어떻게 해야 하나요? 원하는 폴더를 path_out 매개변수에 설정하십시오. 폴더가 없으면 새로 생성합니다. cross_plot(heart_disease, input=&quot;max_heart_rate_3&quot;, target=&quot;has_heart_disease&quot;, path_out=&quot;my_plots&quot;) 작업 디렉토리에 my_plots 폴더가 생성됩니다. 3.12.1.7 예제 5: 여러 변수에 대한 cross_plot 여러 변수에 대해 동시에 cross_plot을 실행하고 싶다고 가정해 봅시다. 이 목표를 달성하려면 변수 이름이 포함된 벡터를 정의하면 됩니다. 다음 세 가지 변수를 분석하고 싶다면: vars_to_analyze=c(&quot;age&quot;, &quot;oldpeak&quot;, &quot;max_heart_rate&quot;) cross_plot(data=heart_disease, target=&quot;has_heart_disease&quot;, input=vars_to_analyze) 3.12.1.8 플롯 내보내기 plotar와 cross_plot은 1개에서 N개의 입력 변수를 처리할 수 있으며, 이들에 의해 생성된 플롯은 path_out 매개변수를 사용하여 고화질로 쉽게 내보낼 수 있습니다. plotar(data=heart_disease, input=c(&#39;max_heart_rate&#39;, &#39;resting_blood_pressure&#39;), target=&quot;has_heart_disease&quot;, plot_type = &quot;boxplot&quot;, path_out = &quot;my_awsome_folder&quot;) 3.12.2 박스 플롯 사용하기 3.12.2.1 무엇에 대한 내용인가요? 중요 변수 분석에서 박스 플롯을 사용하면 이진 타겟 변수의 다양한 값들 사이에서 사분위수가 얼마나 다른지 빠르게 확인할 수 있습니다. # funModeling 로드! library(funModeling) data(heart_disease) plotar(data=heart_disease, input=&quot;age&quot;, target=&quot;has_heart_disease&quot;, plot_type = &quot;boxplot&quot;) ## Warning: `aes_string()` was deprecated in ggplot2 3.0.0. ## ℹ Please use tidy evaluation idioms with `aes()`. ## ℹ See also `vignette(&quot;ggplot2-in-packages&quot;)` for more information. ## ℹ The deprecated feature was likely used in the funModeling package. ## Please report the issue at &lt;https://github.com/pablo14/funModeling/issues&gt;. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. ## Warning: The `fun.y` argument of `stat_summary()` is deprecated as of ggplot2 3.3.0. ## ℹ Please use the `fun` argument instead. ## ℹ The deprecated feature was likely used in the funModeling package. ## Please report the issue at &lt;https://github.com/pablo14/funModeling/issues&gt;. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. Figure 3.15: 박스 플롯을 사용한 타겟 프로파일링 평균선 근처의 마름모꼴은 중앙값을 나타냅니다. Figure 3.16: 박스 플롯 해석 방법 언제 박스 플롯을 사용하나요? 예측할 클래스 전반에서 서로 다른 백분위수를 분석해야 할 때 사용합니다. 이 기술은 이상치로 인한 편향이 평균만큼 큰 영향을 미치지 않기 때문에 강력한 기법입니다. 3.12.2.2 박스 플롯: 좋은 변수 vs 나쁜 변수 하나 이상의 변수를 입력으로 사용하는 것은 박스 플롯을 빠르게 비교하여 최적의 변수를 얻는 데 유용합니다… plotar(data=heart_disease, input=c(&#39;max_heart_rate&#39;, &#39;resting_blood_pressure&#39;), target=&quot;has_heart_disease&quot;, plot_type = &quot;boxplot&quot;) Figure 3.17: 여러 변수에 대한 plotar 함수 Figure 3.18: 여러 변수에 대한 plotar 함수 우리는 max_heart_rate가 resting_blood_pressure보다 더 좋은 예측 변수라고 결론 내릴 수 있습니다. 일반적으로 박스 플롯이 수평으로 정렬되지 않을수록 해당 변수는 더 중요한 것으로 순위가 매겨집니다. 통계 테스트: 백분위수는 예를 들어 그룹 간의 평균이 동일한지 여부를 결정하기 위해 이들에 의해 사용되는 또 다른 기능입니다. 3.12.2.3 플롯 내보내기 plotar와 cross_plot은 1개에서 N개의 입력 변수를 처리할 수 있으며, 이들에 의해 생성된 플롯은 path_out 매개변수를 사용하여 고화질로 쉽게 내보낼 수 있습니다. plotar(data=heart_disease, input=c(&#39;max_heart_rate&#39;, &#39;resting_blood_pressure&#39;), target=&quot;has_heart_disease&quot;, plot_type = &quot;boxplot&quot;, path_out = &quot;my_awsome_folder&quot;) 히스토그램과 박스 플롯을 사용할 때 다음 사항을 염두에 두십시오. 다음 상황에서 이들을 보는 것이 좋습니다: 변수가 잘 퍼져 있을 때 (3, 4..6.. 과 같은 소수의 서로 다른 값에 집중되지 않음), 그리고 극단적인 이상치가 없을 때… (이 지점은 이 패키지에 포함된 prep_outliers 함수로 처리할 수 있습니다) 3.12.3 밀도 히스토그램 사용하기 3.12.3.1 무엇에 대한 내용인가요? 밀도 히스토그램은 분포를 시각화할 때 어떤 책이나 자술 자료에서도 꽤 표준적으로 사용됩니다. 변수 선택에서 이를 사용하면 특정 변수가 클래스를 얼마나 잘 분리하는지 빠르게 확인할 수 있게 해줍니다. plotar(data=heart_disease, input=&quot;age&quot;, target=&quot;has_heart_disease&quot;, plot_type = &quot;histdens&quot;) ## Warning: `summarise_()` was deprecated in dplyr 0.7.0. ## ℹ Please use `summarise()` instead. ## ℹ The deprecated feature was likely used in the funModeling package. ## Please report the issue at &lt;https://github.com/pablo14/funModeling/issues&gt;. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. ## Warning: `group_by_()` was deprecated in dplyr 0.7.0. ## ℹ Please use `group_by()` instead. ## ℹ See vignette(&#39;programming&#39;) for more help ## ℹ The deprecated feature was likely used in the funModeling package. ## Please report the issue at &lt;https://github.com/pablo14/funModeling/issues&gt;. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## ℹ The deprecated feature was likely used in the funModeling package. ## Please report the issue at &lt;https://github.com/pablo14/funModeling/issues&gt;. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. Figure 3.19: 밀도 히스토그램을 사용한 타겟 프로파일링 참고: 점선은 변수의 평균을 나타냅니다. 밀도 히스토그램은 수치형 분포의 일반적인 형태를 시각화하는 데 유용합니다. 이 일반적인 형태는 커널 평활기(Kernel Smoother)라는 기술을 기반으로 계산됩니다. 그 일반적인 아이디어는 인접한 점/바에 존재하는 높은/낮은 피크(노이즈)를 점들을 설명하는 함수를 추정함으로써 줄이는 것입니다. 다음 이미지는 그 개념을 설명합니다: https://en.wikipedia.org/wiki/Kernel_smoother 3.12.3.2 통계 테스트와는 어떤 관계가 있나요? 통계 테스트가 보는 것과 유사합니다. 이들은 빈도론적 접근 방식에서 사용되는 p-value와 같은 일부 통계량에 곡선들이 얼마나 다른지를 반영하여 측정합니다. 이는 분석가에게 곡선들이 예를 들어 동일한 평균을 가지고 있는지 결정할 수 있는 신뢰할 수 있는 정보를 제공합니다. 3.12.3.3 좋은 변수 vs 나쁜 변수 plotar(data=heart_disease, input=c(&#39;resting_blood_pressure&#39;, &#39;max_heart_rate&#39;), target=&quot;has_heart_disease&quot;, plot_type = &quot;histdens&quot;) Figure 3.20: 여러 변수에 대한 plotar 함수 Figure 3.21: 여러 변수에 대한 plotar 함수 그리고 모델도 똑같은 것을 보게 될 것입니다… resting_blood_pressure에서처럼 곡선들이 꽤 많이 겹쳐 있다면, max_heart_rate처럼 더 많이 떨어져 있을 때만큼 좋은 예측 변수가 아닙니다. 히스토그램과 박스 플롯을 사용할 때 다음 사항을 염두에 두십시오. 다음 상황에서 이들을 보는 것이 좋습니다: 변수가 잘 퍼져 있을 때 (3, 4..6.. 과 같은 소수의 서로 다른 값에 집중되지 않음), 그리고 극단적인 이상치가 없을 때… (이 지점은 이 패키지에 포함된 prep_outliers 함수로 처리할 수 있습니다) 참고 문헌 (References) Kuhn, Max. 2017. “Recursive Feature Elimination in r Package Caret.” https://topepo.github.io/caret/recursive-feature-elimination.html. stackoverflow.com. 2017. “What Is Entropy and Information Gain?” http://stackoverflow.com/questions/1859554/what-is-entropy-and-information-gain. ———. 2017a. “How to Interpret Mean Decrease in Accuracy and Mean Decrease GINI in Random Forest Models.” http://stats.stackexchange.com/questions/197827/how-to-interpret-mean-decrease-in-accuracy-and-mean-decrease-gini-in-random-fore. ———. 2017c. “Occam’s Razor.” https://en.wikipedia.org/wiki/Occam's_razor#Probability_theory_and_statistics. "],["model-performance.html", "4 모델 성능 평가 4.1 오차 알기 4.2 시간 외 검증 (Out-of-Time Validation) 4.3 이득 및 리프트 분석 (Gain and Lift Analysis) 4.4 데이터 스코어링 (Scoring Data)", " 4 모델 성능 평가 이 장에서는 예측 모델에서 오차의 방법론적 측면, 교차 검증(cross-validation) 데이터를 통해 오차를 측정하는 방법, 그리고 부트스트래핑(bootstrapping) 기법과의 유사성을 다룹니다. 또한 이러한 전략들이 _랜덤 포레스트(random forest)_나 _그레디언트 부스팅 머신(gradient boosting machines)_과 같은 일부 예측 모델 내부에서 어떻게 사용되는지 살펴봅니다. 또한 시간이 포함된 모델을 검증하는 방법에 대한 내용도 포함되어 있는데, 이는 전통적인 훈련/테스트(train/test) 검증과 유사합니다. 4.1 오차 알기 모델 검증의 방법론적 측면 4.1.1 무엇에 대한 내용인가요? 예측 모델을 구축한 후, 그 품질에 대해 얼마나 확신할 수 있을까요? 모델이 (노이즈를 제외한) 일반적인 패턴(정보)을 잘 포착했을까요? 4.1.1.1 어떤 종류의 데이터인가요? 이 접근 방식은 시간 외 검증(Out-of-Time Validation)에서 다루는 방식과는 다릅니다. 이 방식은 날짜별로 사례를 필터링할 수 없는 경우, 예를 들어 더 이상 새로운 정보가 생성되지 않는 특정 시점의 데이터 스냅샷을 가지고 있는 경우에도 사용할 수 있습니다. 예를 들어 소수의 인원을 대상으로 한 건강 데이터 연구, 설문 조사 또는 연습 목적으로 인터넷에서 사용할 수 있는 일부 데이터 등이 있습니다. 새로운 사례를 추가하는 것이 비용이 많이 들거나, 실용적이지 않거나, 비윤리적이거나, 심지어 불가능할 수도 있습니다. funModeling 패키지에 포함된 heart_disease 데이터가 그러한 예입니다. 4.1.2 예상치 못한 동작 줄이기 모델이 훈련될 때, 그것은 실체의 일부분만을 봅니다. 그것은 전체를 다 볼 수 없는 모집단의 샘플입니다. 모델을 검증하는 방법에는 여러 가지가 있습니다 (정확도 / ROC 곡선 / 리프트 / 이득 등). 이러한 지표들은 모두 분산에 부수적이며, 이는 서로 다른 값을 얻게 됨을 의미합니다. 일부 사례를 제거한 다음 새로운 모델을 맞춤(fit)시키면, 약간 다른 값을 보게 될 것입니다. 정확도 81을 달성한 모델을 구축했다고 가정해 봅시다. 이제 사례의 10%를 제거하고 새로운 모델을 맞춤시키면, 정확도는 78.4가 됩니다. 진짜 정확도는 무엇일까요? 100% 데이터를 사용하여 얻은 것일까요, 아니면 90%를 기반으로 한 다른 것일까요? 예를 들어, 모델이 운영 체제(production environment)에서 실시간으로 실행된다면 다른 사례들을 보게 될 것이고 정확도 지점은 새로운 지점으로 이동할 것입니다. 그렇다면 실제 값은 무엇일까요? 보고해야 할 값은요? 재샘플링(Re-sampling)과 교차 검증(cross-validation) 기술은 가장 신뢰할 수 있는 값에 가까운 근사치를 얻기 위해 서로 다른 샘플링 및 테스트 기준을 기반으로 평균을 낼 것입니다. 그런데 왜 사례를 제거하나요? 그렇게 사례를 제거하는 것은 의미가 없어 보일 수 있지만, 정확도 지표가 얼마나 민감한지에 대한 아이디어를 제공합니다. 우리는 알 수 없는 모집단으로부터 추출된 샘플을 가지고 작업하고 있다는 점을 기억하십시오. 만약 우리가 연구하고 있는 모든 사례의 100%를 포함하는 완전한 결정론적 모델이 있고 모든 사례에서 100% 정확하게 예측했다면, 이 모든 것이 필요하지 않았을 것입니다. 우리는 항상 샘플을 분석하기 때문에 반복, 재샘플링, 교차 검증 등을 통해 데이터의 _실제적이고 알려지지 않은 진실성_에 더 가까워져야 할 뿐입니다… 4.1.3 교차 검증(Cross-Validation, CV)으로 설명해 보겠습니다 Figure 4.1: k-폴드 교차 검증 이미지 출처: Sebastian Raschka 참고 (Raschka 2017) 4.1.3.1 CV 요약 데이터를 똑같은 크기의 무작위 그룹(예: 10개)으로 나눕니다. 이러한 그룹을 흔히 'k'라는 글자로 표현되는 폴드(fold)라고 부릅니다. 9개의 폴드를 선택하여 모델을 구축한 다음, 제외된 나머지 폴드에 모델을 적용합니다. 이를 통해 정확도, ROC, Kappa 등 원하는 성능 지표를 얻게 됩니다. 이 예제에서는 정확도를 사용하고 있습니다. 이 과정을 k번(이 예제에서는 10번) 반복합니다. 그러면 10개의 서로 다른 정확도를 얻게 됩니다. 최종 결과는 이들의 평균이 됩니다. 이 평균값은 모델이 좋은지 아닌지를 평가하는 기준이 되며, 보고서에도 포함될 수 있습니다. 4.1.3.2 실전 사례 iris 데이터 프레임에는 150개의 행이 있습니다. caret 패키지를 사용하여 교차 검증으로 랜덤 포레스트를 구축하면 내부적으로 10개의 랜덤 포레스트가 생성됩니다. 각 모델은 135개의 행(9/10 * 150)을 기반으로 구축되고, 나머지 15개(1/10 * 150)의 사례를 기반으로 정확도를 보고합니다. 이 절차는 10번 반복됩니다. 출력의 이 부분: Figure 4.2: caret 교차 검증 출력 Summary of sample sizes: 135, 135, 135, 135, 135, 135, ...에서 각 135는 훈련 샘플을 나타냅니다. 총 10개가 있지만 출력은 잘렸습니다. 단일 숫자(평균)보다는 분포를 볼 수 있습니다: Figure 4.3: 정확도 분포의 시각적 분석 Figure 4.4: 정확도 분포 최소/최대 정확도는 ~0.8에서 ~1 사이가 될 것입니다. 평균은 caret에 의해 보고된 값입니다. 50%의 확률로 정확도는 ~0.93에서 ~1 사이에 위치할 것입니다. forecast 패키지의 개발자인 Rob Hyndman의 추천 강의: 왜 모든 통계학자가 교차 검증에 대해 알아야 하는가? (Hyndman 2010) 4.1.4 그렇다면 오차란 무엇인가요? 데이터의 편향(Bias), 분산(Variance), 그리고 설명되지 않는 오차(inner noise)(또는 모델이 결코 줄일 수 없는 것)의 합입니다. 이 세 가지 요소가 보고된 오차를 나타냅니다. 4.1.4.1 편향(Bias)과 분산(Variance)의 성질은 무엇인가요? 모델이 제대로 작동하지 않을 때는 여러 가지 원인이 있을 수 있습니다: 모델이 너무 복잡함: 입력 변수가 아주 많을 때이며, 이는 높은 분산과 관련이 있습니다. 모델은 훈련 데이터에 과적합(overfit)되어, 보지 못한 데이터에 대해서는 과도한 세분화로 인해 정확도가 떨어지게 됩니다. 모델이 너무 단순함: 반대로, 모델이 너무 단순해서 데이터의 모든 정보를 포착하지 못할 수도 있습니다. 이는 높은 편향과 관련이 있습니다. 입력 데이터가 충분하지 않음: 데이터는 n차원 공간(n은 모든 입력+타겟 변수)에서 형태를 형성합니다. 점이 충분하지 않으면 이 형태가 제대로 만들어지지 않습니다. “머신러닝에서 더 좋은 것: 더 많은 데이터인가, 더 좋은 알고리즘인가” (Amatriain 2015)에서 더 많은 정보를 확인하세요. Figure 4.5: 편향 vs. 분산 타협(tradeoff) 이미지 출처: Scott Fortmann-Roe (Fortmann 2012). 애니메이션을 통해 편향과 분산을 통한 오차를 직관적으로 이해할 수 있는 방법도 포함되어 있습니다. 4.1.4.2 복잡도 vs 정확도 타협 (Tradeoff) 편향과 분산은 하나가 내려가면 다른 하나가 올라가는 관계이므로 둘 사이에는 타협(tradeoff)이 존재합니다. 이에 대한 실전 사례로는 아카이케 정보 기준(Akaike Information Criterion, AIC) 모델 품질 측정법이 있습니다. AIC는 R의 forecast 패키지에 있는 auto.arima 함수에서 최적의 시계열 모델을 선택하는 휴리스틱으로 사용됩니다 (Hyndman 2017). AIC가 가장 낮은 모델을 선택합니다. 값이 낮을수록 좋습니다. 예측 정확도가 높을수록 값은 낮아지지만, 매개변수의 수가 많아지면 값이 높아집니다. 4.1.4.3 부트스트래핑(Bootstrapping) vs 교차 검증(Cross-Validation) 부트스트래핑은 주로 매개변수를 추정할 때 사용됩니다. 교차 검증은 서로 다른 예측 모델 중에서 선택할 때 주로 사용됩니다. 참고: 편향과 분산에 대해 더 자세히 알고 싶다면 페이지 하단의 (Fortmann 2012) 및 (Amatriain 2015)를 참조하십시오. 4.1.5 실무에 대한 제언이 있나요? 데이터에 따라 다르지만, 10-폴드 CV에 반복을 추가한 10-폴드 CV, 5회 반복과 같은 예시를 흔히 볼 수 있습니다. 때로는 5-폴드 CV, 3회 반복도 사용됩니다. 그리고 원하는 지표의 평균값을 사용합니다. 불균형한 타겟 변수에 대해 덜 편향된 ROC를 사용하는 것도 권장됩니다. 이러한 검증 기법은 시간이 많이 소요되므로, “짧은” 시간 내에 모델 튜닝, 다양한 설정 테스트, 여러 변수 시도 등이 가능하도록 빠르게 실행되는 모델을 선택하는 것을 고려해 보십시오. 랜덤 포레스트(Random Forest)는 빠르고 정확한 결과를 제공하는 훌륭한 옵션입니다. 랜덤 포레스트의 전반적인 성능에 대한 자세한 내용은 (Fernandez-Delgado 2014)에서 확인할 수 있습니다. 또 다른 좋은 옵션은 그레디언트 부스팅 머신(gradient boosting machines)입니다. 랜덤 포레스트보다 튜닝할 매개변수가 더 많지만, 적어도 R에서는 구현이 빠르게 작동합니다. 4.1.5.1 다시 편향과 분산으로 돌아가서 랜덤 포레스트는 편향을 줄이는 데 집중하는 반면… 그레디언트 부스팅 머신은 분산을 최소화하는 데 집중합니다. 더 많은 정보는 “Gradient boosting machine vs random forest” (stats.stackexchange.com 2015)에서 확인하세요. 4.1.6 잊지 마세요: 데이터 준비 데이터를 변환하고 정제하여 입력 데이터를 미세하게 조정하는 것은 모델의 품질에 영향을 미칩니다. 때로는 매개변수를 통해 모델을 최적화하는 것보다 더 큰 영향을 미칩니다. 이 점에 대해서는 데이터 준비(Data Preparation) 장에서 자세히 알아보세요. 4.1.7 마지막 생각 재샘플링 / 교차 검증을 통해 모델을 검증하는 것은 데이터에 존재하는 “실제” 오차를 추정하는 데 도움이 됩니다. 모델이 향후에 실행된다면, 그것이 예상되는 오차가 될 것입니다. 또 다른 장점은 모델 튜닝으로, 특정 모델에 대한 최적의 매개변수를 선택할 때 과적합을 피할 수 있습니다 (caret 예시). Python에서의 해당 내용은 Scikit Learn에 포함되어 있습니다. 가장 좋은 테스트는 여러분의 데이터와 요구 사항에 맞게 여러분이 직접 만든 테스트입니다. 다양한 모델을 시도해 보고 시간 소요와 정확도 지표 사이의 타협점을 분석해 보십시오. 이러한 재샘플링 기술은 stackoverflow.com과 같은 사이트나 협력적인 오픈 소스 소프트웨어 뒤에 있는 강력한 도구 중 하나일 수 있습니다. 편향이 적은 솔루션을 만들기 위해 많은 의견을 수렴하는 것이죠. 하지만 각 의견은 신뢰할 수 있어야 합니다. 여러 의사에게 진단을 요청하는 상황을 상상해 보십시오. 4.1.8 더 읽어보기 튜토리얼: R을 사용한 예측 분석을 위한 교차 검증 Max Kuhn(caret 패키지 제작자)의 튜토리얼: 다양한 종류의 교차 검증 비교하기 교차 검증 접근 방식은 시간 의존적 모델에도 적용될 수 있습니다. 다른 장인 시간 외 검증(Out-of-time Validation)을 확인해 보세요. 4.2 시간 외 검증 (Out-of-Time Validation) 4.2.1 무엇에 대한 내용인가요? 예측 모델을 구축한 후, 그것이 단지 보았던 데이터만을 기억하는 것이 아니라(과적합) 일반적인 패턴을 잘 포착했는지 어떻게 확신할 수 있을까요? 운영 환경에서 실행되거나 실시간으로 작동할 때 모델이 잘 작동할까요? 예상되는 오차는 얼마일까요? 4.2.2 어떤 종류의 데이터인가요? 데이터가 시간에 따라 생성되고, 매일 “웹사이트 페이지 방문”이나 “의료 센터에 도착하는 새로운 환자”와 같은 새로운 사례가 발생한다면, 가장 강력한 검증 방법 중 하나는 시간 외(Out-Of-Time) 접근 방식입니다. 4.2.3 시간 외 검증 예시 방법은? 우리가 1월 1일에 모델을 구축하고 있다고 가정해 봅시다. 모델을 구축하기 위해 10월 31일 이전의 모든 데이터를 사용합니다. 이 두 날짜 사이에는 2개월의 간격이 있습니다. 이진/두 클래스 변수(또는 다중 클래스)를 예측할 때, 이는 매우 명확합니다: 10월 31일 이전의 데이터로 구축한 모델을 사용하여 그 정확한 날짜의 데이터에 점수(score)를 매기고, 그 후 두 달 동안 사용자/환자/개인/사례들이 어떻게 변했는지 측정합니다. 이진 모델의 출력은 각 사례가 특정 클래스에 속할 가능성을 나타내는 숫자여야 하므로(데이터 스코어링 장 참조), 우리는 10월 31일에 모델이 “말한” 것과 “1월 1일”에 실제로 일어난 일을 비교하여 테스트합니다. 다음 검증 워크플로우는 시간이 포함된 예측 모델을 구축할 때 도움이 될 수 있습니다. Figure 4.6: 시간 의존적 문제를 위한 검증 워크플로우 이미지 크게 보기. 4.2.4 이득 및 리프트 분석(Gain and Lift Analysis) 사용하기 이 분석은 다른 장(이득 및 리프트)에서 설명되며, 시간 외 검증에 이어서 사용할 수 있습니다. 10월 31일에 음성(negative)이었던 사례들만 유지하면서, 해당 날짜에 모델이 반환한 점수(score)를 얻고, 타겟(target) 변수는 1월 1일에 해당 사례들이 가졌던 값으로 설정합니다. 4.2.5 수치형 타겟 변수는 어떤가요? 이제 상식과 비즈니스 요구 사항이 더 중요해집니다. 수치형 결과는 어떤 값도 가질 수 있으며, 시간에 따라 증가하거나 감소할 수 있습니다. 따라서 무엇을 성공으로 간주할지 생각하는 데 도움이 되도록 이 두 가지 시나리오를 고려해야 할 수도 있습니다. 이것이 선형 회귀의 경우입니다. 시나리오 예시: 웹 앱 사용량(예: 홈뱅킹)을 측정한다고 했을 때, 일반적인 특징은 날짜가 지남에 따라 사용자가 더 많이 사용한다는 것입니다. 예시: 특정 성분의 혈중 농도 예측. 페이지 방문 수 예측. 시계열 분석. 이러한 경우에도 “예상했던 것” vs. “실제인 것” 사이의 차이가 발생합니다. 이 차이는 어떤 숫자든 될 수 있습니다. 이것이 오차(error) 또는 잔차(residuals)입니다. Figure 4.7: 예측 및 오차 분석 모델이 좋다면, 이 오차는 백색 잡음(white noise)이어야 합니다. (Wikipedia 2017d) 내부의 “시계열 분석 및 회귀” 섹션에서 더 자세한 정보를 확인하세요. 주로 다음과 같은 논리적 특성이 있을 때 정규 곡선을 따릅니다: 오차는 0 주변에 있어야 합니다. 모델의 오차는 0으로 수렴해야 합니다. 이 오차의 표준 편차는 유한해야 합니다. 예측 불가능한 이상치를 피하기 위함입니다. 오차들 사이에는 상관관계가 없어야 합니다. 정규 분포: 대부분의 오차가 0 주변에 있고, 오차가 커질수록 더 작은 비율로 나타날 것을 기대합니다. 즉, 더 큰 오차를 발견할 가능성은 기하급수적으로 감소합니다. Figure 4.8: 좋은 오차 곡선 (정규 분포) 4.2.6 마지막 생각 시간 외 검증(Out-of-Time Validation)은 샘플링에 의존할 필요가 없는 데이터를 사용하여 운영 환경에서의 모델 실행을 시뮬레이션할 수 있는 강력한 검증 도구입니다. 오차 분석은 데이터 과학에서 큰 단원입니다. 이제 이와 관련된 핵심 개념을 다루는 다음 장으로 넘어갈 시간입니다: 오차 알기. 4.3 이득 및 리프트 분석 (Gain and Lift Analysis) 4.3.1 무엇에 대한 내용인가요? 두 지표 모두 예측 모델(이진 결과)의 품질을 검증하는 데 매우 유용합니다. 데이터 스코어링에 대한 더 많은 정보를 확인하세요. 최신 버전의 funModeling(&gt;= 1.3)이 설치되어 있는지 확인하십시오. # funModeling 로드 library(funModeling) # GLM 모델 생성 fit_glm = glm(has_heart_disease ~ age + oldpeak, data = heart_disease, family = binomial) # 각 행의 스코어/확률값 가져오기 heart_disease$score = predict(fit_glm, newdata = heart_disease, type = &#39;response&#39;) # 이득 및 리프트 곡선 그리기 gain_lift(data = heart_disease, score = &#39;score&#39;, target = &#39;has_heart_disease&#39;) ## Warning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use &quot;none&quot; instead as ## of ggplot2 3.3.4. ## ℹ The deprecated feature was likely used in the funModeling package. ## Please report the issue at &lt;https://github.com/pablo14/funModeling/issues&gt;. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. ## Warning: The `guide` argument in `scale_*()` cannot be `FALSE`. This was deprecated in ## ggplot2 3.3.4. ## ℹ Please use &quot;none&quot; instead. ## ℹ The deprecated feature was likely used in the funModeling package. ## Please report the issue at &lt;https://github.com/pablo14/funModeling/issues&gt;. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. Figure 4.9: 이득 및 리프트 곡선 ## Population Gain Lift Score.Point ## 1 10 20.86 2.09 0.8185793 ## 2 20 35.97 1.80 0.6967124 ## 3 30 48.92 1.63 0.5657817 ## 4 40 61.15 1.53 0.4901940 ## 5 50 69.06 1.38 0.4033640 ## 6 60 78.42 1.31 0.3344170 ## 7 70 87.77 1.25 0.2939878 ## 8 80 92.09 1.15 0.2473671 ## 9 90 96.40 1.07 0.1980453 ## 10 100 100.00 1.00 0.1195511 4.3.2 어떻게 해석하나요? 먼저, 각 사례는 점수 값인 덜 대표적인 클래스(less representative class)일 가능성에 따라 정렬됩니다. 그 다음 Gain 열은 각 10%의 행(Population 열)에 대해 양성 클래스를 누적합니다. 따라서 첫 번째 행은 다음과 같이 읽을 수 있습니다: “점수(score)순으로 정렬된 상위 10%의 인구는 전체 양성 사례의 20.86%를 포함합니다.” 예를 들어, 우리가 이 모델을 기반으로 이메일을 보내고 사용자의 20%에게만 도달할 예산이 있다면, 몇 명의 응답을 기대할 수 있을까요? 정답: 35.97% 4.3.3 모델을 사용하지 않는다면 어떨까요? 만약 우리가 모델을 사용하지 않고 무작위로 20%를 선택한다면, 우리는 몇 명의 사용자에게 도달해야 할까요? 당연히 20%입니다. 이것이 0%에서 시작하여 100%에서 끝나는 점선(dashed line)의 의미입니다. 다행히 예측 모델을 사용하면 무작위성보다 높은 성과를 낼 수 있습니다. 리프트(Lift) 열은 이득(Gain)과 우연에 의한 이득 사이의 비율을 나타냅니다. Population=20%를 예로 들면, 모델은 무작위보다 1.8배 더 좋습니다. 4.3.3.1 절단점(Cut point) 사용하기 ✂️ 인구의 30%에 도달하는 점수 값은 무엇일까요? 정답: 0.56 절단점은 우리가 데이터를 세분화할 수 있게 해줍니다. 4.3.3.2 모델 비교하기 좋은 모델에서는 이득이 모집단의 “초기”에 100%에 도달하며, 이는 클래스를 잘 분리하고 있음을 나타냅니다. 모델을 비교할 때 빠른 지표는 모집단의 초기(10-30%) 이득이 더 높은지 확인하는 것입니다. 결과적으로, 초기에 더 높은 이득을 가진 모델이 데이터에서 더 많은 정보를 포착한 것이 됩니다. 설명을 위해… Figure 4.10: 두 모델의 이득 및 리프트 곡선 비교 이미지 크게 보기. 누적 이득 분석(Cumulative Gain Analysis): 모델 1은 인구의 약 10% 지점에서 양성 사례의 ~20%에 도달하는 반면, 모델 2는 인구의 20%에 가까워져야 유사한 비율에 도달합니다. 모델 1이 더 좋습니다. 리프트 분석(Lift analysis): 위와 동일하지만, 모든 리프트 숫자가 감소 패턴을 따르지 않는다는 점이 의심스럽습니다. 아마도 모델이 인구의 첫 번째 백분위수들을 제대로 정렬하지 못하고 있는 것일 수 있습니다. cross_plot을 사용한 타겟 프로파일링 장에서 보았던 것과 동일한 정렬 개념입니다. 4.4 데이터 스코어링 (Scoring Data) 4.4.1 숨겨진 직관 이벤트는 일어날 수도 있고, 일어나지 않을 수도 있습니다. 비록 우리에게 내일의 신문 📰은 없지만, 내일이 어떨지에 대해 좋은 추측을 할 수는 있습니다. 미래는 의심할 여지 없이 불확실성과 맞닿아 있으며, 이 불확실성은 추정될 수 있습니다. 4.4.1.1 그리고 다양한 타겟들이 있습니다… 현재 이 책은 고전적인 예/아니오 타겟(이진 또는 다중 클래스 예측이라고도 함)을 다룹니다. 따라서 이 추정치는 이벤트가 발생할 진실의 값(value of truth)이며, 0과 1 사이의 확률값입니다. 4.4.1.2 이진(Two-label) vs. 다중 클래스(multi-label) 결과 이 장은 이진 결과(두 개의 라벨 결과)를 위해 작성되었지만, 다중 클래스(multi-label) 타겟은 이진 클래스의 일반적인 접근 방식으로 볼 수 있습니다. 예를 들어 4개의 서로 다른 값을 가진 타겟이 있을 때, 특정 클래스에 속할 가능성을 예측하는 4개의 모델이 있을 수 있습니다. 그런 다음 이 4개 모델의 결과를 가져와 최종 클래스를 예측하는 더 상위의 모델이 있을 수 있습니다. 4.4.1.3 뭐라고요? 😯 몇 가지 예시: - 이 고객이 이 제품을 구매할 것인가? - 이 환자가 호전될 것인가? - 향후 몇 주 동안 특정 이벤트가 발생할 것인가? 이 질문들에 대한 대답은 참(True) 또는 거짓(False)이지만, 본질은 스코어(score), 즉 특정 이벤트가 발생할 가능성을 나타내는 숫자를 갖는 것입니다. 4.4.1.4 하지만 우리는 더 많은 통제가 필요합니다… 많은 머신러닝 리소스들은 시작하기에 좋은 단순화된 버전, 즉 최종 클래스를 출력으로 얻는 방식을 보여줍니다. 예를 들어: 단순화된 접근 방식: 질문: 이 사람이 심장 질환을 앓게 될까요? 답변: “아니오” 하지만 “예/아니오” 답변 이전에 다른 무언가가 있으며, 그것이 바로 스코어입니다: 질문: 이 사람이 심장 질환을 앓을 가능성은 얼마나 되나요? 답변: “25%” 따라서 우리는 먼저 스코어를 얻고, 우리의 필요에 따라 절단점(cut point)을 설정합니다. 그리고 이것은 정말로 중요합니다. 4.4.2 예제를 살펴봅시다 Figure 4.11: 간단한 데이터셋 예시 다음 내용을 보여주는 예제 테이블: id = 식별자 x1, x2, x3 = 입력 변수 target = 예측할 변수 Figure 4.12: 스코어 얻기 (예측 모델 출력) 입력 변수는 잊어버리십시오… 랜덤 포레스트와 같은 예측 모델을 생성한 후, 우리가 관심을 갖는 것은 스코어(scores)입니다. 비록 우리의 최종 목표가 예/아니오로 예측된 변수를 전달하는 것이라 할지라도 말입니다. 예를 들어, 다음 두 문장은 같은 것을 표현합니다: 예일 가능성이 0.8이다 &lt;=&gt; 아니오일 확률이 0.2이다 이미 이해하셨겠지만 스코어는 대개 덜 대표적인 클래스인 예를 나타냅니다. ✋ R 구문 -코드를 보고 싶지 않다면 건너뛰셔도 됩니다- 다음 구문은 스코어를 반환합니다: score = predict(randomForestModel, data, type = \"prob\")[, 2] 다른 모델의 경우 이 구문이 약간 다를 수 있지만, 개념은 동일하게 유지된다는 점에 유의하십시오. 다른 언어에서도 마찬가지입니다. 여기서 prob는 우리가 확률(또는 스코어)을 원한다는 것을 나타냅니다. predict 함수에 type=\"prob\" 매개변수를 더하면 15행 2열의 행렬을 반환합니다: 1열은 아니오일 가능성을 나타내고, 2열은 예 클래스에 대한 가능성을 보여줍니다. 타겟 변수가 아니오 또는 예일 수 있으므로, [, 2]는 (아니오 가능성의 보수인) 예일 가능성을 반환합니다. 4.4.3 모든 것은 절단점(cut point)에 달려 있습니다 📏 Figure 4.13: 가장 높은 스코어순으로 정렬된 사례들 이제 테이블이 스코어 내림차순으로 정렬되었습니다. 이는 기본적으로 0.5인 절단점을 가졌을 때 최종 클래스를 어떻게 추출하는지 보여주기 위함입니다. 절단점을 미세하게 조정하면 더 나은 분류가 가능합니다. 정확도 지표나 혼동 행렬은 항상 특정 절단점 값과 연결되어 있습니다. 절단점을 할당한 후, 다음과 같은 유명한 분류 결과를 볼 수 있습니다: ✅ 진양성 (True Positive, TP): 분류가 양성인 것이 참인 경우, 즉 “모델이 양성(예) 클래스를 정확하게 맞춤”을 의미합니다. ✅ 진음성 (True Negative, TN): 위와 동일하지만 음성 클래스(아니오)의 경우입니다. ❌ 가양성 (False Positive, FP): 분류가 양성인 것이 거짓인 경우, 즉 “모델이 틀렸고 예라고 예측했지만 결과는 아니오임”을 의미합니다. ❌ 가음성 (False Negative, FN): 위와 동일하지만 음성 클래스의 경우로, “모델이 음성이라고 예측했지만 결과는 양성이었음”, 즉 “모델이 아니오라고 예측했지만 클래스는 예였음”을 의미합니다. Figure 4.14: 예측 라벨 할당 (cutoff=0.5) 4.4.4 최선과 최악의 시나리오 선(Zen)이 가르치듯 극단을 분석하면 중간 지점을 찾는 데 도움이 됩니다. 👍 최선의 시나리오는 TP와 TN 비율이 100%일 때입니다. 이는 모델이 모든 예와 모든 아니오를 정확하게 예측했음을 의미합니다. (결과적으로 FP와 FN 비율은 0%가 됩니다). 하지만 잠깐만요 ✋! 완벽한 분류를 발견했다면, 그것은 아마도 과적합 때문일 것입니다! 👎 최악의 시나리오 —이전 예시와 정반대— 는 FP와 FN 비율이 100%일 때입니다. 무작위성조차도 이토록 끔찍한 시나리오를 만들어낼 수는 없습니다. 왜 그럴까요? 클래스가 50/50으로 균형 잡혀 있다면 동전을 던져서 결과의 약 절반은 맞힐 것입니다. 이것이 모델이 무작위성보다 우수한지 테스트하는 일반적인 기준선입니다. 제공된 예제에서 클래스 분포는 예가 5개, 아니오가 10개이므로 예는 33.3%(5/15)입니다. 4.4.5 분류기 비교 4.4.5.1 분류 결과 비교 ❓ 퀴즈: 이 33.3%를 정확하게 예측하는 모델(TP 비율=100%)은 좋은 모델인가요? 정답: 그것은 모델이 얼마나 많은 예를 예측했는지에 달려 있습니다. 항상 예만 예측하는 분류기는 TP가 100%이지만, 실제로는 아니오인 많은 사례를 예로 분류하므로 전혀 쓸모가 없습니다. 실제로 이런 경우 FP 비율이 매우 높을 것입니다. 4.4.5.2 스코어에 기반한 라벨 정렬 비교 분류기는 신뢰할 수 있어야 하며, 이것이 바로 ROC 곡선이 TP 대 FP 비율을 도표로 나타낼 때 측정하는 것입니다. FP 대비 TP 비율이 높을수록 ROC 곡선 아래 면적(AUC)이 커집니다. ROC 곡선 뒤에 숨겨진 직관은 스코어와 관련하여 무결성 측정(sanity measure)을 하는 것입니다. 즉, 스코어가 라벨을 얼마나 잘 정렬하느냐 하는 것입니다. 이상적으로는 모든 양성 라벨이 상단에 있고 음성 라벨이 하단에 있어야 합니다. Figure 4.15: 두 예측 모델 스코어 비교 모델 1은 모델 2보다 더 높은 AUC를 가질 것입니다. 위키피디아에 이에 대한 자세하고 유용한 기사가 있습니다: https://en.wikipedia.org/wiki/Receiver_operating_characteristic 다음은 절단점이 0.5일 때 4개 모델을 비교한 것입니다: Figure 4.16: 4개 예측 모델 비교 4.4.6 R로 직접 해보기! 서로 다른 절단점에 따른 세 가지 시나리오를 분석해 보겠습니다. # install.packages(&quot;rpivotTable&quot;) # rpivotTable: 피벗 테이블을 동적으로 생성하며 플롯도 지원합니다. 더 많은 정보: https://github.com/smartinsightsfromdata/rpivotTable library(rpivotTable) ## 데이터 읽기 data=read.delim(file=&quot;https://goo.gl/ac5AkG&quot;, sep=&quot;\\t&quot;, header = T, stringsAsFactors=F) 4.4.6.1 시나리오 1: 절단점 @ 0.5 실제 값 대비 예측 값의 교차점에 몇 개의 사례가 해당하는지 보여주는 고전적인 혼동 행렬: data$predicted_target=ifelse(data$score&gt;=0.5, &quot;yes&quot;, &quot;no&quot;) rpivotTable(data = data, rows = &quot;predicted_target&quot;, cols=&quot;target&quot;, aggregatorName = &quot;Count&quot;, rendererName = &quot;Table&quot;, width=&quot;100%&quot;, height=&quot;400px&quot;) Figure 4.17: 혼동 행렬 (지표: 개수) 다른 뷰입니다. 이번에는 각 열의 합이 100%입니다. 다음 질문에 답하기 좋습니다: rpivotTable(data = data, rows = &quot;predicted_target&quot;, cols=&quot;target&quot;, aggregatorName = &quot;Count as Fraction of Columns&quot;, rendererName = &quot;Table&quot;, width=&quot;100%&quot;, height=&quot;400px&quot;) Figure 4.18: 혼동 행렬 (절단점 0.5) 모델에 의해 포착된 실제 예 값의 비율은 얼마인가요? 정답: 80% 이를 정밀도(Precision, PPV)라고도 합니다. 모델에 의해 던져진 예 중 실제 확률은? 40%. 따라서 마지막 두 문장으로부터: 모델은 10개의 예측 중 4개를 예라고 던지며, 이 세그먼트(예) 중에서 80%를 맞춥니다. 또 다른 뷰: 모델은 10개의 예 예측에 대해 3건의 사례를 정확하게 맞춥니다 (0.4/0.8=3.2, 내림하여 3). 참고: 마지막 분석 방식은 연관 규칙(장바구니 분석) 및 의사결정 트리 모델을 구축할 때 찾아볼 수 있습니다. 4.4.6.2 시나리오 2: 절단점 @ 0.4 절단점을 0.4로 바꿀 때이므로, 예의 양이 더 많아질 것입니다: data$predicted_target=ifelse(data$score&gt;=0.4, &quot;yes&quot;, &quot;no&quot;) rpivotTable(data = data, rows = &quot;predicted_target&quot;, cols=&quot;target&quot;, aggregatorName = &quot;Count as Fraction of Columns&quot;, rendererName = &quot;Table&quot;, width=&quot;100%&quot;, height=&quot;400px&quot;) Figure 4.19: 혼동 행렬 (절단점 0.4) 이제 모델은 예(TP)를 100% 포착하므로, 모델에 의해 생성된 총 예의 양은 46.7%로 증가했지만, TN과 FP는 동일하게 유지 되었으므로 아무런 비용이 들지 않았습니다 :thumbsup:. 4.4.6.3 시나리오 3: 절단점 @ 0.8 FP 비율을 줄이고 싶으신가요? 절단점을 더 높은 값으로 설정하십시오. 예를 들어 0.8로 설정하면 모델에 의해 생성된 예가 감소합니다: data$predicted_target=ifelse(data$score&gt;=0.8, &quot;yes&quot;, &quot;no&quot;) rpivotTable(data = data, rows = &quot;predicted_target&quot;, cols=&quot;target&quot;, aggregatorName = &quot;Count as Fraction of Columns&quot;, rendererName = &quot;Table&quot;, width=&quot;100%&quot;, height=&quot;400px&quot;) Figure 4.20: 혼동 행렬 (절단점 0.8) 이제 FP 비율이 20%에서 10%로 감소했으며, 모델은 여전히 절단점 0.5에서 얻은 것과 동일한 비율인 80%의 TP를 포착합니다 :thumbsup:. 절단점을 0.8로 높임으로써 비용 없이 모델을 개선했습니다. 4.4.7 결론 이 장은 이진 변수를 예측하는 본질에 초점을 맞추었습니다: 타겟 변수를 정렬하는 스코어 또는 가능성 숫자를 생성하는 것입니다. 예측 모델은 입력을 출력으로 매핑합니다. 유일하고 최선인 절단점 값은 존재하지 않습니다. 그것은 프로젝트의 요구 사항에 따라 달라지며, 우리가 수용할 수 있는 가양성(False Positive)과 가음성(False Negative) 비율에 의해 제한됩니다. 이 책은 오차 알기 장에서 모델 성능에 대한 전반적인 측면을 다룹니다. 참고 문헌 (References) Amatriain, Xavier. 2015. “In Machine Learning, What Is Better: More Data or Better Algorithms.” http://www.kdnuggets.com/2015/06/machine-learning-more-data-better-algorithms.html. Fernandez-Delgado, Manuel. 2014. “Do We Need Hundreds of Classifiers to Solve Real World Classification Problems?” http://jmlr.csail.mit.edu/papers/volume15/delgado14a/delgado14a.pdf. Fortmann, Scott. 2012. “Understanding the Bias-Variance Tradeoff.” http://scott.fortmann-roe.com/docs/BiasVariance.html. Hyndman, Rob J. 2010. “Why Every Statistician Should Know about Cross-Validation?” https://robjhyndman.com/hyndsight/crossvalidation/. ———. 2017. “ARIMA Modelling in r.” https://www.otexts.org/fpp/8/. Raschka, Sebastian. 2017. “Machine Learning FAQ.” http://sebastianraschka.com/faq/docs/evaluate-a-model.html. stats.stackexchange.com. 2015. “Gradient Boosting Machine Vs Random Forest.” https://stats.stackexchange.com/questions/173390/gradient-boosting-tree-vs-random-forest. ———. 2017d. “White Noise - Time Series Analysis and Regression.” https://en.wikipedia.org/wiki/White_noise. "],["appendix.html", "5 부록 (APPENDIX) 5.1 백분위수의 마법 5.2 funModeling 퀵스타트", " 5 부록 (APPENDIX) 보충 자료입니다. 5.1 백분위수의 마법 백분위수는 데이터 분석에서 매우 중요한 개념이므로 이 책에서 광범위하게 다룰 것입니다. 백분위수는 다른 관측치와 비교하여 각 관측치를 고려합니다. 고립된 숫자는 의미가 없을 수 있지만, 다른 숫자와 비교할 때 분포의 개념이 나타납니다. 백분위수는 프로파일링뿐만 아니라 예측 모델의 성능을 평가하는 데에도 사용됩니다. Figure 5.1: 백분위수 계산 방법 데이터셋, 계속하기 전의 제언: 이 데이터셋은 세계 개발과 관련된 많은 지표를 포함하고 있습니다. 프로파일링 예제와 상관없이, 사회학자, 연구자 등 이러한 종류의 데이터 분석에 관심이 있는 사람들에게 바로 사용할 수 있는 표를 제공하는 것이 목적입니다. 원본 데이터 소스는 http://databank.worldbank.org입니다. 거기에서 모든 변수를 설명하는 데이터 사전을 찾을 수 있습니다. 이 섹션에서는 이미 분석을 위해 준비된 표를 사용할 것입니다. 단계별 데이터 준비 과정 전체는 프로파일링 장에 있습니다. 지표의 의미는 data.worldbank.org에서 확인할 수 있습니다. 예를 들어, EN.POP.SLUM.UR.ZS가 무엇을 의미하는지 알고 싶다면 http://data.worldbank.org/indicator/EN.POP.SLUM.UR.ZS를 입력하면 됩니다. 5.1.1 백분위수를 계산하는 방법 백분위수를 얻는 방법은 여러 가지가 있습니다. 보간법(interpolation)을 기반으로 할 때 가장 쉬운 방법은 변수를 오름차순으로 정렬하고, 원하는 백분위수(예: 75%)를 선택한 다음, 정렬된 모집단의 75%를 선택하고자 할 때 최대값이 무엇인지 관찰하는 것입니다. 이제 계산 과정 배후에서 무슨 일이 일어나고 있는지 최대한 제어할 수 있도록 작은 샘플을 유지하는 기술을 사용할 것입니다. 무작위로 10개 국가를 추출하고, 사용할 변수인 rural_poverty_headcount 벡터를 출력합니다. library(dplyr) data_world_wide = read.delim(file=&quot;https://goo.gl/NNYhCW&quot;, header = T ) data_sample = data_world_wide %&gt;% filter(Country.Name %in% c(&quot;Kazakhstan&quot;, &quot;Zambia&quot;, &quot;Mauritania&quot;, &quot;Malaysia&quot;, &quot;Sao Tome and Principe&quot;, &quot;Colombia&quot;, &quot;Haiti&quot;, &quot;Fiji&quot;, &quot;Sierra Leone&quot;, &quot;Morocco&quot;)) %&gt;% arrange(rural_poverty_headcount) data_sample %&gt;% select(Country.Name, rural_poverty_headcount) ## Country.Name rural_poverty_headcount ## 1 Malaysia 1.6 ## 2 Kazakhstan 4.4 ## 3 Morocco 14.4 ## 4 Colombia 40.3 ## 5 Fiji 44.0 ## 6 Mauritania 59.4 ## 7 Sao Tome and Principe 59.4 ## 8 Sierra Leone 66.1 ## 9 Haiti 74.9 ## 10 Zambia 77.9 벡터는 학습 목적으로만 정렬되었음을 유의하십시오. 프로파일링 장에서 언급했듯이, 우리의 눈은 순서를 좋아합니다. 이제 rural_poverty_headcount 변수(국가 빈곤선 이하로 생활하는 농촌 인구의 비율)에 quantile 함수를 적용합니다: quantile(data_sample$rural_poverty_headcount) ## 0% 25% 50% 75% 100% ## 1.600 20.875 51.700 64.425 77.900 분석 백분위수 50%: 국가의 50%(5개 국가)가 rural_poverty_headcount 51.7 미만입니다. 마지막 도표에서 확인할 수 있습니다. 해당 국가는 피지, 콜롬비아, 모로코, 카자흐스탄, 말레이시아입니다. 백분위수 25%: 국가의 25%가 20.87 미만입니다. 여기서 보간법이 사용된 것을 볼 수 있는데, 25%는 약 2.5개 국가를 나타내기 때문입니다. 이 값을 사용하여 국가를 필터링하면 모로코, 카자흐스탄, 말레이시아의 세 국가를 얻게 됩니다. 다양한 유형의 분위수와 그 보간법에 대한 더 많은 정보는 help(\"quantile\")를 참조하십시오. 5.1.1.1 의미적 설명 얻기 앞선 예제로부터 다음과 같이 말할 수 있습니다: “국가의 절반은 농촌 빈곤율이 51.7%에 달합니다.” “국가의 4분의 3은 농촌 빈곤율이 최대 64.4%입니다.” (국가를 오름차순으로 정렬했을 때 기준). 그 반대로 생각할 수도 있습니다: “가장 높은 농촌 빈곤 수치를 보이는 국가 중 4분의 1은 그 비율이 최소 64.4%입니다.” 5.1.2 사용자 정의 분위수 계산 일반적으로 우리는 특정 분위수를 계산하고 싶어 합니다. 예시 변수는 gini_index입니다. 지니 계수(Gini index)란 무엇인가요? 소득이나 부의 불평등을 측정하는 지표입니다. 지니 계수가 0이면 모든 값이 동일한(예: 모든 사람의 소득이 동일함) 완전 평등을 의미합니다. 지니 계수가 1(또는 100%)이면 값 사이의 최대 불평등을 의미합니다(예: 많은 사람들 중 단 한 사람만 모든 소득이나 소비를 가지고 있고 다른 모든 사람들은 전혀 없는 경우, 지니 계수는 1에 매우 가까워집니다). 출처: https://en.wikipedia.org/wiki/Gini_coefficient R 예제: 지니 계수 변수의 20, 40, 60, 80분위수를 얻으려면 다시 quantile 함수를 사용합니다. 이 사례와 같이 빈 값이 있는 경우 na.rm=TRUE 매개변수가 필요합니다: # 여러 분위수를 한 번에 얻을 수도 있습니다. p_custom = quantile(data_world_wide$gini_index, probs = c(0.2, 0.4, 0.6, 0.8), na.rm = TRUE) p_custom ## 20% 40% 60% 80% ## 31.624 35.244 41.076 46.148 5.1.3 대부분의 값이 어디에 있는지 표시하기 기술 통계에서 우리는 모집단을 일반적인 용어로 설명하고자 합니다. 두 개의 백분위수를 사용하여 범위에 대해 말할 수 있습니다. 모집단의 80%를 설명하기 위해 10% 및 90% 백분위수를 사용해 봅시다. 국가의 80%에서 빈곤율은 0.075%에서 54.4% 사이입니다. (여기서 80%인 이유는 모집단의 가운데에 집중하여 90번째 백분위수에서 10번째 백분위수를 뺐기 때문입니다.) 80%를 모집단의 대다수로 간주한다면 이렇게 말할 수 있을 것입니다: “일반적으로(또는 일반적인 용어로) 빈곤율은 0.07%에서 54.4%까지입니다.” 이것이 의미적인 설명입니다. 우리는 대부분의 사례가 어디에 있는지 설명하기 위해 모집단의 80%를 살펴보았는데, 이는 좋은 숫자로 보입니다. 또한 90% 범위(95번째 백분위수 - 0.5번째 백분위수)를 사용할 수도 있었을 것입니다. 5.1.3.1 백분위수(Percentile)는 사분위수(Quartile)와 관련이 있나요? 사분위수(Quartile)는 25, 50, 75번째 백분위수(분기 또는 ‘Q’)를 가리키는 정식 명칭입니다. 인구의 50%를 살펴보려면 제3사분위수(또는 75번째 백분위수)에서 제1사분위수(25번째 백분위수)를 빼서 데이터의 50%가 집중된 위치를 구해야 하며, 이를 사분위수 범위(inter-quartile range) 또는 IQR이라고 합니다. 백분위수 vs. 분위수 vs. 사분위수 0 사분위수 = 0 분위수 = 0 백분위수 1 사분위수 = 0.25 분위수 = 25 백분위수 2 사분위수 = .5 분위수 = 50 백분위수 (중앙값) 3 사분위수 = .75 분위수 = 75 백분위수 4 사분위수 = 1 분위수 = 100 백분위수 출처: (stats.stackexchange.com 2017b). 5.1.4 백분위수 시각화 각 백분위수가 위치한 곳과 함께 히스토그램을 그리면 개념을 이해하는 데 도움이 될 수 있습니다: quantiles_var = quantile(data_world_wide$poverty_headcount_1.9, c(0.25, 0.5, 0.75), na.rm = TRUE ) df_p = data.frame(value=quantiles_var, quantile=c(&quot;25th&quot;, &quot;50th&quot;, &quot;75th&quot;) ) library(ggplot2) ggplot(data_world_wide, aes(poverty_headcount_1.9)) + geom_histogram() + geom_vline(data=df_p, aes(xintercept=value, colour = quantile), show.legend = TRUE, linetype=&quot;dashed&quot; ) + theme_light() Figure 5.2: 백분위수 시각화 25번째 백분위수 이전의 모든 회색 막대를 모두 합하면, 75번째 백분위수 이후의 회색 막대 합계와 거의 비슷할 것입니다. 마지막 도표에서 IQR은 첫 번째와 마지막 점선 사이에 나타나며 인구의 50%를 포함하고 있습니다. 5.1.5 순위 및 상위/하위 ‘X%’ 개념 순위 개념은 경기에서 보는 것과 동일합니다. 이는 _pop_living_slums 변수에서 가장 높은 비율을 가진 국가는 어디인가요?_와 같은 질문에 답할 수 있게 해줍니다. 우리는 dplyr 패키지의 dense_rank 함수를 사용할 것입니다. 각 국가에 위치(순위)를 할당하지만, 우리는 내림차순(가장 높은 값이 1순위)으로 할당하고 싶습니다. 이제 살펴볼 변수는 다음과 같습니다: 슬럼(slums)에 거주하는 인구는 슬럼 가구에 거주하는 도시 인구의 비율입니다. 슬럼 가구는 동일한 지붕 아래 거주하며 다음 조건 중 하나 이상이 부족한 개인들의 집합으로 정의됩니다: 개선된 식수 이용, 개선된 위생 시설 이용, 충분한 주거 면적, 주택의 내구성. 답변할 질문: 슬럼 거주 비율이 가장 높은 상위 6개 국가는 어디인가요? # 순위 변수 생성 data_world_wide$rank_pop_living_slums = dense_rank(desc(data_world_wide$pop_living_slums)) # 순위별로 데이터 정렬 data_world_wide = data_world_wide %&gt;% arrange(rank_pop_living_slums) # 상위 6개 결과 출력 data_world_wide %&gt;% select(Country.Name, rank_pop_living_slums) %&gt;% head() ## Country.Name rank_pop_living_slums ## 1 South Sudan 1 ## 2 Central African Republic 2 ## 3 Sudan 3 ## 4 Chad 4 ## 5 Sao Tome and Principe 5 ## 6 Guinea-Bissau 6 또한 _에콰도르(Ecuador)는 몇 위인가요?_라고 물을 수 있습니다: data_world_wide %&gt;% filter(Country.Name == &quot;Ecuador&quot;) %&gt;% select(rank_pop_living_slums) ## rank_pop_living_slums ## 1 57 5.1.5.0.1 상위 및 하위 ‘X%’ 개념 우리가 답하고 싶어 할 만한 다른 질문들: 하위 10%의 가장 낮은 값을 얻을 수 있는 값은 무엇일까요? 10번째 백분위수가 정답입니다: quantile(data_world_wide$pop_living_slums, probs = 0.1, na.rm = TRUE) ## 10% ## 12.5 반대로 생각해 봅시다: 상위 10%의 가장 높은 값을 얻을 수 있는 값은 무엇일까요? 90번째 백분위수가 정답입니다. 이 값보다 큰 모든 사례를 필터링할 수 있습니다: quantile(data_world_wide$pop_living_slums, probs = 0.9, na.rm = TRUE ) ## 90% ## 75.2 5.1.6 데이터 스코어링에서의 백분위수 이 개념을 사용하는 두 개의 장이 있습니다: 데이터 스코어링 이득 및 리프트 분석 기본적인 아이디어는 이진 변수(예/아니오)를 예측하는 예측 모델을 개발하는 것입니다. 예를 들어 마케팅 캠페인에서 사용할 새로운 케이스의 스코어를 매겨야 한다고 가정해 봅시다. 답변해야 할 질문은 다음과 같습니다: 잠재적인 신규 매출의 50%를 포착하기 위해 영업 사원에게 제안할 스코어 값은 얼마인가요? 정답은 스코어링 값에 대한 백분위수 분석과 현재 타겟의 누적 분석을 결합하여 얻을 수 있습니다. Figure 5.3: 이득 및 리프트 곡선 (모델 성능) 5.1.6.1 사례 연구: 부의 분포 부의 분포는 지니 계수와 유사하며 불평등에 초점을 맞춥니다. 이는 (소득과는 다른) 소유 자산을 측정하여, 사람들이 사는 곳에 따라 무엇을 획득할 수 있는지에 대해 국가 간 비교를 더 공정하게 만듭니다. 더 정확한 정의는 Wikipedia 기사와 _Global Wealth Report 2013_을 참조하십시오. 각각 자료 (Wikipedia 2017a)와 (Suisse 2013)입니다. _Wikipedia_의 인용(자료 (Wikipedia 2017a)): 세계 부의 절반은 인구의 상위 1%가 소유하고 있습니다. 상위 10%의 성인이 85%를 보유하고 있으며, 나머지 하위 90%가 전 세계 총자산의 15%를 보유하고 있습니다. 상위 30%의 성인이 총자산의 97%를 보유하고 있습니다. 앞에서 했던 것처럼 세 번째 문장으로부터 다음과 같이 말할 수 있습니다: “전체 부의 3%가 성인 70%에게 분배되어 있습니다.” 상위 10% 및 상위 30% 지표는 분위수 0.1 및 0.3에 해당합니다. 여기서 부(Wealth)는 수치 변수입니다. 5.2 funModeling 퀵스타트 이 패키지에는 탐색적 데이터 분석, 데이터 준비 및 모델 성능과 관련된 일련의 함수들이 포함되어 있습니다. 비즈니스, 연구 및 교육(교수 및 학생) 분야의 사람들이 주로 사용합니다. funModeling은 이 책에서 다루는 다양한 주제를 설명하는 데 대부분의 기능이 사용된다는 점에서 이 책과 밀접한 관련이 있습니다. 5.2.1 블랙박스 열어보기 일부 함수에는 인라인 주석이 있어 사용자가 블랙박스를 열고 어떻게 개발되었는지 배우거나, 함수를 조정하거나 개선할 수 있습니다. 모든 함수는 잘 문서화되어 있으며, 많은 짧은 예제의 도움을 받아 모든 매개변수를 설명합니다. R 문서는 help(\"함수_이름\")으로 접속할 수 있습니다. 최신 버전 1.6.7의 중요한 변경 사항 (이전 버전을 사용하던 경우에만 해당): 최신 버전인 1.6.7(2018년 1월 21일)부터 매개변수 str_input, str_target 및 str_score의 이름이 각각 input, target 및 score로 변경됩니다. 기능은 동일하게 유지됩니다. 프로덕션 환경에서 이전 매개변수 이름을 사용하고 있었다면 다음 릴리스까지는 계속 작동할 것입니다. 즉, 현재로서는 예를 들어 str_input 또는 input을 모두 사용할 수 있습니다. 또 다른 중요한 변경 사항은 discretize_get_bins였으며, 이는 이 문서의 뒷부분에서 자세히 설명합니다. 5.2.1.1 이 퀵스타트에 대하여 이 퀵스타트는 함수에만 초점을 맞추고 있습니다. 함수에 대한 모든 설명과 사용 방법 및 시점은 각 섹션 아래의 “여기서 더 읽어보기” 링크를 통해 이 책의 해당 내용으로 연결됩니다. 아래에는 카테고리별로 나뉜 대부분의 funModeling 함수들이 있습니다. 5.2.2 탐색적 데이터 분석 (EDA) 5.2.2.1 df_status: 데이터셋 상태 확인 사용 사례: 주어진 데이터셋에 대해 0(zeros), 결측값(NA), 무한대, 데이터 유형 및 고유 값의 개수를 분석합니다. library(funModeling) df_status(heart_disease) ## variable q_zeros p_zeros q_na p_na q_inf p_inf type unique ## 1 age 0 0.00 0 0.00 0 0 integer 41 ## 2 gender 0 0.00 0 0.00 0 0 factor 2 ## 3 chest_pain 0 0.00 0 0.00 0 0 factor 4 ## 4 resting_blood_pressure 0 0.00 0 0.00 0 0 integer 50 ## 5 serum_cholestoral 0 0.00 0 0.00 0 0 integer 152 ## 6 fasting_blood_sugar 258 85.15 0 0.00 0 0 factor 2 ## 7 resting_electro 151 49.83 0 0.00 0 0 factor 3 ## 8 max_heart_rate 0 0.00 0 0.00 0 0 integer 91 ## 9 exer_angina 204 67.33 0 0.00 0 0 integer 2 ## 10 oldpeak 99 32.67 0 0.00 0 0 numeric 40 ## 11 slope 0 0.00 0 0.00 0 0 integer 3 ## 12 num_vessels_flour 176 58.09 4 1.32 0 0 integer 4 ## 13 thal 0 0.00 2 0.66 0 0 factor 3 ## 14 heart_disease_severity 164 54.13 0 0.00 0 0 integer 5 ## 15 exter_angina 204 67.33 0 0.00 0 0 factor 2 ## 16 has_heart_disease 0 0.00 0 0.00 0 0 factor 2 [🔎 여기서 더 읽어보기] 5.2.2.2 plot_num: 수치 변수의 분포 시각화 수치 변수만을 시각화합니다. plot_num(heart_disease) Figure 5.4: plot_num: 수치 변수 시각화 Notes: bins: 구간(bin)의 개수를 설정합니다(기본값은 10). path_out: 경로 디렉토리를 나타냅니다. 값이 있으면 플롯이 jpeg로 내보내집니다. 현재 디렉토리에 저장하려면 점(“.”)을 사용합니다. [🔎 여기서 더 읽어보기] 5.2.2.3 profiling_num: 수치 변수에 대한 여러 통계량 계산 수치 변수에 대한 여러 통계량을 검색합니다. profiling_num(heart_disease) ## variable mean std_dev variation_coef p_01 p_05 ## 1 age 54.4389439 9.0386624 0.1660330 35.00 40.0 ## 2 resting_blood_pressure 131.6897690 17.5997477 0.1336455 100.00 108.0 ## 3 serum_cholestoral 246.6930693 51.7769175 0.2098840 149.00 175.1 ## 4 max_heart_rate 149.6072607 22.8750033 0.1529004 95.02 108.1 ## 5 exer_angina 0.3267327 0.4697945 1.4378558 0.00 0.0 ## 6 oldpeak 1.0396040 1.1610750 1.1168436 0.00 0.0 ## 7 slope 1.6006601 0.6162261 0.3849825 1.00 1.0 ## 8 num_vessels_flour 0.6722408 0.9374383 1.3944978 0.00 0.0 ## 9 heart_disease_severity 0.9372937 1.2285357 1.3107265 0.00 0.0 ## p_25 p_50 p_75 p_95 p_99 skewness kurtosis iqr range_98 ## 1 48.0 56.0 61.0 68.0 71.00 -0.2080241 2.465477 13.0 [35, 71] ## 2 120.0 130.0 140.0 160.0 180.00 0.7025346 3.845881 20.0 [100, 180] ## 3 211.0 241.0 275.0 326.9 406.74 1.1298741 7.398208 64.0 [149, 406.74] ## 4 133.5 153.0 166.0 181.9 191.96 -0.5347844 2.927602 32.5 [95.02, 191.96] ## 5 0.0 0.0 1.0 1.0 1.00 0.7388506 1.545900 1.0 [0, 1] ## 6 0.0 0.8 1.6 3.4 4.20 1.2634255 4.530193 1.6 [0, 4.2] ## 7 1.0 2.0 2.0 3.0 3.00 0.5057957 2.363050 1.0 [1, 3] ## 8 0.0 0.0 1.0 3.0 3.00 1.1833771 3.234941 1.0 [0, 3] ## 9 0.0 0.0 2.0 3.0 4.00 1.0532483 2.843788 2.0 [0, 4] ## range_80 ## 1 [42, 66] ## 2 [110, 152] ## 3 [188.8, 308.8] ## 4 [116, 176.6] ## 5 [0, 1] ## 6 [0, 2.8] ## 7 [1, 2] ## 8 [0, 2] ## 9 [0, 3] Note: plot_num과 profiling_num은 수치가 아닌 변수를 자동으로 제외합니다. [🔎 여기서 더 읽어보기] 5.2.2.4 freq: 범주형 변수의 빈도 분포 확인 library(dplyr) # 이 예시를 위해 두 개의 변수만 선택합니다. heart_disease_2 = heart_disease %&gt;% select(chest_pain, thal) # 빈도 분포 freq(heart_disease_2) Figure 5.5: freq: 범주형 변수 시각화 ## chest_pain frequency percentage cumulative_perc ## 1 4 144 47.52 47.52 ## 2 3 86 28.38 75.90 ## 3 2 50 16.50 92.40 ## 4 1 23 7.59 100.00 Figure 5.6: freq: 범주형 변수 시각화 ## thal frequency percentage cumulative_perc ## 1 3 166 54.79 54.79 ## 2 7 117 38.61 93.40 ## 3 6 18 5.94 99.34 ## 4 &lt;NA&gt; 2 0.66 100.00 ## [1] &quot;Variables processed: chest_pain, thal&quot; Notes: freq는 factor 및 character만 처리하며, 범주형이 아닌 변수는 제외합니다. 분포표를 데이터 프레임으로 반환합니다. input이 비어 있으면 모든 범주형 변수에 대해 실행됩니다. path_out은 경로 디렉토리를 나타냅니다. 값이 있으면 플롯이 jpeg로 내보내집니다. 현재 디렉토리에 저장하려면 점(“.”)을 사용합니다. na.rm은 NA 값을 제외할지 여부를 나타냅니다(기본값은 FALSE). [🔎 여기서 더 읽어보기] 5.2.3 상관관계 5.2.3.1 correlation_table: R 통계량 계산 범주형 변수는 건너뛰고 모든 수치 변수에 대한 R 지표(또는 피어슨 상관계수)를 검색합니다. correlation_table(heart_disease, &quot;has_heart_disease&quot;) ## Variable has_heart_disease ## 1 has_heart_disease 1.00 ## 2 heart_disease_severity 0.83 ## 3 num_vessels_flour 0.46 ## 4 oldpeak 0.42 ## 5 slope 0.34 ## 6 age 0.23 ## 7 resting_blood_pressure 0.15 ## 8 serum_cholestoral 0.08 ## 9 max_heart_rate -0.42 Notes: 수치 변수만 분석됩니다. 타겟 변수는 반드시 수치형이어야 합니다. 타겟이 범주형인 경우 수치형으로 변환됩니다. [🔎 여기서 더 읽어보기] 5.2.3.2 var_rank_info: 정보 이론에 기반한 상관관계 데이터 프레임의 모든 변수와 타겟 변수 사이의 여러 정보 이론 지표에 기반하여 상관관계를 계산합니다. var_rank_info(heart_disease, &quot;has_heart_disease&quot;) ## Warning: `funs()` was deprecated in dplyr 0.8.0. ## ℹ Please use a list of either functions or lambdas: ## ## # Simple named list: list(mean = mean, median = median) ## ## # Auto named with `tibble::lst()`: tibble::lst(mean, median) ## ## # Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE)) ## ℹ The deprecated feature was likely used in the funModeling package. ## Please report the issue at &lt;https://github.com/pablo14/funModeling/issues&gt;. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. ## var en mi ig gr ## en13 heart_disease_severity 1.846 0.995 0.9950837595 0.5390655068 ## en12 thal 2.032 0.209 0.2094550580 0.1680456709 ## en8 exer_angina 1.767 0.139 0.1391389302 0.1526393841 ## en14 exter_angina 1.767 0.139 0.1391389302 0.1526393841 ## en2 chest_pain 2.527 0.205 0.2050188327 0.1180286190 ## en11 num_vessels_flour 2.381 0.182 0.1815217813 0.1157736478 ## en10 slope 2.177 0.112 0.1124219069 0.0868799615 ## en4 serum_cholestoral 7.481 0.561 0.5605556771 0.0795557228 ## en1 gender 1.842 0.057 0.0572537665 0.0632970555 ## en9 oldpeak 4.874 0.249 0.2491668741 0.0603576874 ## en7 max_heart_rate 6.832 0.334 0.3336174096 0.0540697329 ## en3 resting_blood_pressure 5.567 0.143 0.1425548155 0.0302394591 ## en age 5.928 0.137 0.1371752885 0.0270548944 ## en6 resting_electro 2.059 0.024 0.0241482908 0.0221938072 ## en5 fasting_blood_sugar 1.601 0.000 0.0004593775 0.0007579095 참고: 수치 및 범주형 변수를 분석합니다. 또한 discretize_df와 같이 이전에 사용된 수치 이산화 방법과 함께 사용됩니다. [🔎 여기서 더 읽어보기] 5.2.3.3 cross_plot: 입력 변수와 타겟 변수 간의 분포 플롯 입력 변수와 타겟 변수 사이의 상대적 및 절대적 분포를 검색합니다. 변수가 중요한지 여부를 설명하고 보고하는 데 유용합니다. cross_plot(data = heart_disease, input = c(&quot;age&quot;, &quot;oldpeak&quot;), target = &quot;has_heart_disease&quot;) Figure 5.7: cross_plot: 입력 vs. 타겟 변수 시각화 Figure 5.8: cross_plot: 입력 vs. 타겟 변수 시각화 Notes: auto_binning: 기본값은 TRUE이며, 수치 변수를 범주형으로 보여줍니다. path_out: 경로 디렉토리를 나타내며, 값이 있으면 플롯이 jpeg로 내보내집니다. input은 수치형 또는 범주형일 수 있으며, target은 반드시 이진(두 개의 클래스) 변수여야 합니다. input이 비어 있으면 모든 변수에 대해 실행됩니다. [🔎 여기서 더 읽어보기] 5.2.3.4 plotar: 입력 변수와 타겟 변수 간의 박스플롯 및 밀도 히스토그램 변수가 중요한지 여부를 설명하고 보고하는 데 유용합니다. 박스플롯(Boxplot): plotar(data = heart_disease, input = c(&quot;age&quot;, &quot;oldpeak&quot;), target = &quot;has_heart_disease&quot;, plot_type = &quot;boxplot&quot;) ## Warning: `aes_string()` was deprecated in ggplot2 3.0.0. ## ℹ Please use tidy evaluation idioms with `aes()`. ## ℹ See also `vignette(&quot;ggplot2-in-packages&quot;)` for more information. ## ℹ The deprecated feature was likely used in the funModeling package. ## Please report the issue at &lt;https://github.com/pablo14/funModeling/issues&gt;. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. ## Warning: The `fun.y` argument of `stat_summary()` is deprecated as of ggplot2 3.3.0. ## ℹ Please use the `fun` argument instead. ## ℹ The deprecated feature was likely used in the funModeling package. ## Please report the issue at &lt;https://github.com/pablo14/funModeling/issues&gt;. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. Figure 5.9: plotar (1): 박스플롯 시각화 Figure 5.10: plotar (1): 박스플롯 시각화 [🔎 여기서 더 읽어보기] 밀도 히스토그램(Density histograms): plotar(data = mtcars, input = &quot;gear&quot;, target = &quot;cyl&quot;, plot_type = &quot;histdens&quot;) ## Warning: `summarise_()` was deprecated in dplyr 0.7.0. ## ℹ Please use `summarise()` instead. ## ℹ The deprecated feature was likely used in the funModeling package. ## Please report the issue at &lt;https://github.com/pablo14/funModeling/issues&gt;. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. ## Warning: `group_by_()` was deprecated in dplyr 0.7.0. ## ℹ Please use `group_by()` instead. ## ℹ See vignette(&#39;programming&#39;) for more help ## ℹ The deprecated feature was likely used in the funModeling package. ## Please report the issue at &lt;https://github.com/pablo14/funModeling/issues&gt;. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## ℹ The deprecated feature was likely used in the funModeling package. ## Please report the issue at &lt;https://github.com/pablo14/funModeling/issues&gt;. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. Figure 5.11: plotar (2): 밀도 히스토그램 시각화 [🔎 여기서 더 읽어보기] Notes: path_out: 경로 디렉토리를 나타내며, 값이 있으면 플롯이 jpeg로 내보내집니다. input이 비어 있으면 모든 수치 변수에 대해 실행됩니다(범주형 변수는 건너뜀). input은 수치형이어야 하고 타겟은 범주형이어야 합니다. target은 다중 클래스(이진 클래스뿐만 아니라)일 수 있습니다. 5.2.3.5 categ_analysis: 이진 결과에 대한 정량적 분석 범주형 입력 변수를 기반으로 이진 타겟의 대푯성(perc_rows)과 정확도(perc_target)를 각각의 입력 값별로 프로파일링합니다. 예를 들어 국가별 독감 감염률 등이 있습니다. df_ca = categ_analysis(data = data_country, input = &quot;country&quot;, target = &quot;has_flu&quot;) head(df_ca) ## country mean_target sum_target perc_target q_rows perc_rows ## 1 Malaysia 1.000 1 0.012 1 0.001 ## 2 Mexico 0.667 2 0.024 3 0.003 ## 3 Portugal 0.200 1 0.012 5 0.005 ## 4 United Kingdom 0.178 8 0.096 45 0.049 ## 5 Uruguay 0.175 11 0.133 63 0.069 ## 6 Israel 0.167 1 0.012 6 0.007 Note: input 변수는 반드시 범주형이어야 합니다. target 변수는 반드시 이진(두 개의 값)이어야 합니다. 이 함수는 예측 모델링에서 변수의 카디널리티(cardinality)를 줄여야 할 때 데이터를 분석하는 데 사용됩니다. [🔎 여기서 더 읽어보기] 5.2.4 데이터 준비 5.2.4.1 데이터 이산화 (Data discretization) 5.2.4.1.1 discretize_get_bins + discretize_df: 수치 변수를 범주형으로 변환 두 개의 함수가 필요합니다: discretize_get_bins는 각 변수에 대한 임계값(bins)을 반환하고, discretize_df는 첫 번째 함수의 결과를 가져와 원하는 변수를 변환합니다. 구간화(binning) 기준은 동일 빈도(equal frequency)입니다. 데이터셋에서 두 개의 변수만 변환하는 예시입니다. # 1단계: 원하는 변수인 &quot;max_heart_rate&quot;와 &quot;oldpeak&quot;에 대한 임계값 가져오기 d_bins = discretize_get_bins(data = heart_disease, input = c(&quot;max_heart_rate&quot;, &quot;oldpeak&quot;), n_bins = 5) ## Variables processed: max_heart_rate, oldpeak # 2단계: 임계값을 적용하여 최종 처리된 데이터 프레임 얻기 heart_disease_discretized = discretize_df(data = heart_disease, data_bins = d_bins, stringsAsFactors = TRUE ) ## Variables processed: max_heart_rate, oldpeak 다음 이미지는 결과를 보여줍니다. 변수 이름은 동일하게 유지됨을 유의하십시오. Figure 5.12: 자동 이산화 과정의 결과 Notes: 이 2단계 절차는 새로운 데이터가 들어오는 운영 환경(production)에서 사용하도록 고안되었습니다. 각 구간의 최소값과 최대값은 각각 -Inf와 Inf가 됩니다. 최신 funModeling 릴리스(1.6.7)에서 수정된 사항으로 인해 특정 시나리오에서 출력이 변경될 수 있습니다. 버전 1.6.6을 사용하고 있었다면 결과를 확인하십시오. 이 변경 사항에 대한 자세한 정보는 여기를 참조하십시오. [🔎 여기서 더 읽어보기] 5.2.4.2 convert_df_to_categoric: 데이터 프레임의 모든 열을 범주형 변수로 변환 수치 변수에 대한 구간화 또는 이산화 기준은 동일 빈도입니다. 팩터(factor) 변수는 문자형(character) 변수로 직접 변환됩니다. iris_char = convert_df_to_categoric(data = iris, n_bins = 5) # 첫 6개 행 확인 head(iris_char) 5.2.4.3 equal_freq: 수치 변수를 범주형으로 변환 동일 빈도 기준을 사용하여 수치 벡터를 팩터(factor)로 변환합니다. new_age = equal_freq(heart_disease$age, n_bins = 5) # 결과 확인 Hmisc::describe(new_age) ## new_age ## n missing distinct ## 303 0 5 ## ## Value [29,46) [46,54) [54,59) [59,63) [63,77] ## Frequency 63 64 71 45 60 ## Proportion 0.208 0.211 0.234 0.149 0.198 [🔎 여기서 더 읽어보기] Notes: discretize_get_bins와 달리, 이 함수는 -Inf와 Inf를 각각 최소값과 최대값으로 삽입하지 않습니다. 5.2.4.4 range01: 변수를 0에서 1 사이의 범위로 스케일링 수치 벡터를 최소값이 0이고 최대값이 1인 0에서 1 사이의 척도로 변환합니다. age_scaled = range01(heart_disease$oldpeak) # 결과 확인 summary(age_scaled) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000 0.0000 0.1290 0.1677 0.2581 1.0000 5.2.5 이상치(Outliers) 데이터 준비 5.2.5.1 hampel_outlier 및 tukey_outlier: 이상치 임계값 계산 두 함수 모두 이상치로 간주되는 값의 임계값을 나타내는 두 값으로 된 벡터를 검색합니다. tukey_outlier와 hampel_outlier 함수는 prep_outliers 내부에서 사용됩니다. 터키(Tukey) 방식 사용: tukey_outlier(heart_disease$resting_blood_pressure) ## bottom_threshold top_threshold ## 60 200 [🔎 여기서 더 읽어보기] 햄펠(Hampel) 방식 사용: hampel_outlier(heart_disease$resting_blood_pressure) ## bottom_threshold top_threshold ## 85.522 174.478 [🔎 여기서 더 읽어보기] 5.2.5.2 prep_outliers: 데이터 프레임의 이상치 준비 데이터 프레임을 가져와 input 매개변수에 지정된 변수들에 대해 변환된 데이터 프레임을 반환합니다. 단일 벡터로도 작동합니다. 두 개의 변수를 입력으로 사용하는 예시입니다: # Hampel 방식에 따른 임계값 가져오기 hampel_outlier(heart_disease$max_heart_rate) ## bottom_threshold top_threshold ## 86.283 219.717 # 임계값에서 이상치를 멈추게(stop) 하는 함수 적용 data_prep = prep_outliers(data = heart_disease, input = c(&#39;max_heart_rate&#39;, &#39;resting_blood_pressure&#39;), method = &quot;hampel&quot;, type = &#39;stop&#39;) max_heart_rate 변수의 변환 전후를 확인합니다: ## [1] &quot;변환 전 -&gt; 최소값: 71; 최대값: 202&quot; ## [1] &quot;변환 후 -&gt; 최소값: 71; 최대값: 202&quot; 최소값은 71에서 86.23으로 변경되었으며, 최대값은 202로 동일하게 유지되었습니다. 참고: method는 bottom_top, tukey 또는 hampel이 될 수 있습니다. type은 stop 또는 set_na가 될 수 있습니다. stop 인 경우 이상치로 플래그가 지정된 모든 값이 임계값으로 설정됩니다. set_na 인 경우 플래그가 지정된 값은 NA로 설정됩니다. [🔎 여기서 더 읽어보기] 5.2.6 예측 모델 성능 5.2.6.1 gain_lift: 이득 및 리프트 성능 곡선 예측하고자 하는 클래스에 대한 스코어나 확률을 계산한 후, 이를 gain_lift 함수에 전달하면 성능 지표가 포함된 데이터 프레임을 반환합니다. # 머신러닝 모델 생성 및 양성 케이스에 대한 스코어 가져오기 fit_glm = glm(has_heart_disease ~ age + oldpeak, data = heart_disease, family = binomial) heart_disease$score = predict(fit_glm, newdata = heart_disease, type = &#39;response&#39;) # 성능 지표 계산 gain_lift(data = heart_disease, score = &#39;score&#39;, target = &#39;has_heart_disease&#39;) ## Warning: The `guide` argument in `scale_*()` cannot be `FALSE`. This was deprecated in ## ggplot2 3.3.4. ## ℹ Please use &quot;none&quot; instead. ## ℹ The deprecated feature was likely used in the funModeling package. ## Please report the issue at &lt;https://github.com/pablo14/funModeling/issues&gt;. ## This warning is displayed once per session. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. Figure 5.13: gain_lift: 예측 모델 성능 시각화 ## Population Gain Lift Score.Point ## 1 10 20.86 2.09 0.8185793 ## 2 20 35.97 1.80 0.6967124 ## 3 30 48.92 1.63 0.5657817 ## 4 40 61.15 1.53 0.4901940 ## 5 50 69.06 1.38 0.4033640 ## 6 60 78.42 1.31 0.3344170 ## 7 70 87.77 1.25 0.2939878 ## 8 80 92.09 1.15 0.2473671 ## 9 90 96.40 1.07 0.1980453 ## 10 100 100.00 1.00 0.1195511 [🔎 여기서 더 읽어보기] 참고 문헌 (References) ———. 2017b. “Percentile Vs Quantile Vs Quartile.” https://stats.stackexchange.com/questions/156778/percentile-vs-quantile-vs-quartile. Suisse, Credit. 2013. “Global Wealth Report 2013.” https://publications.credit-suisse.com/tasks/render/file/?fileID=BCDB1364-A105-0560-1332EC9100FF5C83. Wikipedia. 2017a. “Distribution of Wealth.” https://en.wikipedia.org/wiki/Distribution_of_wealth. "],["download-book.html", "6 도서 다운로드", " 6 도서 다운로드 이 책이 마음에 드셨고 프로젝트를 지원하고 싶으시다면, PDF, mobi, Kindle 버전의 휴대용 데이터 과학 라이브 북(Data Science Live Book)을 영구 소장하실 수 있습니다. 세 가지 버전을 모두 드립니다! 🙂. Gumroad를 통한 안전 결제 🔐: 참고: 결제 중에 ‘Ghostery’ 애드온을 비활성화해 보세요. 알려진 문제가 있습니다. 불러오는 중… 위 양식이 나타나지 않거나 Gumroad 사이트에서 직접 구매하고 싶다면 다음 링크를 클릭하세요: https://gumroad.com/l/tORBv 아마존에서 종이책과 킨들 버전 구매 가능! 이 책은 현재 아마존에서 구매 가능합니다. 확인해 보세요! 📗 🚀. 온라인 버전과 PDF, mobi 또는 Kindle 버전 사이에 차이가 있나요? 아니요, 동일합니다. 데이터 과학 라이브 북은 언제나 완전히 접근 가능하며, 실제로는 오픈 소스(Attribution-NonCommercial-ShareAlike 4.0 International)입니다. 새 버전이 출시되면 어떻게 되나요? / 최신 버전에 접근할 수 있나요? 물론입니다! 구매 후 새로운 버전이 출시될 때마다 이메일로 알림을 받게 됩니다 (이 알림은 원치 않으시면 수신 거부하실 수 있습니다). 가격은 원하는 만큼 지불하는 방식(name-your-price)이며, 최소 가격은 미화 5달러입니다. 즐거운 독서 되세요! 🚀 어떻게 연락할 수 있나요? 저에게 연락하고 싶으시다면 pcasas.biz gmail.com으로 메일을 보내주세요. 새로운 주제 제안부터 이곳에서 배운 개념을 적용한 좋은 경험 공유까지, 어떤 이유로든 환영합니다! 😟: 기여하고 싶지만 충분한 돈이 없습니다. 하지만 휴대용 버전이 정말 갖고 싶어요. 걱정 마세요, 우리 모두 힘든 시기가 있을 수 있으니까요. pcasas.biz (at) gmail.com으로 연락주세요 😉. 아르헨티나, 핀란드(및 다른 몇몇 국가들)의 대학 교육 사례와 같이, 가능할 때마다 교육에 대한 무료 접근권을 가지는 것이 좋다고 생각합니다. 그래서 공부하고 새로운 지평을 탐구하고 싶은 사람이라면 누구나 그렇게 할 수 있어야 합니다. 돈이 지식을 습득하는 데 장벽이 되어서는 안 됩니다. 트위터에서 최신 소식을 확인하세요. "],["참고-문헌-references.html", "참고 문헌 (References)", " 참고 문헌 (References) Amatriain, Xavier. 2015. “In Machine Learning, What Is Better: More Data or Better Algorithms.” http://www.kdnuggets.com/2015/06/machine-learning-more-data-better-algorithms.html. Caban, Jesus J., Ulas Bagci, Alem Mehari, Shoaib Alam, Joseph R. Fontana, Gregory J. Kato, and Daniel J. Mollura. 2012. “Characterizing Non-Linear Dependencies Among Pairs of Clinical Variables and Imaging Data.” Conf Proc IEEE Eng Med Biol Soc 2012 (August): 2700–2703. https://doi.org/10.1109/EMBC.2012.6346521. Fernandez-Delgado, Manuel. 2014. “Do We Need Hundreds of Classifiers to Solve Real World Classification Problems?” http://jmlr.csail.mit.edu/papers/volume15/delgado14a/delgado14a.pdf. Fortmann, Scott. 2012. “Understanding the Bias-Variance Tradeoff.” http://scott.fortmann-roe.com/docs/BiasVariance.html. Handbook, Engineering Statistics. 2013. “Measures of Skewness and Kurtosis.” http://www.itl.nist.gov/div898/handbook/eda/section3/eda35b.htm. Hyndman, Rob J. 2010. “Why Every Statistician Should Know about Cross-Validation?” https://robjhyndman.com/hyndsight/crossvalidation/. ———. 2017. “ARIMA Modelling in r.” https://www.otexts.org/fpp/8/. Izbicki, Mike. 2011. “Converting Images into Time Series for Data Mining.” https://izbicki.me/blog/converting-images-into-time-series-for-data-mining.html. Kuhn, Max. 2017. “Recursive Feature Elimination in r Package Caret.” https://topepo.github.io/caret/recursive-feature-elimination.html. McNeese, Bill. 2016. “Are the Skewness and Kurtosis Useful Statistics?” https://www.spcforexcel.com/knowledge/basic-statistics/are-skewness-and-kurtosis-useful-statistics. Raschka, Sebastian. 2017. “Machine Learning FAQ.” http://sebastianraschka.com/faq/docs/evaluate-a-model.html. Reshef, David N., Yakir A. Reshef, Hilary K. Finucane, Sharon R. Grossman, Gilean McVean, Peter J. Turnbaugh, Eric S. Lander, Michael Mitzenmacher, and Pardis C. Sabeti. 2011. “Detecting Novel Associations in Large Data Sets.” Science 334 (6062): 1518–24. https://doi.org/10.1126/science.1205438. stackoverflow.com. 2017. “What Is Entropy and Information Gain?” http://stackoverflow.com/questions/1859554/what-is-entropy-and-information-gain. stats.stackexchange.com. 2015. “Gradient Boosting Machine Vs Random Forest.” https://stats.stackexchange.com/questions/173390/gradient-boosting-tree-vs-random-forest. ———. 2017a. “How to Interpret Mean Decrease in Accuracy and Mean Decrease GINI in Random Forest Models.” http://stats.stackexchange.com/questions/197827/how-to-interpret-mean-decrease-in-accuracy-and-mean-decrease-gini-in-random-fore. ———. 2017b. “Percentile Vs Quantile Vs Quartile.” https://stats.stackexchange.com/questions/156778/percentile-vs-quantile-vs-quartile. Suisse, Credit. 2013. “Global Wealth Report 2013.” https://publications.credit-suisse.com/tasks/render/file/?fileID=BCDB1364-A105-0560-1332EC9100FF5C83. Wikipedia. 2017a. “Distribution of Wealth.” https://en.wikipedia.org/wiki/Distribution_of_wealth. ———. 2017b. “Monotonic Function.” https://en.wikipedia.org/wiki/Monotonic_function. ———. 2017c. “Occam’s Razor.” https://en.wikipedia.org/wiki/Occam's_razor#Probability_theory_and_statistics. ———. 2017d. “White Noise - Time Series Analysis and Regression.” https://en.wikipedia.org/wiki/White_noise. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
